#+TITLE: Neo-Algol
#+AUTHOR: Aleksandr Petrosyan

* What this is

  Just a collection of thoughts and what can be considered a book on a
  programming language that I have yet to build.  This is very much an
  early-stage design document so many things will need to be revised,
  if not removed.
  
  The syntax chosen for this presentation is reminiscent of Algol,
  though with some modifications.  This is entirely accidental, the
  syntax was chosen as it fits the ideas better.  The fact that it is
  reminiscent of the ill-fated Algol 68 might itself be both a
  blessing and a curse. 
  
* Prologue: Meditations on GUI programming.

  The catalyst to this document was a collection of thoughts on
  graphical programming and the observation that with very few
  exceptions Graphical programming, despite oftentimes being lighter
  weight than command-line programming, and often easier to design, is
  prone to messy boilerplate-heavy code-bases that nobody including
  the original authors can traverse with ease.

  What is programming but an interface.  It needs to provide an easy
  interface to doing what a program does.  But effectively a user of a
  programmer, and one that configures the application are both in
  essence programmers.  There is an arbitrary distinction between
  settings that can be changed only by touching the source code, and
  some that can be done within normal operation of the program.
  However problems in GUI programming are problems of programming
  period.  We have not learned how to make arranging buttons and
  creating interface elements easy, so how are we supposed to make
  arranging Booleans and fixing problems easy?

  In the following sections we shall attempt to classify the programs
  according to some common key features.  These shall be made in
  parallel to existing programming language features, and thus allows
  us to build up the conceptual framework of treating programs, much
  like a Software development environment in its own right. Without
  further ado, the first category is

** Functions/Processors.

   A good example of this would be =ls=. It's a basic terminal utility
   that does one thing. Given a selection of input parameters it
   produces a result.  A singular one.

   You would not expect it, but Gimp is also a processor. It can be
   told what it needs to do in a comprehensive text format, and it
   would do what you expect it to do.  It could be adjusting gamma, it
   could be cropping, but it has specific inputs and specific outputs.
   Most programs have that feature, but there's an inherent difference
   between what GIMP does and what =ls= could.  When we say a program
   is exclusively a processor, or a function, we mean that it has a
   specific input and a specific output.

   If I weren't more experienced, I would say that they're functions,
   but their behaviour does sometimes depend on external
   state. Usually the input and output are strongly correlated, in
   that similar inputs soft-guarantee similar outputs.  The issue may
   arise because things like =ls= ordinarily don't have configuration
   files, and thus the user input is what =argv= sees from within the
   source code. It is, however worth noting that =ls= receives input
   from the operating system and the shell.  A functional version of
   =ls= would require you to provide the working directory, so /every/
   invocation of it would look like

   #+begin_src shell
     ls .
   #+end_src

   This is /slightly/ more cumbersome, the implied argument of the
   current directory, =.= is easy to type and invocations of =ls=
   don't become unintelligibly difficult to read as a result. However,
   there are issues, in cases where the implied behaviour is
   conditional, and complex.

   Most programs can be viewed as information processors. For them,
   the command-line interface or the shell is likely to be the best
   (and only) reasonable course of action: one standard input, one
   standard output.  However, other more complex behaviour is
   possible, especially when the program is a

** Viewers.

   These exist to represent data. A good example of this would be
   ls. It provides you with a human readable, tunable representation
   of data.

   Adobe PDF viewer, with all its faults, is a viewer. It does so
   slowly, very inefficiently, and by definition insecurely, but it
   does actually present you a rendering of the Adobe flavour of
   postscript.

   The main requirements to these programs is that

*** they get out of the way

    The greatest failure in this area is Adobe PDF viewer.

    When I click on a .pdf document in my file manager I do want to
    see the file represented right in front of me, as soon as
    possible, possibly accounting for if I've opened this file
    previously, that I might want it open at the same place to
    continue reading (my inner lambda screams).

    What the Adobe "Viewer" provides me, is something else. No, I
    didn't want to update a simple program that does one simple
    thing. No I don't want to register a program that needs to show me
    goddamn postscript document. No I don't want to see what a company
    that provided me with this inefficient piece of software does in
    other areas.

    This might have been presented silently, via static interface
    components. It could also have been via modal dialogs. This is not
    good design, but it is intentionally so.

    You see, they have bills to pay. That highlights who this article
    might be useful for: people who don't want to make good things
    worse, to make money.

    Also, arguably, this is where performance becomes crucial. People
    are more tolerant of fire-and-forget tasks taking a long time, but
    when they need immediate feedback on opening a file, suddenly a
    low-end low-memory device becomes far better than a power-hungry
    highly inefficient miracle of design. The miracle being that
    somebody actually needs so much power.

*** represent data in reproducible ways.

    At this point you might reasonably say, that every single piece of
    data can be sent via a .doc file. For starters, a person who has
    never worked with a computer would not understand how pdf
    (Portable Document Format) is even decoded. I didn't.

    A .doc file is a more natural fit, this is how you produce a
    document, this is how you save it. Everyone surely has Microsoft
    office.

    Well, at some point, you'll give it to a Mac owner, and realise
    the error of your ways.

    PDF was revolutionary, because it capitalised on the poor
    decisions made by microsoft. A .pdf file opened everywhere is
    opened the same way. It would at the very least look the same, on
    every computer that can open it.

    A failure of this approach, however is that we create documents
    that overspecify the data needed to represent the information.

    A .doc can be up to a few kilobytes, but a .pdf that contains the
    same information is several megabytes.

    Does it have to be that way? Probably not, but fixing this issue
    isn't easy. You see, you have to be convinced that this is an
    absolute necessity, that a book that can easily be reduced in size
    actually has to take up so much space.

    You would argue that larger size brings benefits. Indeed, I enjoy
    a .flac piece of music more than an .mp3. One is lossless, one is
    lossy. But, there are some people that claim to be able to hear
    the difference between a 96 000Hz sample rate and 48 000Hz.

    Trying to keep "audiophile" level recordings is as simple as
    compiling your programs with "-O1" and equally as pointless.

** Editors.

   These by many definitions are the same as the other two, but by
   many are their own beasts. You fire up a processor, if you want the
   program to do all the legwork. You fire up a viewer if you want to
   see the result.

   Why would you open an editor? Because you want both. In this case
   though, you are the one who's producing all of the actual data. The
   editor is simply converting that between two representations: the
   one on-screen and the one on your file system. The editor is a
   window into the machine, that allows you to interface with all the
   low-level abstractions, and create a finely-tuned result.

   Let me explain the difference further. An editor is closer to a
   processor that has an efficient incremental way of constructing the
   result.

   Often editors have an efficient way of representing data, but they
   don't have to. Ed is by definition an editor, but lacks any data
   representation.

   So isn't it then a blur between a processor and an editor? Yes and
   no. The timescales and intent are very different. An editor can be
   used as a processor and visa versa.

   An editor spends a lot of time interacting with the user. A
   processor takes the bare minimum of information and starts working
   right away.

   So Gimp is both an editor, if used to retouche the specific
   problems with your family photo that require human input, and a
   processor. I can batch correct the colour in a large collection of
   pictures, without ever creating a GUI.

   So editors are the main applications that would require a good GUI,
   by definition they need to have a good grasp of what the user
   wants, because the user will spend a lot of time interacting with
   the problem.

   So what exactly does a good editor do?

*** It gets out of the way when you don't need it.

    Imagine a text editor with toolbars and several layers of
    snippets, shown next to a minimap of the code, that constantly
    asks if you are sure you want to insert the p character.

    If you had to deal with programs designed that way, you're not
    alone. Even the greatest, fail. Emacs, which in my book is the
    best program ever written, has some insane defaults.

    Some functions that are relatively harmless are disabled by
    default. The practically useless header bar is turned on by
    default.

    All of this is subjective, though, as some people would also say
    "yeah and the menu bar, what were they thinking?", when I would
    argue that I use the menubar quite frequently.

    So, we have the first problem: useless things for one person are
    useful for others.

*** Represents data in reproducible ways.

    Ugh. You know that it's the same as with a viewer, so the same
    here. But the intention is different again.

    The reproducibility here means something entirely different. I
    mind data corruption, I mind irreversible changes upon saving, and
    almost all editors actually do that. If you close emacs, and
    re-open, you won't have the undo history. In premiere, you can't
    possibly store all of that information.

    git, while being arguably not an editor the same way ed isn't,
    actually has the issue fixed: you save a persistent data structure
    that you can only prune if you really want to, but won't
    inadvertently.

*** Allow for important things to be reached easily.

    Here's also where the crux of the argument of emacs vs vi{m} comes
    from. One has one-button mnemonics that allow you to do simple
    things quickly, forward one word, is an unmodified button
    press. yaking a line is two quick button presses. Up, down left
    and right are right there on the home row. It's only incidental
    that these ergonomics are an accident, they are ergonomic.

*** Allows for the most obscure functions to be reached.

    Here, both emacs and vim shine again: colon + stuff lets you run
    whatever command you want. The reason why you can do so many
    things is that the functions are given very good names, and these
    names are unique, when trying to represent a concept.

    For =emacs=, the above couldn't be further from the
    truth. =delete-forward-char= is not a good name for the concept
    that it represents. For one, how sure are you that it isn't
    =delete-char-forward= off the top of your head. Sure the name is
    there, but the words in the sentence can sometimes be re-arranged,
    preserving the human meaning, but altering the indexed search
    behaviour.  This is a common issue in programming languages, where
    /vaguely knowing/ what a function must do, has you going through
    Merriam Webster to know which of the five possible synonims in
    each permutation does what you want it to do.  So far, the onus
    had been on the programmer to give the function a succinct name,
    and all programmers have to use that same one.  Unfortunately, I
    don't think that in the long run and with larger projects, this is
    sustainable.

    Vim is better in that regard, but it fails to be consistently
    better. You see, =d= for delete, a motion argument for determining
    both in which direction, how much and of what object to delete; is
    a brilliant abstraction, but how does, e.g. =dd= fit here? The
    other =d= is a command. So does it take the command as an
    argument?  Is this how the actual internal vim command is invoked?
    Is it =:delete delete=?  No. Herein breaks the abstraction to
    binding correspondence, which Vim is incredibly proud of.

    But the breaking of abstraction is worse than you thought, the
    function doesn't even have _delete_ in its name: it's actually
    =kill=. This word was the worst pick you could make, that stuck
    because of backwards compatibility.

    This problem persists across programming languages.  Wolfram
    Mathematica doesn't have a =map= function, though it has what is
    semantically the same thing: a function that takes another
    function and applies it to all elements in a traversible
    container.  For some insane reason displaying output is called
    =print=, even though to some, the word =display= would have made
    more sense.  Haskell has =putStrLn= and =show= which are not the
    same as =toString=.  Some functions are functions, but sometimes
    they're treated as attached to the object and invoked with a funny
    dot symbol to denote the first argument.

    These issues are extremely hard-to-solve problems.  Many have
    tried.  So far, none had been successful.

*** Has a natural abstraction ort model that makes it easy to understand data manipulation.

    Let's take a more representative example, in fact let's take
    representative examples in each category.

    First: we want an IDE, like QtCreator (I know, I'm mostly using
    software used to create other software, not something like a video
    editor. We'll get to that). Now, it has an interesting layout, and
    an interesting model.  First of all, menus. Menus aren't bad, but
    often obscure the functionality of the application.  The =file=
    and =edit= menus contain what you expect them to contain; fairly
    normal. But, where would you look for a refactoring function?
    Well... From the standpoint of a standard application, the IDE is
    a text editor + quirks, so you want to have that in =edit=. But,
    when you think about it, it's actually a =tool=. Worse yet, you
    could be spending hours going through a neat hierarchy of names
    given to things in an arbitrary fashion.  The hierarchical
    structures cease being useful at the point when categories cease
    being mutually-exclusive. If you were looking for a =cat=, would
    you be looking in =pets= or =vertebrates=.

    To add insult to injury, there's a plethora of functions that are
    not available through the menus.  It should be a hard and fast
    rule that /anything/ that application can do, should be accessible
    from a central hierarchical system.  Otherwise, what's the point
    in the system?

    Another example? Finder. All Mac OS applications pride themselves
    at being very intuitive, which is a sad misnomer.  Every
    application has a menubar, to the extent to which if your
    application doesn't have a menu, Mac OS will provide you with one.
    Intuitive shortucts for everything: =Save= is Command-S; =Copy= is
    Command-C; =Open= — Command-O. Except for screenshots, we'll go
    for ergonomics on that one.  =Save as...=? I don't know what
    you're talking about, we have =create duplicate=, but sometimes
    also =Save as...=, especially if the program originally targeted
    Windows.

    Another problem is in the naming of things. I'm used to calling
    the stuff with folder locations the /side-bar/ and the thing with
    tool buttons that I don't ever touch, unless I forgot the
    keybinding the top the /tool-bar/. What do you think happens when
    I ask it to =hide the toolbar=? Well, they both go away. Also,
    what really is in the toolbar? Mostly appearance settings. The
    program called finder should contain utilities for finding things,
    not for viewing file systems.  Search functions, tagging systems,
    the search bar should be the centerpiece.  And sadly, if you told
    me that Mac OS had a program called finder, and asked me to guess
    which one of them was "the finder" my first guess would be
    spotlight.  It finds stuff. The problems with malfunctioning
    abstraction go deeper: First try to go to your home folder. If
    you're using a well-tuned Mac, like I do, that should be in the
    sidebar, but... what if you're using stock finder? Tough luck,
    then you need to scrounge the menus. And of all menus, you have to
    go to =go=. I would presonally expect this to be a file manager,
    and hence have its core functionality in the =File= menu. Guess
    what =File= doesn't contain.  Well, you can make the argument that
    a directory isn't a file, but most people would still think that
    it is.  After all, a =file= in ordinary parlance isn't the
    document, but the roll that contains documents.

    This sounds like nitpicking, but the only reason people stop
    getting confused, is because people use these non-functional
    mal-abstractions on a daily basis and they simply got used to the
    illogical cargo-cult, vestigial nomenclature.  Changing said
    nomenclature would be hard, but not at all impossible.

    Confusing the hell out of a new user here, means that they spend
    more time reading about your program than actually using it.
    Some, particularly Arch or Gentoo users can find this fascinating,
    but the vast majority goes for the lowest hanging fruit.

*** A good editor needs to be tunable.

    Why is this so important?

    A couple of reasons were already touched upon. You don't argue
    about taste: a version of =emacs= that didn't have a menu bar
    would irritate an equal number of people to the number that
    disables it blindly. [fn::Well, not exactly equal, but still].

    Another important reason is that the purposes why people might
    want to use an applications might be *very* different. I mainly
    write scientific texts. So my word processor would need to
    prioritise scientific stuff: equations, sectioning, references. A
    typographer might want to have better font control. A mum trying
    to create a CV might want to have some style control. So, do you
    create six word processors? Or just one?

    Emacs actually has an edge in this: functions that you might want
    to have exclusively when using the editor for a specific type of
    text are only available when you run in the requisite /major
    mode/. Yeah, I agree that the name is crap but the idea is itself
    rather sound. A /minor mode/, counter-intuitively is a set of
    functions that you want with all the types of file. THey may be
    triggered much in the same way as /major modes/, but definitely work
    in a different way.

    Contextuality and inference save people time.  =Emacs= is one of
    the better programs in this regard.  You can personally customise
    your particular set-up to change assumptions about what you do. If
    you are editing a scientific text in e.g. LaTeX, you can hook
    other convenience features useful only when doing science work,
    like a minor mode that allows for speedy input of LaTeX
    mathematical symbols.  Microsoft Office is a good example of how
    this is done in practice, because the exemplary contextual control
    is exceptionally useful, and clearly highlighted.

    All good programs have to balance the out-of-the-box experience
    with the need to sometimes adjust the program.  The more rigid the
    program, the easier it is to integrate into a pipeline, but the
    harder it is to integrate into a workflow that's already present.
    Ideally, if all programs were 100% static, you wouldn't need to
    ever change any settings, and if programs were at least as
    customiseable as =emacs=, you would never have the frustration of
    having to do a task repeatedly.  You could set your own defaults
    for every parameter, have plugins that amplify what you want, and
    move all of your most commonly used functions somewhere
    accessible.  Sadly, not all programs are rigid, and not all
    programs are as self-documenting customiseable behemoths.

** What are the common problems.
*** Modal Dialogues.

    This is one UI element that I wish no UI toolkit implemented.
    This is a patchwork solution.


    So why do I hate it?

    Simply put: there's no good reason to use this element.

    It's annoying for end users, especially if you make the mistake of
    creating a modal dialogue that has a complicated element that may
    fail to load (e.g. advertising, which we'll talk about
    separately).

    A classic example of using a modal dialogue is when something goes
    wrong. For example, =emacs= that does them right, asks what do you
    want to do if a file you're editing is changed on disk, and you're
    trying to reopen a buffer. This happens because the user is
    clearly doing something that isn't unambiguous and destructive. In
    other words of the evils of annoying the user and potentially
    destroying hours upon hours of work, it asks, politely, the user
    to say which of the two contains the work.  But that is not why it
    does them right.  It does them right, because you can turn off
    these =y-or-no-p= prompts, and just do what needs to be done every
    time.

    However, the issue as with any bad design is obfuscation: there
    indeed is a problem. In our previous example, the user can
    potentially destroy their work by doing an unsafe
    operation. That's usually considered bad design, made worse by the
    fact that the operation is frequent and not labeled as
    such. Indeed, nobody would expect that a read operation can
    destroy data. Essentially, instead of fixing the design issue
    directly, the programmer has the safety net of delegating the
    responsibility to the end user. In this instance the programmer is
    lucky: the problem is easily understood using the limited
    information given in the mode-line.

    But what would be a better approach? Well, addressing the issue
    should prove to be a good start: a read operation is only
    dangerous, because there's a one-to-one correspondence between the
    buffers. What if I modified it? What if a single file could have
    multiple open buffers corresponding to it? There's even a natural
    ordering: we can include the date opened and date modified in the
    hash, so that there were no collisions in keys.

    A lot of the time the "Are you sure you want to do X" is an excuse
    for having poor documentation.

    Do I want to delete a file? Yes of course I do. What if I marked
    the wrong file and only realised it after the fact? Well, that's
    really a tough question, and, again, a problem. People might have
    mistakenly marked a file for deletion, and being unforgiving in
    such a case is... well... a tad cruel for the lack of a better
    word. That being said, there is a better design that can
    accommodate for both leniency and expedience.

    My approach would be to change the way the file-system is
    represented to the end user. Even if you perma-delete a file, you
    don't actually perform anything destructive, you simple flagged
    previously occupied space as free, and much like in the recycle
    bin or the trash, or suchlike, the file is only irreversibly
    damaged if something else was written, that overwrote some of the
    newly marked free space. So why not get rid of the perma-delete
    function altogether?

    In my book, this creates a much better workflow: you don't get
    into the habit of preemptively deleting files without the ability
    to roll that decision back. The only good reason to delete files
    is to free up space for new ones, (or to shred them irrecoverably,
    which is very different) which in this workflow adds an extra step
    before writing: just empty trash.

    On Linux the same is much more complicated, mainly because the
    trash is only allowed to grow so much, and things don't get
    auto-deleted from it. Ideally, the trash should grow until it
    engulfs all free space. In practice it represents about 20%. So
    instead of fixing this problem, which by the way isn't easy, we
    simply add the ability to irreversibly delete things.

    The good part of the above approach is that if you =rm -rf= the
    object, the operation won't ask you, and won't even show you
    what's going on, (note always better to use =rm -vfr=).

    Are you sure you want to undo typing on IOS is the most egregious
    example of that. The shake gesture is an un-intuitive one, unless
    you're an entitled 12-year-old that is inclined to shake the
    device in rage if something goes wrong. So instead of coming up
    with a better approach, they use a modal dialog.

    I'm not exempt from this technique of obfuscation myself: when I
    was writing =QDolist= (largely to explore these issues directly),
    I used it to create a move dialogue (which I thankfully scrapped)
    and allow editing the task data.

    Needless to say, that idea was not a good idea. The button for
    editing the task in Kirigami (a QtQuick package that has a
    more-less native look and feel, that I used primarily to make it
    look like a native KDE application) was not visible by
    default. The saving grace was that it was visible on hover, but I
    also need descriptions inside the dialog, to map the information
    given in the task, to the fields in the dialogue.

    This is a frustrating design. It's not that people don't read
    descriptions, it's that if I do my job right, they won't have
    to. In this case, every single modal dialogue is a sign of
    failure.

*** Form-centric design

    Imagine that you wanted to create a program that wraps another
    Command-Line application. The most simple approach is to create a
    single page that shows you the selection of all the possible
    values of all the possible arguments, and thus you have a page
    that contains all of the functionality of the application in one
    place. Neat!

    Except no. This overabundance of options and lack of
    differentiation between options, switches and arguments is not
    doing the user any favours.

    What most UI designers don't realise is that we don't actually
    read the text. I personally tune out the UI elements
    altogether. In a text editor like emacs there are some UI elements
    that contain some useful information, like the buffer name, but my
    attention is square on the text.

    My controversial opinion is that neither icons nor text actually
    play a role in user interface navigation, at least not in the long
    run. What do I mean? When You're dealing with Photos on IOS do you
    immediately think to pinch in and pinch out, or are you using the
    magnifying glass icon? Gesture navigation is becoming more
    prevalent: Apple mobile devices don't have a dedicated home
    button, neither do the Android flagships.

    So what would be the equivalent of gesture navigation on PCs?
    Well, it's having proper control elements. IOS and Android have an
    example of how to do this wrong: you have a button that means
    "select" or "enter selection mode" which is such a common design
    mistake, that it actually looks like an intentional design
    choice. Don't provide a moving mode and special buttons for moving
    stuff around, just give the user drag handles.

    Now what did I mean by saying "Form-centric design"? Simply, I
    meant providing input methods for data that doesn't at all account
    for the type of input that you might be receiving: a table with
    "arguments", "switches without the leading --". Instead, you might
    want to communicate the intent: these options are mutually
    exclusive, so they're represented with radio buttons. These
    options are either on or off, and presence of one doesn't affect
    the other.

*** Half-Baked redundancy.

    Redundancy is good. It usually means that people are able to do
    things in more than one way. Is this good? Yes, if done right, but
    this is usually mishandled.

    A good example is `Geary` allows you do some things to a letter via
    a keyboard shortcut, but not everything. Another example is
    `Octopi`. You can do multiple things, but one that's sorely
    missing, e.g. the ability to mark programs for installation via
    keyboard.

    A fair question may be asked, ``do we really need all of the
    redundancy?'' My firm belief is yes. It is not merely a frivolity
    or a luxurious waste to support multiple input methods or even
    paradigms. On the one hand, there's the question of accessibility:
    some might find it harder to use the mouse, some - the
    keyboard. Some might use an emulation of a pointing device, that's
    simply presenting itself as the mouse. Secondly, you have the
    option of unorthodox ergonomics: having this redundancy will allow
    you to expand in the future, as e.g. touch-inputs present
    themselves as pointing devices, but have much greater potential.
    Thirdly, it's the question of having a coherent interface. If for
    each keyboard shortcut there's a corresponding visual interface
    element, which clearly delineates its function and purpose, then
    documentation is less necessary. Moreover, if you follow this
    convention to the letter, it makes the scripting of your programs
    together a less-tenuous proposition.

*** Non-codified conventions.

    Of all the problems this one is less specific to graphical
    programs, and more of a general issue with modern day programming.

    Consider PEP8. It declares a collection of stylistic guidelines
    that themselves suffer from a single problem. If the guideline is
    not enforced at interpretation or compilation, then it is not a
    rule. Some of the decisions on Pep8 are so fundamentally good, that
    not following them should only be allowed in certain cases. Some of
    these are so bad, that when you have to add them as rules, you
    might reconsider adding them to the guideline, thus making the
    half-measure of the guidelines fully qualified measures and coding
    requirements.

    A good example of the good that this can do is in Haskell. One does
    not need to guess at the coding style to figure out if a
    Capitalised identifier means a class or an instance. Because this
    convention is at the language level, it is followed
    universally. Moreover, much like you don't argue whether or not you
    should use an infix addition or prefix in Lisp, you no longer have
    to keep track of the guidelines yourself.

    LaTeX has an even better solution to this problem. It is due to a
    crucial insight that we recognise that the logical structure of a
    piece of writing is completely factorised from the representation
    and certain stylistic choices. In a way, LaTeX, org and Markdown
    are the high-level languages of the modern day, because they allow
    one to delineate the styles and mark them, separately from having
    to define every aspect of the style. Because that's the case, the
    papers' authors are only worried about the contents, while the
    typesetters and compositors of the scientific journals worry about
    providing the styles. Thus, instead of having to compose the
    references by hand, in e.g. LibreOffice writer (or god forbid
    Word), we instead have the luxury of using a class file that
    describes the style and a structured data format for storing the
    information about the reference.

    Yet, unfortunately this is not the case in GUI programming. There
    are few guidelines that are baked into the standards of the
    toolkits. One is given far too much freedom to stray, and far too
    few tools to make following the conventions take less than half of
    the effort.

* The need for a new paradigm.

   When I was studying computer science at university, I heard the
   following remark, ``GUI code is always messy''. I believe that this
   is wrong. Not that it's incorrect, but that this is not necessarily
   an acceptable state of being. I believe that the way we approach
   GUI programming in the modern day is very similar to how we
   approached programming in general, before high-level languages were
   invented.

   Consider that you need to write a program that asks the user for
   the input, then prints the output that's a function of the
   input. if you were writing this in a language like e.g. C++ it
   would look like this:

   #+BEGIN_SRC C++
     #include <iostream>
     #include "somefunc.hpp"

     int main(){
       int x;
       std::cin>>x;
       std::cout<<somefunc(x);

       return 0;
     }
   #+END_SRC

   This is terse, readable, and easily intelligible. So how about we
   do the same thing, but in a GUI program?

   Well, for starters we need to choose a toolkit. Because there's no
   such thing as a cross-platform interface to a GUI toolkit that is
   installed by default. We can expect everyone to have a 256 colour
   terminal, but not everyone can be expected to have a 640x480
   monitor.

   Picked the toolkit? now you need to write tons of boilerplate. You
   can't just have an input, you have to have an input box, whose
   state and lifetime you need to control. You need to have a special
   app handler that starts the event loop. You have to add things to
   the layout and then execute them. Moreover, you would be lucky if
   it turned out that the standard containers that you can use are
   standard to the language. What if a =QLabel= doesn't accept a
   =std::string=, but rather a =QString=.

   Is it any wonder that nobody bothers writing a GUI, unless they
   need to?

   But why are we in such a pitiful state of being? Is it because
   writing a GUI program is more complex? I'd argue that it's supposed
   to be simpler than console... Think about how much work the
   language designers did for you in that small snippet. You don't
   worry about reading input from keyboard, symbol by symbol,
   converting into a character string, and then parsing it into an
   integer. You don't worry about garbled input, or invalid
   symbols. You don't even worry about the fact that you have to
   maintain memory. But for some reason only if you're using the
   console.

   Why then do I have to use a =QApplication?=. Why then do I have to
   have a =Qlabel= inside a container, and then execute the
   =QApplication=. Why is it that I have to provide extra metadata
   when writing a GUI application? Why?

   Mainly because the toolkits in use today are subservient to the
   libraries that were written in the '90s. These are thin-layer hacks
   that have never evolved. A slightly better situation can be
   observed with Objective C programs, Cocoa and Mac OS. Getting
   started there is much easier, because the language makes writing
   GUI programs easier, not harder, by maintaining an Object-oriented
   rather than Class oriented programming paradigm.

   But the issue is that objects are not the best approach. They don't
   work as well. Ownership is mostly unclear, lifetime is confusing,
   and some of the language features making up the toolkit actively
   fight against the features needed for a pain-free experience.

** What do we need?

   We need a new paradigm for GUI programming. It can't be purely
   procedural, as state cannot all be global, and procedural programs
   have a tendency to be unreadable if the code-base is sufficiently
   large. It can't be only object oriented as the object orientation
   does not neatly map the paradigms of the interaction onto code. It
   can't be purely functional either, as interfaces are primarily
   means of configuring or mutating the state of the program.

   So we need a new paradigm.

*** Signals and slots.

    This is one of the few language features that are basically a
    requirement for efficient GUI programming. Sure, under the hood
    the representation may be procedural, but events need to be
    first-class citizens of the language in which the program is
    written in. Every programming language that is used for GUI's in
    any significant capacity is either capable of or extended to
    include signals and messaging.

*** Bindings.

    Broadly speaking, an interface is a collection of bindings. It is
    a form of representing the internal state of the program, to the
    end user, binding the state of the program to a visual element,
    and mapping user input to changes in the aforementioned state.

    I would argue, though that the language should support handling
    input events directly. I should not have to include an obscure
    header file to work with the keyboard, much like I don't have to
    include "ascii.h" to write to standard output. I should have to
    use constants like =XK_X= much like I don;t have to use constants
    like std::string::hello, to represent "hello". Input events should
    be first class citizens of the language, and the concepts of
    connecting or daisy chaining signals to slots and input events to
    functionality should be part of the core language.

*** Dependencies.

    Living in the terminal, one expects for everything to be
    sequential. This allows one to represent the dependencies
    directly; you cannot =std::cout<<function x=, if you hadn't
    received =x= before.

    In a GUI, however, most things are not sequential, nor should they
    be. So you might be thinking that the problem of getting two
    numbers is not part of the language's business, and one should
    hand-roll a solution each time. Perhaps. I would argue that
    providing an integral type is also not a requirement of the
    language, instead it should allow you to do bit mashing, and you
    should then be able to hand-roll your own types. Of course, nobody
    does that. Moreover, this problem is not unique to GUI's and is a
    bit of a nuisance for terminal applications.

    There is no good solution for it in terminal land. You have
    libraries like =getopt= which are unnecessarily obtuse, and
    over-complicated, that allow you to input data out of order and
    filter out options from positional arguments. You can have
    remarkably easy to use libraries like =ArgParse= that somehow have
    you carry lots of state around, instead of being internal and
    implicit.

    Of course, the best solution to this would have been
    convention. See, you already have standards about how to format
    the arguments, how to request help in the command line. Why not,
    then change the signature of the main function and have the
    arguments be parsed before the program is invoked.

    Indeed, why can we have

    #+BEGIN_SRC java
    public static void main(String[] args){
    ...
    }
    #+END_SRC

    but not

    #+BEGIN_SRC java
    public static gui void main(String[] posargs, Map<String, bool>  options)
    #+END_SRC

    In case of languages like Python, the lack of good argument
    support is even more silly. This oversight can be summarised by
    the fact that you can't just do

    #+BEGIN_SRC python
      if not 'opt1' in  sys.args.options:
         print('--opt1 is required')
      else:
         if not 'output' in sys.args.named_args:
            print('no output specified')
    #+END_SRC

    seems to run counter to the zen of python (although this is not
    the most egregious omission).

    So from this brief detour into comparing GUI to terminal
    programming, one sees that the functions are good for tracking
    dependencies. However, while we can expect that the shell will
    pass the function arguments to the program, we cannot expect for
    it to parse them, nor to generate the usage message on its
    own. Worse yet, the usage, the built in help message exists
    completely independently from man pages. I can cut C programmers
    some slack for not wanting to embed these features into the
    programming language and make it totally unusable, but the
    designers of Unix should have known that this wasn't the best
    approach.

    So in a GUI programming language, you want to be able to express
    the requirement that UI element =X= should have value =F(Y)= if
    and only if =Y= is set.

*** Options are not variables.

    There are many things that programmers refer to as variables,
    which in truth are not. The word Variable implies mutability. We
    have constants in most major languages, albeit not always
    well-named (java's final comes to mind as a particularly odd
    choice), yet we refer to them as variables.

    It's about time we stopped. A constant is a value referred to by
    its name in code. It might be represented as a location in memory,
    it can be a compile-time constant that gets optimised away, and
    only the result of the calculation it was used in, is
    represented. Or it may be rendered as an operation (0 can be
    obtained by subtracting a register from itself, and is often much
    faster than a load). A mutable variable can be represented in
    other ways, and while there is some overlap between the two
    concepts, I'm sure no mathematician would argue that =pi= is a
    variable.

    The semantics of program options are also, very clear, and quite
    distinct from variables. They are almost always stored in
    persistent storage, almost always have layers, like
    e.g. system-wide configuration versus per-user versus
    per-session. Options are almost never anything more than a
    primitive type: it can be a single number, or it can be a string,
    or it can be a choice, which reduces to a Boolean or an
    enumeration. Yet, we almost never treat them as separate from any
    other run-time variable. Yes we might read them from the disk, but
    we almost never declare them differently.

    #+BEGIN_SRC C++
    std::string maybe_option;
    const std::string definitely_a_constant;
    // but not
    option std::string definitely_an_option;
    #+END_SRC

    An important point needs to be made, though. While you could argue
    that you could do fine without an option as a first-class citizen
    of a language, the same can be said of any type of type annotation
    in many languages. Constant serves the purpose of decaying to a
    literal, and you can achieve much of the same effect by simply not
    modifying the variables. Will the compiler know what to do?
    Probably. It's actually not that difficult to check whether thee
    had been any assignments to a particular symbol and thus infer
    that it's a constant. However, the people who design languages
    deemed it necessary to include this.

    The point is that this takes the cognitive load off of the
    programmer, and onto the compiler to prevent further assignment,
    it also clearly communicates intent.

*** Separation of logic, structure and representation.

    This is a concept that can be regarded as borrowed from QML. One
    does not have to worry about as much boilerplate as one would, if
    choosing to work with QML. The simple reason is that one has a
    single, simple language to define the user interface in, and a
    separate file for representing the business logic of the program.

    Sadly the implementation leaves much to be desired, and suffers
    from some major problems. One of the bigger omissions that QML
    doesn't yet retain is the separation of structure and
    representation.

    There's no level of abstraction between ``this is a collection of
    structured elements'' and ``This is how I show it on
    screen''. What I want to be able to do is to write something like

    #+BEGIN_SRC qml
            Page{
             ListView{
               model: native_language_construct
               delegate: {
                   Text{
                       model.name
                   }
                   Text{
                       model.year
                   }
                   \\
                   Bool{
                        model.isSelected
                    }
                }
            }
            }

    #+END_SRC
    Instead of having to specify the anchors, the widths and the
    explicitly bind the events to objects.

    It does however, provide the tools necessary for that to be
    achievable. For example, a library based on Qt and QML called
    Kirigami, allows one to specify things in a fashion of logical
    binding where the library handles conformity, but preserves the
    fine-grained control that one may have by supporting the
    primitives of pure QML.

*** IO

    This should be obvious and easy to do. But sadly it isn't, to my
    knowledge, even remotely in a working condition on any GUI
    toolkit.

    What do I mean by IO, then? Well, a bit of this and a bit of
    that. What I noticed when I was trying to debug application on
    Linux, was that I was usually trying to understand what went wrong
    with an application by running it from a terminal. Why did I do that?

    The simplest answer is that the verbose output written to the
    terminal allowed me to ascertain what went wrong and make
    amends. Another way to describe this, would be that the program
    did not provide me with sufficiently intelligible feedback if it
    did not execute properly. On Windows, there is an inkling of a
    solution, you get a modal dialogue that tells you
    something. However, the quality of the error message varies. It
    can be something as useful as "libbotan.dll" not found. So one can
    easily search for the error condition. Or it could be "An error
    has occurred!". Obviously. What kind, though is left up to the
    person who happens to know where the log is located. On Mac OS X,
    you shall find many of the best practices of error handling. If a
    program had failed to run, you get clear feedback. Moreover, if it
    fails to run due to a catastrophic failure, e.g. a segmentation
    fault, you get an error message that defaults to a modal dialogue,
    but can be turned into a notification. Sadly, this notification,
    usually doesn't contain anything useful to the end user, (and
    something that I as a developer had very rarely found to be
    useful).

    What I really want, is a server-side solution that acts like
    stderr, but for graphical programs. A useful shorthand is that
    every program when run, creates a splash screen, wherein each stage
    of the loading, that is normally written to stdout is written in
    clear, and select-able text. A progress-bar is not necessary, but
    usually useful.

    If a graphical program had encountered an error and needs to be
    stopped by the OS, a measure should exist for preserving the
    output, at least temporarily. This should be possible, as we can
    achieve this effectively by running the failed program in a
    terminal and the terminal windows don't close upon segfault.

    Another thing that would have been nice to have is a sort of
    notification log. Or a persistent log that can be accessible in
    the program.

    One usually presents information of the sort that would go into
    such a log into something called the status bar, but that rarely
    exceeds the bare minimum of just a text label.

    A much better approach would be a interactive log of every
    operation that the program had performed, where the points of
    no-return are clearly delineated, and one can perform the "undo"
    operation by clicking items in a graphical representation of the
    list.

    This is a rather complex construct, but i think that supporting
    this as part of the standard library of a graphical programming
    language is useful., If the language is compiled, turning this off
    can compile it out, and thus prevent the programmer from paying
    fro something that they don't use. On the other hand, you as a
    developer need to opt out of feedback to the user, rather than opt
    into it.

** Types functions and views.

   At this stage you might be wondering: everything I've described is
   available as part of QML, and doesn't seem to be particularly
   difficult to implement in QML, as a guideline, or as an add-on.

   Up until now, yes. However, I have substantial differences in
   areas where this language is the hardest to change.

   A program is an ebb and flow of interaction between the user and
   the machine. One must obtain a file to open it, and must press
   keys to edit it, and must signal the program to save it, (ideally
   the program should be smart enough to preserve the information at
   regular intervals). The way that QML models its interactions is
   declarative state description. Specifically, one provides a JSON,
   that declares the locations of each object, as if that is the most
   important thing in the world.

   I would argue that the most important thing in this regard is the
   interaction itself. When I write a console application, I don't
   think of the fact that I want a cursor a Keyboard manager and a
   buffer to store the input in. Instead, I say, prompt the user and
   get me an integer. One can simulate the approach I just described,
   by having numerous callbacks and signals and slots, but consider
   the following example. Say you want to get a file name from the
   user. How do you do that? The currently best way to do that is to
   Create a =FileDialog= object, give it an ID, that's less generic
   than =fileDialog=, and whenever you need to get information from
   the user, set it to =visible: true=. Of course you need to use that
    information somehow, so when accepted, you will need to notify the
   object that requested the file-name that the job's done. Naturally,
   this means that you need two separate functions to request and to
   process the file name.

   One could (and should) wrap this in a JavaScript function, that
   creates a temporary object, connects the "job done" signal from
   =fileDialog=, to it's process function, conveniently written as a
   lambda, forks off a thread, runs the processing, and blocks until
   the object processes the =jobDone= signal. This is a very general
   and generic thing, that you would think should be easy to
   automate, however, nobody has yet provided a library with a usable
   interface to do just that. It's very similar to the Boolean type in
   C, in that this is a function that almost every developer will
   benefit from, that wasn't originally part of the language. And most
   likely with QML this construction will not become standard for many
   years.

   However, this reveals the issue: we don't see our programs as a
   bunch of state that needs maintaining. This isn't why we use
   them. We use them in spite of the internal state. Our programs and
   behaviours are much more closely mirrored by functions rather than
   objects, so the approach of modelling your entire interface,
   predicating that through the state one can emergently produce the
   desired outcome is like claiming that assembly is the superior
   programming experience. It probably is more efficient, but is not a
   rational choice in most cases.

   So if the interaction is modelled around functions, how do we
   represent the out of order prodding? Well, the input widgets can be
   regarded as the function parameters. A function is not fully
   evaluated unless all of its arguments are provided. If one can nest
   inputs, i.e. a function can be regarded as an argument, then one
   can argue that one really deals with a chain of partially applied
   single-argument functions, and all functions accept only one
   argument. This principle is called Currying and is considered a
   cornerstone of Functional programming. What this means is that one
   does not need another language to express dependency
   chains. Instead, as soon as one of the inputs gets validated, it
   allows a single application to be performed somewhere inside the
   chain. The chain is only reduced as soon as the arguments are all
   provided. The only issue may arise with optional arguments, but
   they too can be represented as functions.

   OK. Are there any other benefits to choosing functions to represent
   GUI views? Well, aside from having an explicit method of
   representation of dependencies, and a straightforward method of
   propagating changes, at the very least, changes within the
   interface. Another boon is that type systems often retain much more
   information than they let on. This principle is the cornerstone of
   Coq, and is also a common method of documentation surfing in
   languages like Haskell. Often, the name and shape of the types,
   tell you more about a function than its source code.

   So imagine, that instead of having to write all of the boilerplate
   code to get a small dialogue to pop up, and having to check if the
   input is a number, one could instead write

   #+BEGIN_SRC C++
   dialogue_input<int>();
   #+END_SRC

   Tidy. The benefit of inferring, is that the best input for a
   datatype, is really dependent on the datatype and the context and
   the target platform. So a best input for an integral type
   representing a midi value from 0 to 256 might be a circular
   indicator, or a numerical window, depending on your device and
   whether or not typing is easier than dragging.

** Literacy

   Literate programming is a relatively old concept in computer
   science. It is how people approach teaching, but rarely how they
   approach designing programs.

   Emacs, calls this self-documentation. I would argue that the
   concept is far deeper than just documentation. You see, a program
   provides functions and processes events. Emacs provides you with a
   way to interrogate the way that it's built: separately allowing one
   to investigate functions and peek at the event processing. Every
   function is documented. Every function parameter is documented, and
   not only is the documentation there for the developer, but also for
   the end user. This distinction is important: when a developer is
   writing code, the assumption is that the code is what's important,
   the comments are not, and therefore clear documentation of each and
   every function is also not important. However, when doing literate
   programming, the assumptions are reversed: the main body text is
   what's important, and the code is just a rephrasing, an example if
   you will.

   So should we use literate programming for the todo-list
   application? No. Instead consider the following premise. If
   interface views are regarded as functions, then its arguments are
   the interface elements. The type should describe the kind of data
   one deals with each input element. Having borrowed heavily from
   functional programming, we should recall that OOP has a few tricks
   too. Specifically, each data type should have internal
   implementation details, that need not be pretty, but classes should
   have easy to understand behaviours. Moreover, the documentation for
   each input should not be divorced from the function it
   serves. Meaning, that if one has a to-do list application, and
   wants to create a new task, a suitable representation would look
   like this.

   For every kind of information that one can represent in the
   interface, a datatype is created, in this case, =todoItem=. I prefer
   a terse syntax, of algebraic data-types, but I can foresee a more
   class-oriented syntax being equally as useful.

   #+BEGIN_Example
   Todo.Item :: Record
   Todo.Item = (deadline :: Valid Date, state :: Required Enumeration {TODO | DONE})
   #+END_EXAMPLE


   Since this is an algebraic datatype, a constructor from these two
   items being procured separately should be easily inferred. Ideally
   you want the compiler to generate the view that would allow you to
   create the item. Since every parameter has a documentation string,
   often inferred, for simple cases, one does not have to worry about
   providing any default implementation details other than explicitly
   asking the compiler to infer the view:

   #+BEGIN_EXAMPLE
   Todo.Item.cons :: Infer
   Todo.Item.cons = infer
   #+END_EXAMPLE

   We somewhat abuse the type annotation syntax, and it may very well
   be a stupid decision. However, one can conceivably reserve the
   Infer type-name for cases where the type can be inferred. The same
   can be said about the constructor, except lowercase =infer= refers
   to functions whose signature can be inferred from the
   data-structure.

   * Validation

     The key difference between mainline paradigms is how they handle
     validation.  Let's discuss how one would define a date, to elaborate
     on the importance of strong typing in a GUI programming
     context. Types, depending on what brand of programmer you talk to,
     are either a major headache, or a massive boon. Why? Mainly because
     if an object or a structure of a specific type forms, and is proper,
     then a programmer needs not worry about making sure that the state
     of the object and relative members of it are in a cohesive
     state. Things like private members are only modified using accessor
     functions, to ensure that the state of the object is compatible with
     the mental model that a programmer might have. This is one of the
     cornerstones of object oriented programming (or rather
     class-oriented programming), with encapsulated state, i.e. not
     accessible to the outside world, and therefore maintain the
     coherence of the object. This approach is a valid option in many
     cases, however it is a tad too restrictive.

     To illustrate this point, consider a complicated hierarchy of
     classes and interfaces. Consider how one would represent a tree? Is
     it *just* a collection of nodes? Is it a linked structure that
     contains a node object and links to several other nodes? More
     importantly, how does one avoid the trap of creating containers
     within containers? Is a linked list the first node? Or is it
     something greater?

     Another diametrically similar approach is the notion of an
     algebraic datatype. It resolves the issue of unnecessary verbosity,
     by allowing recursive definitions. The notion of *multiple
     data-types* represent the same kind of data, is achieved via
     type-classes, rather than interface classes and inheritance,
     however, the crucial problem of maintaining a coherent state of the
     data is still there.

     Type-classes and algebraic data-types are common in functional
     languages, because one can have the convention, that the data-types'
     construction is the only location where the validity of the datatype
     can be called into question. Later on, the datatype shall be valid,
     because its value cannot change. Thus instead of maintaining a
     linearly growing number of setter methods that preserve the
     validity, the aforementioned validity needs to be checked once.

     So let us return to the problem in question. We want to have a
     "best way" to create the datatype for representing a date. Take
     one, is how one would do it in a class-oriented language. First an
     interface.

     #+BEGIN_SRC C++
       class Date {
       public:
         int getDayOfMonth() const;
         Date& setDayOfMonth(const int & newDayOfMonth);

         int getMonth() const;
         Date& setMonth(const int& );

         int getYear() const;
         Date& setYear(const int& newYear);
       };
     #+END_SRC

     Then one can use inheritance to declare a sub-class, that would
     suffice any function that needs a date. E.g.

     #+BEGIN_SRC C++
       class NaiveDate : Date {
       private:
         int _day;
         int _month;
         int _year;
       public:
         NaiveDate(const int& day, const int& month, const int& year)
         {
           if(day >=1 && month >=1 && month <= 12){
             if((has31Days(month) && day <=31) \
                || (month !=2 && day <=30) \
                || ((month==2) && \
                    (isLeapYear(year) && day<=29 || day <=28))){
               _month = month;
               _year = year;
               _day = day;
             }
           }
         }
         bool has31Days(int);
         bool isLeapYear(int);
         int getYear() {return _year;}
         int getMonth() {return _month;}
         int getDayOfMonth() {return _day;}
       };
       #+END_SRC

     I wouldn't call this datatype elegant. But I will give it a few
     merits: for starters, the data-structure is immutable, as it
     doesn't implement setters. That's also the reason why it wouldn't
     compile.

     So adding in the trivial setters should achieve what we want,
     right?  Well no. The trivial setters will allow arbitrary code to
     cause our object to be in an invalid state. So what can we do?

     One suggestion is to always check if we are dealing with a
     consistent object after mutation, and cause a runtime error or
     message if we're not. Another, more important question, is that
     of the datatype that we're using. Ideally, one has to use signed
     integers wherever possible. Even the creators of the standard
     library have regretted their decision to make =size_t= an
     unsigned integral. But do we really need to store all of that
     extra information to retain normal semantics of arithmetic
     operations?  Perhaps not, but we do this for safety's sake.,
     Nobody wants to have integer overflow.

     Not an ideal solution.

     In the functional programming world, some things are better, and
     some are worse.

     The good news is that We can separate out the concepts of a valid
     day and valid month and Valid Year, and only have to worry about
     their interactions:

   #+BEGIN_SRC haskell
	 data Day = Day Int
	 data Month = Month Int
	 data Year = Year Int

	 data Date = (Day, Month, Year)

   #+END_SRC

   Easy, right? Well no. We do get some benefits out of this
   architecture: you cannot accidentally misplace the type
   constructors, and end up with an incoherent mess. Moreover, here
   you are forced into thinking about the datatype as immutable. If
   you think about it more than a second, it doesn't make sense for
   dates to change, but it does make sense to construct and unbox the
   data-types in place.

   The unfortunate issue is that while we know that the datatype
   doesn't change until after construction, the data that we construct
   it from has to be checked.  The class-oriented approach has the
   benefit of the data-structure being self-maintaining. Another
   unfortunate consequence, is that in functional languages all state
   must be immutable. It makes sense here: you don't change the date
   to which you schedule your meeting, you use a different date to
   which to schedule. This doesn't make sense for the =Todo.Item= that
   we've just defined. It can change state. Moreover, the state change
   is the main purpose for which the Todo list application exists. For
   games, a similar argument can be made.

   Another subtle problem is inherent to the way dates are validated.
   What constitutes a valid day, specifically if the value 29, 30 and
   31 is valid, really depends on the month. All months except
   February are fine with 30, half are also fine with 31, and for
   February, 30 and 31 are always wrong. But to make matters worse,
   whether 29-Th of February is a valid date, is a statement that
   depends on the year. And then the precise algorithm depends on
   whether or not we are referring to the Gregorian calendar or the
   Julian calendar. This itself is a messy problem, which can
   reasonably be said to be attributable to the stupidity of Pope
   Gregory and allegedly Julius Caesar. The connections to those
   individuals are as tenuous as the usefulness of the alternative
   calendars that have been proposed.

   Even if one tries to avoid the problem of having to skip a day, the
   problem is really just shifted onto times of year, because
   depending on the year in question, the counter for when the year
   has to change technically moves. One cannot be rid of a
   discontinuity only mask it, and even then poorly. In a procedural,
   or class-oriented world, this is all fine. The functional
   programmers would be told that their type systems are extremely
   powerful and that this is fine, but the functional paradigm has
   another problem swept under the rug.

   Any programming system, has to categorise objects that exist into
   different types, and everything has a type. This sounds plausible
   and type theorists would even say that something like =Day= in our
   example is a reasonable example of a type; which itself is a valid
   (albeit unfalsifiable) statement. However, languages will mostly
   differ in terms of the relationship of the type of the object and
   what constitutes an operation on the object that cannot be
   performed.

   The crux of the problem is to ask, how is the parsing behaviour
   related to the type essence of =Date=. The static type school of
   thought would say that the type inherently and inextricably linked
   to the procedure of either parsing or validating a particular date.
   Specifically, they'd say that the parsing procedure must accept as
   input some more generalise-able /type/ (a fact which we will dispute
   later): a string of sorts, and produce *some type* that is derived
   from =Date= as defined above; specifically it's either a Monoid of
   =Date= or =Date= itself, specifically to signal that the parsing
   failed. And the validation is similarly an operation that accepts a
   thing of the type =Date= and its output is a binary: not a type
   level construct, but either another monoid, or a boolean.

   There's also another school of thought. Dynamic typing would
   suggest that while everything has a type, the type of a thing is
   not something set in stone for the duration of the execution of the
   program nor something that the programmer needs to worry about save
   for a few key moments. As such, the type of a thing that can be
   used to resolve what operation must be performed in what particular
   instance. As such, parsing can be done to whenever one needs to
   interrogate a particular property of a particular instance, and can
   be postponed, almost indefinitely. From the point of view of
   dynamic typing, the parsing operation is what determines the type
   of its output. It *could* be a =Date=, or it could be something
   else. There is no intrinsic difference between a string containing
   a valid date, a string with a non-date, and an invalid date, all
   that changes is what one wants to do with the value afterwards. As
   such there is no fundamental difference between a string or a
   =Date= object passed into a function. If a string can be
   interrogated for presence of a =year=, it *is* a =date= with a
   lowercase =d=.

   But all paradigms fail in one area. What is a =Day= in this
   context. In a statically typed world, it would be a type, that is
   tightly coupled to a =Date=. You could not parse a single =Day= at
   a time, because whether or not one can parse it, or indeed, whether
   a given day is a valid value is dependent on the =Date= object's
   other fields. But it is a class nonetheless. The dynamically typed
   person would ask the question of how much storage one needs to keep
   the =Day=, and say that if one had to ask what the type was, just
   say that it was a single integer of that size. They might even
   gloat because it allows them to not have to redefine the operations
   of addition and subtraction. The former would produce a ton of
   boilerplate code that does not automatically participate everywhere
   it should, while the latter will very soon realise that the
   addition of =Day= objects also needs to take care of the month, and
   so the expression =31+30= is not always meaningful; it can mean how
   many days are from now to two months later if today is October the
   first, but the programmer must keep that information in their heads
   at all times, and will rarely do so.

   Yet another question is whether a =Day= is intrinsically mutable.
   From the point of view of a class-oriented programmer, sure it is.
   But you have to do it via a special function called =set=, which
   will modify the =Day= in such a way that it will always remain
   valid. One cannot take a =Day= off the shelf, one must define the
   intricacies of how to preserve the inner state. This is all fine
   until one realises that it is impossible to do unless one also does
   the same for =Month= and the same for =Year=. From the point of
   view of a functional programmer it is not mutable, there are
   different dates that can be constructed from different inputs. And
   in a simple world, what one would end up with would be fairly
   usable; however, as I explained earlier, in order to construct a
   valid value of type =Day= one must know what month it is, what
   year, whether or not the current geographical location uses a
   Gregorian calendar, and since when. Because if your system shows
   the day of the October revolution in November, it's not exactly
   accurate to the high standards of functional programming. Moreover,
   in both cases, the code that tracks these invariants must be
   generated for each of those objects individually.

   The class-orientation and interfaces in functional programming
   emerged to solve the problem of the necessity to uphold invariants
   and to delineate that objects participate in certain interactions a
   certain way. To put it another way, you don't have the ephemeral
   property of "30 days hath MeCember and Crobember" for free, and one
   must define it. But having access to an off-the-shelf =DateTime=
   object that is merely a count of seconds from the Unix Epoch, one
   has the same logic but cannot extract it. Static types are assumed
   to be different until demonstrated to be the same. Dynamic types
   are assumed to be the same, unless demonstrated to be different. 

   So, we have two gigantically popular paradigms, that both fail at
   providing a robust /and/ elegant solution, though functional
   programming succeeds in providing a Robust, and class-oriented
   programming succeeds at providing an elegant solution individually,
   it is always at the cost of the counterpart. It is thus, completely
   unsurprising that programming languages of either persuasion must
   compensate for the added load of extra work that must be done, with
   some form of generic programming mechanism. Worse yet, the programs
   have to actively lie to their users about both the programmers'
   intentions and the actual end result of executing the code.

   Consider this exact situation in Rust; the functional and OOP approach is
   accomplished by creating a mutable instance of a structure that has
   this following definition:
   #+BEGIN_SRC rust
	 pub struct Day(u8);

	 pub struct Month(u8);

	 pub struct Year(u8);

	 pub struct Date {
		 day: Day,
		 month:  Month,
		 year: Year
	 }
	 #+END_SRC
One would have you believe that it is thus not possible to mutate
the object of this instance at all, even if you did declare them to
be =mut= in some contexts. But that is not true. Even given Rust's
unstable ABI, it is possible to mutate this object by writing from
a bit of =unsafe= code and using raw pointers. And crucially in its
current form, this is just as good a way as any to generate the
next day. This is accompanied by 

   #+BEGIN_SRC rust
	 impl Date {
		 pub fn next_day(self) -> Self {
			 if self.day.0 < 28 {
				 Self {
					 day: self.day.0 + 1,
					 ..self
				 } 
			 } else {
				 // ... Complex logic
			 }
		 }
	 }
	 #+END_SRC
This would seem to suggest that what we are indeed working with is a
new value of =Date= and yet, the LLVM compiler will /sometimes/
optimise this functional approach to an in-place mutation. Moreover,
it would give you, the programmer the impression that the original
value was cleared, because of the /move-by-default/ semantics of Rust
function calls. While the compiler knows full-well that what it is
dealing with is, for all intents and purposes a packed set of bytes
that is free to lay out as it pleases, you the programmer have to
pretend that it is an expensive object to copy, and thus have to
construct dates in a peculiar fashion. Of course, if you want to
communicate the intent that the object is indeed a value that is
trivially copy-able, you would =£[derive(Copy)]= on it, which would in
principle fix the breakdown of communication. An OOP-aficionado would
gloat now and say, that indeed the method of defining setters is
superior, not realising that for one, this approach would be
significantly less ergonomic to use (one would have to return =&mut
Self=, to be able to chain setters, using setters falsely communicates
to the programmer that there would be a function call overhead, when
indeed the same aggressive inlining would take place. For all of its
faults, =Python= at least allows setters to use native syntax, but the
corollary of that, is that the benign-looking assignments that are not
so benign in most cases, are even more /less-benign/ when in Python; a
function call in Python can do anything up to and including using an
actual physical printer when one calls the =print= function, but that
is a common problem of using functions to control modification. There
are no other mechanisms that would enforce the interaction.

In either case, one cannot directly mutate a sub-object and be
confident in that they have enforced the requisite invariants. We have
no language to express the constraint, and if the programmer was not
generous enough to leave abundant "comments" to explain what the
invariant was, one has to reconstruct the invariant from the way in
which it is enforced.  Moreover, one only has the option of
disallowing direct mutation, but allowing one to express allowable
mutations.  It is a safe, though false, assumption that if a type of
modification was not envisioned when the structure was defined, than
that it must be disallowed.  The result of that is an explosion of
code and a slow development cycle, which are benign problems for the
time being, that can be fixed without altering the system too much.

The real problem is that programmers routinely engage in a kind of
*doublespeak*, relying on the compiler to optimise certain operations
that they /know/ are /inefficient/, but that are reliably converted to
the efficient operations that they intend to use, but which are
frowned upon by the "best practices" of their programming
language. The paradigms are no more than a pretence, the objects are
not opaque and the data is not immutable, yet we write programs as if
they are, but fully expect the compiler to optimise the code towards
the dangerous operations.  It's almost as if the type systems are
inherently incomplete and modifications that work on top of them, are
insufficient.

There is also a clear description of how programmers think; nowadays,
every programmer worth their salt decouples the specification of the
program, as in what the program is meant to do as described in the
highly abstract sense, from the implementation, which they often quite
inaccurately reason into being, or sometimes, provide a highly
specialised version of, as in the disassembly or the intermediate
representation.  However, rarely if ever do we get the option to
control how the intermediate representation is being generated, other
than dealing the opaque compiler. We sacrifice ABI stability, but we
are rarely involved in the underlying decisions. At the same time, the
type systems of modern languages do not represent adequately even the
simple data types. Can we do better?

How about we start from a principle that both paradigms support. That
notion of a constraint. These would then make types a derived concept
that agglutinates multiple constraints into one, albeit the point
being that constraints have an arithmetic that does not have the exact
same shape as the types do.  One thing can abide by multiple
constraints.  Moreover, the constraints can somewhat abide by the
Linde-Millner algebra, and thus some form of constraint inference can
be implemented. However, the interactions of constraint systems and
functions can be wildly different, and present itself as a mix of
static and dynamic typing, with the ability to do things that were
previously thought of as only possible with a considerable amount of
effort on the programmers' part. 

Think back to C++. =const= is a constraint, while the enforcement of
it is up to debate, there are some interesting properties of how it
works that make it a staple of good design. Perhaps, one could have a
semantically similar =positive=, as a language keyword. That would be
different from the unsigned keyword in the following ways. First (and
foremost), the unsigned is a directive. It narrows the datatype, but
not the operations that can be carried out over it. So I can for
example subtract two unsigned integers, and moreover, despite the
potential mathematical issues, I'm *guaranteed* to receive an unsigned
integer in return. Even in case of over and under-flow the type
dictates how the data is interpreted, not what data is. unsigned and
signed integers, as well as floating point numbers are not distinct,
and one can run the same kinds of operations on them. The big
difference is that the compiler, knowing that one type is annotated as
integral, and the other as floating point, that when we say =a*b= we
mean floating point multiplication not any other kind.

Re-interpretation of data, is what the type system was supposed to
prevent. Segmentation faults became rarer, and being off by one in the
index variable does not cause a buffer overflow. This is not to say
that types are useless.  In fact, I would argue that during the 80-odd
years that the type systems existed, we have not yet explored their
full potential, because we have applied them at a very particular
level, without really considering the implications. 

So, how should we proceed? Well, if we think about it, much of what
the compiler is doing is constraint checking. Much of programming
revolves around makings sure that critical architectural requirements
are met, and types are the most basic form of contract. On the one
hand, we want to have a system, in which a constraint about the
state of a valid instance of a data-structure can be very precise. On
the other hand, one should like to be able to abstract away
constraints that the programmer does not use, or need. In simple
terms, /you want your code to be specific enough to run, but generic
enough to be useful./

** The constraint based typing system.

   So it might be more convenient to build a type system from the top
   down, rather than bottom-up, as is customary in most other
   programming languages. We shall define a constraint, as the only
   type keyword for the language, and build up the familiar notions of
   class, object, concept all the way to the particular data-types. A
   constraint is an expression of the form

         #+begin_example
   <constraint_name> validator.
   #+end_example

   where a =validator= is a function that can be executed at compile
   time or run-time. We could define a convenient shorthand of a single
   type: a yes or no question. So an example of a constraint can be

   #+begin_example
   <hasBlahFunction> (a) |-> (blahFunction:: type(a) -> * )
   #+end_example
   Assuming a built-in compile time introspection that looks at the
   source code AST and sees if a =blahFunction= is defined and has the
   signature we want. This would exclude many other signatures, like,
   e.g. functions that take a type of a as the second argument, but
   doesn't mind if the return type is not something specific.

   Another example would be enumerable.

   #+begin_example
   <enumerable> (a) |-> (_:: type(a) -> B::<enumerable>)
   #+end_example

   Here we say that a type is enumerable if there exists a mapping that
   takes it onto another enumerable. A subtlety needs to be pointed
   out. The constraint is given an instance of a value or variable, not
   the type. Hence the anonymous function definition =(in) |-> (out)= is
   used. Of course, the function could have been bound to a name.

   So now that we know what constraints are, How do we build up a
   type? And how do we ensure that a constraint is satisfied?

   #+begin_example
   Int ::= One | Succ Int
   #+end_example

That is the Peano definition of an integer. Of course following the
example of Coq, we shall use a shorthand for this Datatype and its
values, such that five is =5= and not =Succ $ Succ $ Succ $ Succ
One= where we've used the Haskell-like =$= notation for
parenthesis.

So Is it an =enum=? Yes. But how do we prove it? Well, the first step
is to prove that the value =One= is an enum. It obviously is to us,
but our constraint didn't say that it was. Hm. Of course, if =one=
were an enum, then the function that we wanted to provide that maps
it to an enum would exist, the identity. so we can use this to
prove that the fact that the literal value One is an enumerable as
an ansatz. First we need a working identity function.

   #+begin_example
id::a* -> a
   id x = x

   .ansatz: One :: <enumerable>
   #+end_example
   Here, notice that the identity function is defined for all types
   a. This is because the language's type inference and signature
   definition system needs to be based around type constraints first,
   and any other kind of typing system - second. In other words, our
   language must be generic before supporting any types.

   OK. So have we proved that =One= is an enumerable? Yes. Now, if the
   compiler has done its work, it should complain about the ansatz,
   but recognised that the some function that maps an enumerable onto
   an enumerable exists. Why did we need the ansatz? Well, mainly
   because if we did not, and let the compiler assume that all
   asantzen are true, we'd end up with all values being automatically
   enumerable, including ones that we wouldn't want to.

   So how do we extend that to all integers?
   #+begin_example
   predecessor :: Int -> Int
   predecessor a =
   .case a : Succ n  = n
   .case a : Succ One = One
   #+end_example

Of course this is remarkably inefficient. To prove that all
integers are enumerable, one has to walk through the entire
recursion chain to get to =One=. Of course, one could say, that OK,
we're going to assume that the built-in type of integers is
enumerable, and that we do not need this kind of trickery. Of
course, one has to remember that people are ingenious, and that
this whole issue can be averted:
#+begin_example
divideBySelf::Int -> Int
divideBySelf a = One
#+end_example
   This is cheating. It's clever and remarkably useful cheating. It
   saved the compiler from having to walk the recursion for each
   arbitrary Integer back to square one, each time a constraint needs
   to be checked. Thus many types can be made enumerable. E.g.

#+begin_example
Bool = True | False
.ansatz Bool :: <enumerable>
#+end_example

Note that we have abused the notation adopted from Haskell. As we
have a very different language, the abuse will be acceptable, but
the eagle eyed may be already trying to come up with a counter
example, where the expression and its type and constraint cannot be
made sane using a =::= notation.

While one might say (quite appropriately) that this is a weakness of
the language, the syntax itself is irrelevant. It is actually my firm
belief that if a language follows a very particular semantic meaning
for objects, the fact that it uses curly braces, or that it has a
significant semicolon, is about as important as the fact that it uses
the =\r\n= instead of =\n=, a technical detail that people get too
hung up on, instead of peering within.

So a temporary syntactic sugar, would be to say that
   #+begin_example
   value :: Type<extra_constraint1, extra_constraint2, >

   Type :: <constraint1, constraint2>
   #+end_example
   mean that the value has a type ( a function that maps a to be is
   also a type), and that each time a constraint is mentioned, it is
   being checked. If it fails, then the next code block is not being
   compiled. Think of of these expression as a hybrid between the C
   declarations and a top-level =#ifdef=.


   So why do we need this? What we see is a dependently-typed
   language, one of the very few, and for good reason: they're hard to
   maintain. However, they will offer an immense value to the typing
   system. How?

   Consider the following situation. You want to define an absolute
   difference function on natural numbers.

   #+begin_example
   difference a b =
    .case a : Succ aa, b : Succ bb = difference aa bb
    .case a : Succ aa, b : One = a
   #+end_example

   Can you spot the issue? The pattern match is non-exhaustive. We've
   missed the case where the one value is greater than the other. Now
   it might be tempting for the functional programmer in you to try
   and define it, and make it the absolute value of the difference. If
   you do that, and the function is used in differentiation code at
   some level of optimisation, you will end up with the wrong sign of
   the derivative. This is a non-issue for integers, really, because
   the true integers (and not our straw-man example), include things
   like negative integers. So should we change our definition?

   Well, think back to when you were in second or third grade. Did you
   know the negative numbers existed? Yet you doing subtraction was
   mostly fine. Why? Well, you could have been 100% sure that if you
   were given two numbers, that subtraction of one from the other,
   would never yield a negative number. But how can *we* be sure that
   such a case may never arise? Moreover, /how *do* we know/ that, if
   the function is provided as a library?

   Well, I'd argue that in most languages, we just wing it. In
   Haskell, while the pattern match is non-exhaustive, and you would
   get an error message almost every time you'd try to compile it
   (unless you turned off warnings, or turned them into errors). In
   C++, this would most likely either not compile, or be handled in a
   weird way with switch statement magic. But we want to be better.

   A constraint can be inferred from this signature. The first step is
   that the compiler will recognise that you're missing one case. So
   the constraint shall be that you never reach that state. Of course
   that is not as simple as saying that the constraint is that
   a::<is_not_One>, because for every case that a is smaller than b,
   this case will arise. Will the compiler be smart enough to identify
   this pattern? Perhaps, but the end user can easily fix the issue
   and silence any warning by saying that

   #+begin_example
   difference :: Int -> Int -> Int
   difference a b :: <isTrue> (a>=b)
   #+end_example

   So, fine, we handled the noise on our end. What would then happen
   to our end user? Well, depends on the programming language. Most
   likely -- the program will crash, and the user will have to start
   over, wondering what went wrong. Hopefully, In the GUI, a helpful
   message will pop up and say 'undefined behaviour requested.'

   But the better approach would be that if on the language level,
   these constraints could be tracked. See if I have an =if=
   statement, the body of it can be fairly sure that the value of the
   thing inside the =if= statement is constrained. So why not just
   have the constraints from if statements interact with the
   constraints on the values. e.g. that you would get a compile error
   if you tried to write

#+begin_example
somefunc a b = difference a b
#+end_example

but not if you wrote

#+begin_example
somefunc a b =  if a> b then difference a b
#+end_example

Hm. What does this mean? Well, it's like Python's duck-typing, only
functional. For one, the biggest reason why nested if statements are
dis-courage, and polymorphic decision-making is *en*-couraged, is
that the parallel if statement anti-pattern is a common trap. But in
this case, a redundant constraint check can be caught early and the code -- greatly simplified. 

** Constraint based Fizz Buzz.

What a constraint based type system would allow us to do is
something that was previously relegated to the realm of high octane
academic reading.

This drinking game is used to test the abilities and style of
problem solving amongst up-and-coming programmers. Up until
recently, (before anonymous functions became mainstream), solving
the problem proper would require quite a bit of thinking.

to illustrate, one can approach the problem simply.

   #+BEGIN_SRC python
     def fizzBuzz(n):
         if n%3 ==0:
             return 'fizz' if not n%5 else 'fizzbuzz'
         elif n%5 ==0:
             return 'buzz' if not n%3 else 'fizzbuzz'
         else:
             return n

   #+END_SRC

   While this is one of the most naive approaches one may take, most
   other naive approaches will share the same problem. The condition
   for divisibility is checked twice. Not in any impactful way, the
   code paths contain only the two necessary checks, and the program
   runs as well as one can expect. The problem isn't that this code is
   slow, or doesn't do what it's set out to do. It's that solving an
   adjacent problem requires as much work, as solving the original.

   One can do slightly better; one can express the conditions of the
   fizz-buzz sort of problem and externalise the dependencies.

   #+BEGIN_SRC python
     def fizzBuzzTake2(n):
         rules = {
             3: 'fizz',
             5: 'buzz'
         }
         ret = ''
         for k in rules:
             if n%k==0:
                 ret += rules[k]
         return ret if ret !='' else str(n)
   #+END_SRC

   We have a marked improvement: we no longer are constrained by the
   simple definition of the game, as if we needed to, we could add
   additional rules to the rules data-structure, and we could extend
   the game to other possibilities, other names and other, potentially
   overlapping nomenclature (if we added say the number 9, which is
   divisible by 3).

   We've solved the more apparent issue, but have ignored a deeper
   conceptual trap. We still have to check the same condition
   twice. Did anything happen before we are to return. A minor issue,
   but of crucial importance in many cases. Consider who should check
   if a pointer is null? Who should free a referenced shared object?
   How much dead code is in the code-base? Is the library, as written,
   too general?

   let us come back to how to resolve the issue. First, naively the
   functionality we want to achieve, is through a function that takes
   a number a factor and returns a result that's either a string or
   the original number. I say naively, because we actually want
   something else; something slightly more abstract. Consider,

   #+BEGIN_SRC python
   def tagger(n, factor, tag):
       return tag if n%factor==0 else n
   #+END_SRC
   It's not ideal in that we cannot easily compose two such
   invocations because of the fact that the returned type cannot then
   easily be checked for divisibility.

   So what if we pass in an extra instruction for what to do
   afterwards? This could be, just print the answer, or it could be
   something else... So, we could do

   #+BEGIN_SRC python

     def tagger2(n, rule, callback):
         factor, tag = rule
         if n % factor ==0:
             return tag+callback(n)
         else:
             return callback(n)
   #+END_SRC
   So if we had just one rule, the callback would be just returning an
   identity or a string representation of its input. And how do we
   compose them? Well, we can partially apply the taggers and pass
   them as callbacks. E.g.
   #+BEGIN_SRC python

     def fizzBuzz_naive(n):
         def buzzer(n):
             return tagger2(n, (5, 'buzz'), lambda x:x)

         def fizzerBuzzer(n):
             return tagger2(n, (3, 'fizz'), buzzer)

         return fizzerBuzzer(n)
   #+END_SRC

   We're certainly near something useful. Not necessarily what we
   want, but we have the behaviour that is close to what we need, the
   buzzer almost does its job. Pass in 7, you get 7. Pass in 5, you
   get 'buzz5'. So do you get rid of the string concatenation? But in
   that case the fizzer wouldn't do what it has to. It would give you
   a fizz, a buzz, a number, but not fizzbuzz. What if we allowed to
   pass in two callbacks?

   Hm. Let's see, then we could pass the instruction on what to
   concatenate with.  So..
   #+BEGIN_SRC python
     def tagger_final(n, rule):
         factor, tag, callbacks = rule
         call_if_true, call_if_false = callbacks
         if n % factor ==0:
             return tag + call_if_true(n)
         else:
             return call_if_false(n)
   #+END_SRC
   This stub lets us now define a function that combines the right behaviours:

   #+BEGIN_SRC python
     def fizzbuzz(n):
         def buzzer(n):
             return tagger_final(n, (5, 'buzz', (lambda _:'', lambda x: x)))

         def fizzer(n):
             return tagger_final(n, (3, 'fizz', (buzzer, buzzer)))

         return fizzer(n)
   #+END_SRC

   We've done it! We're not, to my knowledge, repeated checking any of
   the conditions. Moreover, this is purely functional, as much as
   purely functional can be done in Python.

   However, we have a bit of an issue. For one, it took us
   considerably more effort, to, in the words of Kevlin Henney, "make
   the control flow follow the information flow". Secondly, while the
   conceptual satisfaction of having avoided the pitfall of repeated
   condition checking (which in this case is obvious and benign),
   already, the appeal of a good-enough, maintainable code is too
   great. Moreover, we have introduced an asymmetry to the problem:
   All version of working code produce 'FizzBuzz', but in the second
   attempt, this is due to integers hashing to integers, and the
   incidental implementation detail of python, that the integers hash
   to themselves, and thus the first condition to be checked is always
   the Fizz and not the Buzz. Finally, if I had asked you to extend
   this to a case where an extra tag would be used, how would you do
   it? In the second case it is enough to add an element to a
   data-structure, in the third, you probably need another helper
   function in the chain. You have the potential to add the helper
   function in the wrong part of the chain, thus introducing a
   discordant cacophony of FizzBarBuzz rather than FizzBuzzBar.

   This problem is plaguing modern programming languages. Different
   ones, to a different extent, however all sharing the issue of that
   expressive code and efficient code are at odds with each other.

   But, if you think about it, almost everything novel and good in a
   modern programming language is a result of some kind of
   optimisation, based on some kind of constraint. Functional lets you
   easily do multi-threading, provided your source code is constrained
   to immutable state. Tail call optimisation is an optimisation based
   around a constraint that your function doesn't do anything after,
   that might need the results of the previous function call. Can you
   do this efficiently? Do you really need a constraint based type
   system to do that?

   Yes and no. People are forgetful, but not always forgetful. People
   make typos, but not always. Why was the =const= keyword introduced?
   Because a constant value was a common optimisation trick. Do you
   need it? probably not. Most likely not. But does it help?

   Let us illustrate the kinds of benefits that one may reap from a
   generic, constraint-based type system.

   #+begin_example
     <divisibleBy(n)> (a) |-> a mod n == 0 & a::<positive>

     rules =
         3: 'fizz',
         5: 'buzz'.

     tagger::Int<divisibleBy(factor::<Int(rules))> -> String
     tagger n =
         rules_{factor} ++ (tagger (n::relax::<divisibleBy(factor)>)|"")?

     fizzBuzz::Int -> String/Int
     fizzBuzz n = (tagger n|n)?


   #+end_example

   Whoa! There's a lot to unpack here! So where to begin? Well, for
   starters, look at the rules. This is a dictionary, a mapping that
   has most of the good conventions of python dictionaries. So using
   this type, we can define a constraint, that the tagger function
   needs satisfied in order to accept the input. Notice that the list
   is non-exhaustive of the integers. This is achieved by having
   co-dependent constraints. The factor can only constrain =n=, if and
   only if the factor is one of the keys of the rules. So the function
   tagger is only resolved and applied if the input is divisible by
   factor from the rules. Each instance of the function can be similar
   to a template in C++, or it could be a single function that simply
   checks the inputs. We shouldn't have to know, as this is the
   compiler's job.

   However, because the type is so tightly constrained, we cannot use
   the tagger function on any odd integer. Ideally the compiler gives
   you an error or a warning, if a naked application of tagger occurs,
   and no preceding function constrained the type to be divisible,
   then this is undefined behaviour. So, we use the black magic
   question mark operator. What is it? Tl;Dr is that given a parenthesised list, followed by a question mark e.g.

   #+begin_example
   (foo n| bar n| baz n)?
   #+end_example
   an implicit partitioning based on the constraints will be used, and
   whichever result returns (at all) shall be instantiated. For every
   constraint there is an implicit binary partitioning: you either
   satisfy it or you don't. But you could have other, non-implicit
   partitioning: you could be a mammal, a reptile or an insect. If
   previous code ruled out the possibility of you being a plant, or an
   amphibian, either one of the three and only one of the three will
   resolve to executable code. This allows us to do ternary operations
   using short convenient syntax, and define our own if statement.

   Next, note that we've relaxed the constraint that the number is
   divisible by another before re-invoking the tagger. This explicitly
   tells the compiler not to take the already checked rule in the
   possible constraints and double check. Assume it's false.

   As a result, we have exceptionally clean and maintainable code,
   just like we did in the second case of python. But do we still
   double check the condition? Well, we simply don't know. This is
   part of the internal implementation of the language, and how
   constraints are stored.

   In the initial stages of the language's development, the
   constraints are likely to be stored as objects at run time. So if
   a::<constraint> then a boolean flag is set, and the answer is `yes,
   we double check the condition', plus a lot of runtime overhead. But
   we have accomplished something; a clever compiler can look at the
   code-base, and carry through the boolean flags. it effectively knows
   that the language defines what to do, but it doesn't *have to* map
   functions to functions, and necessarily have as many as the end
   user wanted. It can effectively look at this, and create a code-path
   that doesn't double check any of the answers. The crucial reason is
   that much like in the case of the final take of the purely
   functional fizzbuzz that uses complicated lambdas to encode what to
   do next, our encoding of the problem doesn't contain an explicit
   requirement for the condition to be checked twice. So a mature
   compiler can optimise away any repeated checks.

* Constraint based typing in detail.

  What we have found is not new. In C++ you have a built-in constraint
  that a value has to be =const=. It will not allow you to perform
  operations that alter the object inside the body of a function as if
  the assignment and mutators were undefined for it. The whole of
  functional programming can be expressed as a constraint. No data can
  change. So everything you know about OOP or FP or any other paradigm
  that's currently in vogue can be expressed using a constraint. But
  is it necessarily good? Wouldn't you have to commit to one style of
  programming, as you would in e.g. Haskell, except by choosing my
  awesome language, you wouldn't necessarily communicate to the
  developers the paradigm, and they wouldn't know what to do? Good question.

** Mixing paradigms :idea:

   Haskell, isn't exactly a purely functional language. It has a
   mechanism for mutating state, called monad. The genius of the
   monad is in that it allows on the level of the language to express
   purity of some code and imperativity of other. In other words, if
   you ran =ghci= and imported a library, you could tell, which
   functions are functions, and which are side-effect ridden
   procedures, based on type. Moreover, the interaction of said
   functions is also tightly controlled, a pure function cannot call
   an impure function and still consider itself a pure function.

   The same can be said of other languages that you wouldn't consider
   functional, but that can support a functional paradigm. =constexpr=
   functions are pure functions, hence one can annotate an entire
   code-base as =constexpr= and have a small subset of code that isn't in
   a single main function. From the perspective of the programmer, the
   two paradigms have converged on the fact that purity allows for
   extra optimisations that are definitely worthwhile as they come
   essentially for free. However, one of the main reasons why we want
   programs is because we want side effects: we want IO, and we can't
   truly shove all of it into the Operating system, because programs
   whose entire point is to present information are still
   programs. Besides, re-=regging= the operating system to accept return
   types other than integers would require an extensive change of the
   ABIs all over the world. So we're essentially stuck in a world that
   wants both no side-effects and side-effects. The solution is at
   least to keep the flies and the soup, but keep them separate.

   A constraint based typing system is here to do just that, but to do
   it better. We cannot reasonably expect that a more general,
   constraint based typing system will require less maintenance than a
   traditional class-oriented system. However, the benefit of such an
   approach would be a trade-off between compile time and developer
   time. A well-built constraint based type system allows a developer
   to feel like the right answer is writing itself. A consequence of
   this, is that in most cases the code can *actually* write itself.

   So let's see what we gain. FizzBuzz is a very complicated example
   to start studying from. Let's write a factorial function.

   #+begin_example
   factorial::Int<equals(1)> -> Int<equals(1)>
   infer

   factorial::Int -> Int
   factorial n = n * (factorial (n-1))
   #+end_example

   First of all, the constraints are tight-enough that a particular
   value can be inferred, so the programmer doesn't need to provide a
   function with all the boilerplate. Sure, here we've saved typing
   factorial n = 1, and since pattern matching is a redundant feature
   in a constraint-based typing system, it's longer than Haskell. But,
   consider what would happen at compile time. There are no other
   constraints other than the type of input - Int, and the fact that a
   base case is annotated with a type, rather than a definition. Can
   the compiler tell you that there is a problem?

   Well, first of all, a compiler knows that unless you want a
   generator - the only thing that we permitted never to decay to a
   value, you have to have a recursive function terminate. Based on
   the (n-1) the compiler knows that the input of the next call is
   constrained respective of the earlier input. So as soon as we
   invoke the function, we know that all successive function calls
   will be less than the input. Thus, we know that we shall only be
   able to run the base case, if and only if the factorial function
   was called with a greater than base case value. Knowing that
   infinite loops aren't what we want, the compiler can fail. Can we
   always do this? Of course not! That would solve the halting
   problem. But there are a plethora of programs for which the halting
   problem is obvious; like this one. Cool!

   OK so we fixed it, by requiring, just like any up and coming
   software developer during an interview, that we need the value to
   be positive:

   #+begin_example
   factorial:: Int<greater(0)> -> Int<greater(0)>
   #+end_example

   except now we are faced with pretty much the same error message:
   =unsatisfied constraint:
   (factorial::<terminates>|factorial::<generator>)= But we know that
   factorial(0) = 1. Do we change the base case and lose the inferred
   function? Do we make the base case, then include the case 0?
   Maybe. This is of course the cleanest way. But why?

   Well, if you wrote

   #+BEGIN_SRC C++
   int factorial(int x) {
   return x==1?1:x*factorial(n);
   }
   #+END_SRC
   which is a lot shorter, you would have an issue. It would not be a
   compile time warning, or an error, but if you ran this program,
   you'd get an infinite loop.

   If you wrote this in Haskell, you'd get something a bit better.

   #+BEGIN_SRC haskell
   factorial 1 = 1
   factorial n = n* factorial (n)
   #+END_SRC
   So, you know that the compiler would complain of a non-exhaustive
   pattern match, which your long experience will suggest is usually a
   bad thing. So you will at least be forewarned not to pass in a
   negative number, if you had bothered to compile this translation
   unit. Perhaps, a sophisticated compiler would have argued that the
   pattern match is non-exhaustive even in a different translation
   unit, e.g. a user of your maths library. But neither would pick up
   the most common type of mistake: human error. In both examples, you
   do not actually create a chain of recursion that leads to 1. You
   would get an infinite loop.

   A constraint based system, however, needs to *know* how to
   propagate constraints. In this example no programmer would be
   expected to do this, as Integers and basic primitives should have
   this as part of standard library support, but for the sake of
   argument, we should talk about how constraints interact.

** Interlude: Constraint interaction.

   By default every object in code represents the most generic type of
   thing, a unit[fn::the eagle eyed reader will have noticed that we
   preferrentially borrow concept names from functional programming. ]
   It is the one thing that is guaranteed not to have any constraints
   and all the constraints at once. This is your `void*` if you will,
   a piece of code representing data that you might want to pass
   around, but not interact with. This is what
   #+begin_example
   id x = x
   #+end_example
   implies for the type signature. This is what you would get if you
   had meticulously relaxed every constraint set on a single datatype.

   Now let's say that we've created an object of a type. Contrary to
   what your experience of a Von Neumann type language would suggest,
   that doesn't imply the creation of a memory location. It *could* be
   a literal, or it *could* be a heap allocated piece of data. Or, it
   could be nothing at all. What determines what it *shall* be is
   contingent on the additional constraints that were set.

   And here comes the first kind of interaction of constraints (even
   before we've defined any), as the compiler at its discretion is
   allowed to constrain the data. So, let's say we have a top level
   statement like
   #+begin_example
   n = 5
   #+end_example
   The compiler can see that the data is only being created and bound,
   but not assigned to. Hence, it's =immutable=. It hasn't been
   exported, so it isn't =global=, i.e. it's =local=. It has been
   bound, but not used, so it's =vestigial=. It's bound to a literal,
   so it is =literal=. Based on the way it's written, this can be an
   integer of varying sizes, a floating point constant and even a
   string, but we have no evidence of use to the contrary. If you had
   compiled this, the constant would be killed off. If you had not,
   and JITTED everything, then the type would be determined by the
   first usage, so if you passed it to =factorial= that we've just
   defined, it would constrain to an =Int=, and later uses that have a
   conflicting type would have to relax the constraint.

   OK, so the compiler is eager to do a lot of constraining... Why
   didn't it constrain to an =enumerable=? Assuming that the module is
   visible, this is because the =enumerable= is a lazy constraint. By
   default, all constraints are lazy. This is a good thing. If you see
   how we've defined =enumerable=, we don't want to have to walk a
   recursion tree as deep as the infinite precision integers every
   time we want to instantiate an enumerable. So is enumerable a bad
   constraint? Not necessarily, in some cases it can be applied
   reasonably easy. For example:

   #+begin_example
   x = 5
   enumify :: Int<enumerable> -> Int
   enumify = id
   y = enumify x
   #+end_example
   here =x= is bound, and because there's a usage of =x= as part of
   enumify, it is an enumerable. What happens will have some
   differences, depending on whether the program had been compiled or
   interpreted, but the gist is that the constant 5 is being traversed
   down to 1, enumerability is proved, and the function can run no
   problem. However, based on what you know, would =y= also be
   enumerable?

   This is why constraint based typing is so important. We can
   propagate the fact that the rebound copy of an enumerable is also
   an enumerable. No problem so far, if we let our language infer that
   =y= is also enumerable.

   OK, but how can we avoid the expensive compile-time computation?
   Many ways. There's a syntax that allows you to override the
   constraints a shortcut if you will. In our case, the shortcut is
   that we already have an integer type, and literals for it, and
   hence we want to avoid the expensive recursive check. So we use a
   constraint equation:

   #+begin_example
   <_> => = (a) |-> (_:: type(a) -> Int) .implies <enumerable>
   #+end_example
   Here we've used an anonymous constraint name to imply
   enumerable. Indeed, we want to have a quick alias for that, but
   notice that neither how we've defined enumerable, nor this alias
   are actually good definitions of what an enumerable is. All it
   takes for me to prove that any type is enumerable is to provide a
   function that maps to an integer. Any integer. Usually this is
   fine, because you can hash *most* data-types in functional
   programming, but what would happen if I relaxed the =immutable=
   constraint? I would still have a non-enumerable, non-hashable type
   that for some reason I can enumerate. Hm.

   Two ways to fix this problem. For one, you could literally have a
   clause like this one:
   #+begin_example
   <enumerable> requires <hashable>
   <hashable> requires <immutable>
   #+end_example
   or a similar clause like this one:
   #+begin_example
   not <immutable> implies not <enumerable>
   not <immutable> implies not <hashable>
   #+end_example
   What's the difference? Remember, that the constraints are
   classified as eager and lazy, and most user defined constraints are
   lazy.

   So let's make a mutable binding.
   #+begin_example
   mutify::Int<immutable> -> Int
   mutify x = .relax x::<immutable> x
   #+end_example

   #+begin_example
   x = mutify 5
   y = enumify x
   #+end_example

   So what would happen? If the requires equation is used, then when
   the enumerability is being checked, the immutability check fails,
   and the program fails to compile. However, if the implies equation
   is used, then the enumerability is only disproved if and only if
   the body of enumify checks the immutability. Right now, the =enumify = id=
   implies that it is not. Hence, in the second case it would compile.

   Why have these two methods? The implies clause is a shortcut to
   avoid expensive computations. The requires clause is a part of the
   definition.

   But, I've given you just enough rope to hang yourself with, haven't
   I? You can specify mutually exclusive constraints as required. For example:

   #+begin_example
   <A> (a) |-> isA a
   <B> (a) |-> !isA a
   <A> requires <B>
   #+end_example
   [fn::Here we've used an anonymous function syntax to literally call
   a literal function on the literal individual type, rather than make
   broad statements that "a function of this signature exists"].

   Now, whenever an <A> test is invoked, so is the <B> test, but not
   the other way, hence something can be <B>, but not <A> and nothing
   can be <A>. Worse yet, I can have
   #+begin_example
   <B> implies <A>
   #+end_example
   somewhere down the line. What this would mean that I can create
   something that is <B>, but not <A>, and then turn it into an <A>
   after. Doesn't look like something that we'd like to allow language
   wide. Fortunately, there's one more choice that we can make that
   will allow us not to worry about it.

** Constraints as types; Constructs, Conditionals

   Do we really need conditional functions?

   To answer this question, consider what a type is? Specifically, if
   you have a thing, in memory, what is its type? Well, the type
   carries with itself a semblance of how to interpret the
   data. Specifically, it tells you: this *word* is a *floating point*
   number, and needs to be handled as a floating point
   number. Depending on the context, that might invoke a form of
   polymorphism on the arithmetic operations: e.g. exactly what
   instruction does
   #+BEGIN_SRC c++
   a + b;
   #+END_SRC
   correspond to in the compiled binary, depends on the declarations
   of the variables. The trouble here is that the type system has had
   most of its potential... misused, if not ignored altogether.

   Don't get me wrong, much of this potential is explored in
   Object-Oriented programming. For example, we could have a crude
   implementation of a double length integer:

   #+BEGIN_SRC c++

     class doubleInt{
     private:
       int contents[2];
     public:
       char* toDecimal();
       char* toBinary();
       //and so on.

       doubleInt operator+(const doubleInt& b);
       doubleInt operator/(const doubleInt& b);
       // and so on
     };
   #+END_SRC
   Now we can transparently use =doubleInt= s as if they were built
   in. We can also trivially compute the result of adding an integer
   or a floating point to the =doubleInt=, we shall be losing precision
   in the general case, but as we want to be able to add a rational
   number to an irrational as we do in maths, it makes sense.

   But why oh why can we not attach a property such as an array is
   sorted after sorting it, if we know that it is sorted? Simply put:
   it's too hard to keep track of. None of what the proposed language
   can do couldn't be done in any other type of language; it's Turing
   complete, they can do what we can do with pen and paper. But you
   don't hear people complaining about functions and saying, "there's
   nothing that functions can do, that I couldn't have achieved using
   =goto= s". The simpler approach has fallen out of fashion, because
   it is hard to execute, and hard to reason about.

   How hard are we talking? Let's say we want to encode in C++'s type
   system the concept of a sorted array, or in fact any other sorted
   collection. Well, first of all, you need the concept of a
   container, because templating over primitive unstructured and
   unsubscriptable types is a waste of compiler's time: if you had
   passed a single integer, as opposed to a int[1] to a function that
   has anything to do with sorting, then you have made a mistake
   (probably more than one). Then you need to take care of the access
   to the collection, so sadly you must also inherit from the type in
   the template. Then you must provide an override to any operation
   that may break the ordering. It's fine if the collection has an
   insert, how about if it has an insert_if? How about a shuffle
   method? Can you reasonably expect to track down every single thing
   that can break the ordering? Well, what if any modification to the
   internal state broke the ordering and any =const= method didn't?
   That is a way to do it. Do you know how to? Is it obvious? Can you
   do it without going to stackoverflow? Hopefully, by this point,
   even if you had answered yes to all of the above, you realise that
   the same can be said of functions and bare assembly jump
   instructions. Select few can. You are more likely to be one of
   them, but that doesn't mean that there isn't a problem.

   Having convinced you that OOP is not a solution to this, let's turn
   to functional. On the one hand, the solution is quite simple: the
   collections are immutable in functional programming, so any case of
   carbon copying will carry the information through. The problem is,
   that any equivalent of inserting into the collection will actually
   create a /mostly/ copy of the collection, which may not necessarily
   be sorted. So do I have to track down every call that might cause
   the loss of the property? Yes, but you can make your life easier,
   by wrapping a sorted collection into a type-class. And then
   implement every collection as an instance of the type-class until
   your compiler shuts up. That is a titanic undertaking, probably not
   going to yield the result you expect, will make your library
   non-trivial to use, if it is possible to accomplish at all. Hence
   why nobody would bother doing it.

   So can constraint based typing solve this issue? Perhaps. If done
   right, there should be an implication that checks for the kinds of
   primitive operations that would break the property. For example,
   the definition of the constraint should be that the collection has
   a natural access ordering, and along that ordering, the elements
   are also ordered, in the specified way. Of course checking the
   constraint would be too difficult, so you can say that every output
   of a sort function is automatically sorted. Or you could ask your
   programmers to manually =.constrain ret::<sorted>= on the output.
   You could also say that if on the type there exists a function that
   maps =a= and a collection of =[a]= to a collection of =[a]=, then
   it is suspect to breaking the property. And then provide specific
   exemptions from the rule, e.g. if the input and the output
   collections are the same, for any function then it is preserving
   the property, regardless of signature. These rules are surprisingly
   easy to implement with the limited constraint syntax that I've just
   shown. Try it!

   But of course, we can do more. The title of this chapter refers to
   conditional functions. Barring potential typos, in all source
   examples there have been expressions starting with a dot, and
   expressions without. Names prefixed with an interpunct (the dot),
   are language built-ins.  Their list shall be revised, and before
   the first compiler shall be made, should be reduced to a
   minimum. Notably, the if statement is not one of them.

   Syntactically, this is a construct: neither a function, nor a
   built-in keyword. One can, if given just the lambda calculus
   reconstruct the entirety of logic, meaning that if need be, all
   conditionals, can be reduced to a collection of lambda
   expressions. One would like that as the intermediate language, but
   the fact that the interpretation of the structure is left to the
   compiler, is a bit concerning. So a good language may leave the
   door open for some syntactic sugar to be present, and definable by
   the user [fn::throughout this text, the word user and programmer
   are used interchangeably. One might argue that the production
   software shouldn't be made for programmers. I argue that if the
   user configures the program, then they are doing programming at a
   high level of abstraction. Hence the only thing that is using the
   program, and not programming it, is the thing that uses the program
   as is; which is most likely the original developer.]. So the syntax
   of the construct is:
   #+begin_example
   /if\s+([expr]:cond)\s+then\s+([expr2]:result)/=\
     .local_state::<constraint<cond>> -> result
   #+end_example

   There's a lot to unpack here. First, the syntax looks remarkably
   similar to a regular expression. Ideally, there is a parsing step
   that is done between these, that verifies that the things between
   the =if= and the =then= are indeed, as they would seem,
   expressions. If other kinds of constructs are needed,
   e.g. imperative if statements, they can be added. If you want curly
   brace syntax, then you can have it. It will most likely not have
   the semantics you expect, but you can have it.

   The second point is that here we used an anonymous function that
   maps the built-in, =.local_state=, which ... is not trivial. It's
   implicit that every point of the program has an associated local
   state, that is dependent on how you got there. Can we evaluate the
   local state? For the particular point in the program, that would be
   a memory dump along with the dumps of the registers. But what we
   actually mean, is that the local state of the program that would
   have the property that the constraint holds. So let's look at an
   example:
   #+begin_example
   x = 0
   y = 1
   if x < y then z=0
   #+end_example
   Here, we create a function that only works and accepts input if y >
   x. At this stage, nothing happens at runtime, the constraint is
   checked at compile time, it always holds and we always have z=0.

   What would happen if we had a condition that would not hold, which
   we also know at compile time?
   #+begin_example
   if y < x then z=0
   #+end_example
   In this case we'd get a compile error: unsatisfied
   constraint. Indeed, we need the input to be of one type, but we had
   not constrained the input to that type, and we have no way of
   propagating the constraint to satisfy it. One way to fix it would
   be to artificially constrain y<x, but that would simply move the
   compile time error to a run-time error: depending on what the final
   implementation wants, it could be undefined behaviour, a panic, or
   a number of things. Instead, we may recall the use of the question
   mark operator, either in the usage:
   #+begin_example
   (if y<x then z=0|z=1)?
   #+end_example
   or in the definition of the construct (exercise), to give us the
   familiar if/else.

   Can we have a /traditional imperative/ =if= that, when fails,
   simply does nothing. You can define such a construct in the
   language (exercise, recall the () is also an expression), but
   should you?

   We shall discuss this in greater detail in a later section, but the
   model of the distribution of code, permits for people to have the
   pie and eat it too. The very reason for the existence of constructs
   is that they are nothing more than syntactic sugar. On the one
   hand, the compiler never has to worry about them. On the other
   hand, the user can map the syntactic sugar even when it is not
   exactly the source code that someone wrote. This prevents the
   proliferation of moot "best practices", unless they offer a
   tangible benefit.

** Constraints over Types

   A key point in the development of C++ was that the language is to
   have a strong static type system. Why?

   As an example, C++ has a language keyword for =constexpr=, meaning a
   compile time, purely functional (as in no side effects are
   possible) value or function. As it was added rather late,
   optimisations that would allow for things to be substituted under
   referential transparency have to be built into the compiler and not
   libraries supplied to the end user. If I want to implement a
   library that optimises calls of a specific kind[fn::we shall later
   define what a =kind= is. ], I have to contact either LLVM or GNU
   compiler collection maintainers, ask them to implement this. If
   it's good, it shall become part of the standard, and every compiler
   in the rest of the world will be forced to implement this, or it
   remains a non-standard extension, meaning that some compilers
   support this, and some don't. But the bigger weakness of this
   approach is that the end user (the programmer), has to think about
   which functions can be made compile time functional or not. Most
   often they're wrong, and more often they don't even bother to
   think. Wouldn't you want a language that sees a function, sees that
   none of its behaviour is contingent on external state, and does not
   call anything that *can* have external effects, and simply labels
   it as pure itself? Well, you can argue that that should be done in
   the compiler, not on the language level.

   I would argue that people who develop your software are mostly
   interested clever people and unnecessarily taking them out of the
   equation is just wasting their potential. I myself had had a couple
   of cases of that happening, when I realised that the optimisation I
   had come up with was far inferior to what the compiler had had in
   mind.

   Why do we bother? Why should we be able to re-implement the
   conditionals? Can't we instead go back to good, old-fashioned
   procedural. You could. What you could have, is no polymorphism at
   all. Instead, you would have conditionals that are baked into the
   process, and are optimised away as seen fit in an utterly opaque
   manner. You wouldn't necessarily avoid the penalty of =vtable=
   look-ups, because odds are that you would be creating a table and
   looking up behaviour. It would also be exceptionally difficult to
   package information. The whole point of OOP and inheritance based
   polymorphism is exposing a potential branch point in the base
   class, and packaging each of the branch behaviours with the
   definition of what the type is.

   What we have done in the constraint oriented approach, is we've
   allowed similar organisation of information, without requiring the
   overhead of class-based design. Just like in Haskell, one can argue
   that the Monads inject impure and imperative programming into the
   pure functional world, one can say that we have injected OOP into
   our paradigm agnostic approach. However, as in the analogical case,
   we have created a tool far more powerful than imperative
   programming for Haskell, for we can do OOP without much of the
   overhead of actual class-oriented languages. We do not have to have
   a different class for each of the different possible constraints in
   the branch point. We have not pulled things that effectively model
   the domain to the same level as the abstract perception of the
   branch. Just because in the one place in our code, we need to have
   a difference in behaviour between a fire truck and an ambulance, we
   didn't need to pollute the namespace. We may have instead used a
   constraint that more coherently reflects the requirements of
   municipal councils, without going into too many descriptions of a
   fire truck.

   Consider the difference between the following:
   #+BEGIN_SRC c++
     class Car{
     virtual void honk();
     };
     // File driving.cpp
     class OrdinaryCar : Car {
       void honk(){
         std::cout << "honk" << "\n";
       }
     };
     // File accidents.cpp
     class FireTruck : Car {
       void honk(){
         std::cout << "HOOONK" << "\n";
       }
     };
     // File accidents_human.cpp
     class Ambulance : Car {
       void honk(){
         std::cout << "pew-woow-pew-wooow" << "\n";
       }
     };
   #+END_SRC

   and the better option (in my humble opinion).

   #+begin_example
   (<OrdinaryCar>, <FireTruck>, <Ambulance>) .partition <isType(Car)>

   honk::Car -> String
   honk c =
     (if c::<OrdinaryCar> then "honk"|"HOONK"|"pew-woow-pew-wooow")?
   #+end_example
   Notice that in the constraint typed system we used the construct
   that seemed to be useless: our if then that only worked if the
   constraint was satisfied and caused an error otherwise. We used it
   here, so that the partitioning choice operator can correctly select
   which constraint partitioning are we talking about.

   The declaration that a car can be either an ordinary car, an
   Ambulance or a fire truck is necessary also, we do not know much
   about these constraints, beyond their names, but we have not yet
   made them into Types. Car, however, is a type, that we have not
   defined. What we can say, though is that for our purposes, if
   something is a car, it has to be one of the three. Then by using
   the parenthesis, we've specified that the order in which they're
   taken matters. This is mostly used for the same reason as we've
   used it here, to have an implicit ordering and to avoid having to
   repeat the partitioning every time. It's enough to constrain the
   first of the arguments to the first of the partitions and the rest
   shall be inferred to be the remaining partitions in the order in
   which they were specified[fn::if the orders don't match up, the
   elements in the choice operator are evaluated left to right, and
   each individual choice needs to be constrained to the
   partition. Same, if the number of choices is less than the
   partitions. More choices is an error.].

   So far the advantages are moot. It's only slightly better than
   using procedural, in which case you could have just as well used:

   #+BEGIN_SRC python
     def honk(car : Car):
         if isinstance(car, OrdinaryCar):
             return "honk"
         elif isinstance(car, FireTruck):
             return "HOONK"
         else:
             return "pew-woow-pew-wooow"

   #+END_SRC

   The difference isn't in the brevity, but in the usage of each
   concept.

   Let's say that we have two functions, both using honk, where one of
   them determines if the car is a firetruck or not, and the other
   uses polymorphic honk. Of course we determine the type of car
   purely based on some external properties.

   So in a traditional OOP approach,  we get:
   #+BEGIN_SRC C++
     Car determine(const Car& car){
       if (car.color=="red"){
         if (car.hasFireExtinguisher()){
           return FireTruck(car);
         }
         else if (car.name()=="ambulance"){
           return Ambulance(car);
         }
       }
       return OrdinaryCar(car);
     }
   #+END_SRC

   Well, now we see a lot of problems. How do we know if the copy
   constructor even exists? Well we don't. We can use a global
   enumerator, and somehow attach it to the car, but that's cheating:
   it's not proper OOP. So our dear programmer now has to think of a
   copy constructor for the Ambulance and the OrdinaryCar. This can
   prove a daunting, if not an intractable task. Moreover, because of
   the constraints set by the type of Car, we cannot simply add
   another layer of abstraction and call it a day. We have to think of
   something.

   But to be fair verbosity and high workloads are typical of
   Object-Oriented C++, and a highly productive developer will be
   quick to type those things out. Except they will now have to think
   about two other things, let's say that Car doesn't have a trivial
   Copy constructor. Does that mean that you should implement one
   yourself? Do you then need a copy assignment operator and a
   destructor? What if you need none of them, but the designer of the
   original Car class said otherwise? That's why a clever programmer
   will notice that you can win lots of performance with a static
   variable, that's shared be tween this and the consumer function:

   #+BEGIN_SRC c++
     void consumer(const Car& car){
       car.honk();
       car.honk();
     }
   #+END_SRC
   Which instead becomes much more procedural:

   #+BEGIN_SRC c++

     void consumer(const Car& car){
       switch(someInt){
       case CAR::Ordinary:
         OrdinaryCar::honk();
         OrdinaryCar::honk();
         break;
       }
       // etc.
     }
   #+END_SRC

   This defeats the entire point of polymorphic design.

   In python, the story is actually much better:

   #+BEGIN_SRC python

     def determine(car):
         if car.color == 'red':
             if car.hasFireEquipment():
                 return FireTruck(car)
             elif car.name == 'ambulance':
                 return Ambulance(car)
         return OrdinaryCar
   #+END_SRC

   And the consumer doesn't worry much about this either:
   #+BEGIN_SRC python
     def consumer(car):
         car.honk()
         car.honk()
   #+END_SRC

   Everything is neat and tidy. Except a duck (or a goose) can honk
   too. We can stop this with a guard:

   #+BEGIN_SRC python
     def consumer(car):
         if isinstance(car, Car):
             car.honk()
             car.honk()
   #+END_SRC

   Yep. We can stop it for good:
   #+BEGIN_SRC python

     class goose(Car):
         def honk(self):
             print('Hi, I\'m an armenian virus. Please enjoy your insecure software. ')

   #+END_SRC
   can defeat that. We cannot stop anyone from doing that from outside the code-base.

   Come to think about it, this vulnerability is also present in the
   C++ version. It can be mitigated somewhat, by clever use of modules
   and obfuscation, and moving things into private and friend
   fields. It's non-trivial to come up with. Sure, you can work around
   this, but you shouldn't have to.

   By contrast, in our language, none of that is an issue.

   #+begin_example
   determine::Car -> Car
   determine car =
     if car.color == red
         if car.hasFireEquipment
           .constrain car::<FireTruck>
         elif car.name == "Ambulance"
           .constrain car::<Ambulance>
         else
           .constrain car::<OrdinaryCar>
      else
         .constrain car::<OrdinaryCar>
   #+end_example
   Sure, it's slightly more verbose, and we have code repetition,
   however, that is because of the need to draw the analogy with the
   previous cases, in our language the idiomatic approach would not
   look like this, but rather:
   #+begin_example
        <Ambulance> .requires
                    <_> => (a::Car) |-> a.name == Ambulance
                    .and <_> => (a::Car) |->  a.color == 'red'
        <FireTruck> .requires
                    <_> => (a::Car) |-> a.hasFireEquipment
                    .and <_> => (a::Car) |-> a.color == 'red'
   #+end_example
   Where admittedly the lines are longer, however, now, the determine
   function can be reduced to

   #+begin_example
     determine::Car -> Car
     determine car =
         (if car::<OrdinaryCar> | car | car)?
   #+end_example
   And better yet, the consumer is
   #+begin_example
     consumer::Car -> .infer
     consumer car =
              car.honk();
              car.honk()
   #+end_example

   Now think about what will happen in all possible scenarios. First
   of all, the word =if= appears only once and only in the choice
   operator. We have almost eliminated the need for conditionals. In
   truth, they will be fully evaluated, but the compiler is free to
   choose how to order them (we forced its hand previously). In fact,
   because the partitioning statement restricts the possibilities, no
   honk implementation will be able to inject its own code, without
   also overriding (or adding a constraint specialised version) honk
   to explicitly accept the type. Effectively that's the same as
   inheriting and adding a honk method. But if that is a concern, it
   can be defeated by a familiar construction repeating the choice
   operator.
   #+begin_example
     consumer car =
              (if car::<OrdinaryCar> then car.honk(); car.honk() | car.honk(); car.honk() | car.honk(); car.honk())?
    #+end_example
   And if you still dislike the repetition, you can use the previous
   definition of the consumer as a helper, and have a single
   statement. If you see these kinds of repetitions as an
   anti-pattern, then you can (and should) create a construct. You can
   even call it partition map.

   What have we accomplished? We have prevented code injection, with a
   simple construction. Because of the semantics of the choice
   operator, the relevant constraint here is the
   Ordinary/FireTruck/Ambulance partitioning. Any other constraints
   that may apply, and potentially inject malicious behaviour are
   ousted, and irrelevant. The dynamic binding of the methods clearly
   resolves to a run-time check that can be carried through as soon as
   the information is known. The fact that we represent the
   information flow via functions, doesn't necessitate that the
   implementation follow suit.

** Conflict resolution

   Yes. But how? Well, we can start by defining some
   side-effects. Easily the most important one is without which the
   language is impossible to work with is loading another
   namespace. Sadly there's no internal representation for "this piece
   of code depends on other pieces of code". However, we can
   coarse-grain. We can say that this function needs these functions
   to work. These are the kinds of constraints that you would hope the
   compiler can figure out on its own: it only needs the names[fn::and
   types, or rather constraints needed for the functions to operate]
   of the functions. Provided that any of the included name-spaces has
   one such function said constraint is satisfied. Can you have
   multiple? Well, this is where you get yourself into conflict
   territory.

   We've just seen an (contrived) example of when a specific
   constraint is given priority. But suppose you don't want to
   highlight the correct constraint every time? Well, you do have a
   choice. The more strict specialisation of a function should be
   given a priority. So for example if you have a function square,
   that has three versions:
   #+begin_example
   square::Int -> Int
   ...
   square::Float -> Float
   ...
   square::Int<Even> -> Int<Even>
   ...
   square::Int<PowerOfTwo> -> Int<PowerOfTwo>
   #+end_example
   The obvious chain of thought is as follows. A numeric type can be
   either an Int or a Float: they partition the numbers. This means
   that the situation when a number satisfies both conditions is
   impossible. Then, the question of being of one type or the other
   type, versus being of the same type plus constraint. Of course the
   stricter case should be used. But what if you are a power of two?
   Just looking at it, and without knowing if being a power of two has
   any relation to being even, we can't just say which one is
   stricter.

   Now you may think of one more thing that the compiler may (and will
   do). Being a power of two, implies being even. More specifically:
   #+begin_example
   <PowerOFTwo> .requires <Even>
   #+end_example
   Surely this concludes that the one is stricter than the
   other. Similarly if the same is expressed with =.implies=. But we
   can't always rely on such statements being unambiguous
   indicators. Recall that we can have two constraints that are
   equivalent. Moreover, this connection may take a lot of digging to
   establish, if say the aliasing was indirect, and always happened
   through a third constraint. In other words, automatic precedence
   resolution is a problem that cannot be solved for all valid
   configurations.

   So instead we shall employ a simple tactic of leaving this
   optional. If the user wants to, the compiler is left in a state
   where it tries to infer implicit connections. When it discovers
   that it is possible to infer implication (by looking at the graph,
   identifying that there is no path other than the direct one, or
   that leads to the same exact conclusion), then it would suggest
   marking precedence explicitly in the source code, as a good
   compiler should be able to generate code. If not, the user is
   required to resolve all such conflicts either in place (by
   highlighting the relevant constraint at the call site) or otherwise
   as a general rule. The simplest form is an explicit =.precedes=
   clause (which may be added by the compiler).

   #+begin_example
   <PowerOFTwo> .precedes <Even>
   #+end_example
   Thus in every conflicting instance, the PowerOfTwo overload takes
   precedence.

   Another situation is when the number of types of constraints is so
   large, that the amounts of constraints in code can literally become
   noise. In fact, let's consider the ordering constraint.

   #+begin_example
   <lessThan(a)>: (b) |-> b<a
   (<lessThan(a)>, <greaterThan(a)>) .partition <Ordered>
   #+end_example
   We can see that if we had applied a function that periodically and
   recursively constrained the number, then the number of constraints
   on that number would grow. So if we had used the factorial, and
   started with the number =n=, then we would get every less than
   constraint on the way down to the answer. Not great.

   For that reason we shall have a constraint reduction mechanism:
   #+begin_example
   <lessThan(a)> .and <lessThan(b)> .reduces <lessThan(min(a,b))
   #+end_example
   The problem here, is that we've lost the ability to infer that if
   something is less than 5 it is also less than 6. Let's add that back in:
   #+begin_example
   <lessThan(a)> .and a::<lessThan(b)> .implies <lessThan(a)>
   #+end_example

   Let's put it all together.  Now that we know how conditionals are
   implemented in the standard library, we know that whenever a
   condition is satisfied, inside its body the constraint is true.

   #+begin_example
   a::<Ordered>
   if a<10
      a::<lessThan(10)>
      if a< 11 then
        a::<lessThan(10)> .and <LessThan(11)>
   else
      a::<greaterThan(10)>
   #+end_example

   So the same symbol can contextually obtain multiple
   constraints. Crucially, knowing in which branch of the conditional
   the value appeared, carries the condition through at the type
   level. Moreover, the nested if then, will never raise an error,
   because the constraint implies the one it needs.

   Thenceforth, the linter shipped with the language tools, will
   provide you with lots of useful information. For one, looking at
   the layout of the code, we can see that the check =if a<11= is
   extraneous. There is no conceivable way that the constraint is
   lifted by the time we reach the nested =if=, moreover, this
   constraint is implied at the time when it was written. Naturally,
   if we actually had two language-keyword conditionals, we would have
   to have the optimiser think about these sorts of things, and have
   something that has no conditional jumps despite there being no
   necessity to do so.

   Our language tool, however, might simply suggest that the extra
   conditional is redundant, and can be removed (and remove at compile
   time, in the source code). Not great, but certainly not terrible.

   The issue is that the optimiser is always fighting to make the
   binary simpler, but almost never to make the code simpler. We
   should strive for code simplification first, and binary
   simplification we shall obtain as a consequence of simpler, better
   code. The overwhelming reason why that is not the standard, is
   because people cannot agree on what better code is. I'd argue that
   the best way to fix this is to add an deactivation token.

* Programming in the modern day.
** Comments, deactivation, and documentation

   There are three types of comments that are currently in use by the
   public at large. One of the most common types and the one that is
   actually enforced by enterprise (the lame one), is a description of
   what a function's function is to the functionality of the
   code-base. It may occasionally be mistaken for describing the
   invariants of a datatype, but normally those are neatly encoded in
   the name of the datatype (c.f. =OutdoorsOberverSingletonController= as
   opposed to =WindowBlinds=), with comments mostly reflecting the
   programmer's musings on their years of college education, where they
   learned asymptotic notation.

   These are also sometimes included in the description of
   tests. Surely the test is too complex to map to the description that
   is provided.

   Among the positively brilliant design decisions, such comments are
   sometimes represented in languages as doc-strings. Thus instead of
   being used as some things that are simply ignored by the compiler,
   they serve an intent: to acquaint the user with the
   functionality. Sadly, in some cases, the doc-strings, which are
   strings, are exempt from linting. The most important linting tool,
   which is "your string tells the programmer next to nothing", is
   relatively hard to write, as that would require someone more capable
   than the end programmer to understand and correct for. The second
   most important one, is to check if the information is up to
   date. Consider that this is functioning Python code.

   #+BEGIN_SRC python
     def helloWorld(*args, **kwargs):
         """A combinator of the hello and world functions.

         Parameters
         ----------
         *args : hello's args

         **kwargs : World's kwargs


         Returns
         -------
         out : Stuff

         """
         return hello(**kwargs) + world(*args)
   #+END_SRC
   Fun! It's also remarkable that because Python's type system is
   hamstrung to such an extent by the combination of being duck typed
   and interpreted, there's no way to tell if the function operates
   normally, can cause issues or whether or not the typo was in the
   documentation or the name of the function.

   For another example of doc-string abuse, let's see
   #+BEGIN_SRC python
     def incabulator(incabulee, workbench, incabulationStrategy=None, **kwargs):
         """The famous incabulator function.

         Parameters
         ----------
         incabulee : object
         What needs to be incabulated

         workbench : int
         Which workbench to incabulate on.

         defraculator : object, optional
         How to defraculate. Could be a callable, or a string.

         incabulationStrategy : callable, optional
         callable or string.

         Returns
         -------
         out : Stuff

         """
         incabulee +3
         if workbench.hasHammer():
             workbench = 15
         else:
             workbench = incabulee

          return kwargs.pop('defraccculator')(workbench) + incabulationStrategy - 22
   #+END_SRC

   So in this example, almost every type hint is violated, and the name
   of the keyword argument doesn't match the doc-string. How long would
   it take you to figure this out? if the defraccculator argument was
   supposed to optimise some behaviour, because of the placebo effect
   of the end programmer, you might never know that it doesn't work! I
   don't need to tell you that this defeats the purpose of doc-strings.

   Surely if the documentation strings were part of the source code,
   just freely intermixed with prose, one could avoid having to worry
   about such issues. So it pays to have documentation as a separate
   syntactic unit. And it is not at all difficult to implement, just
   look at emacs. If you try to write code that does not conform to
   Richard Stallman's blessing, you will be notified. And that's in a
   language, where the type of something is a run-time
   characteristic. It's of course a question of whether or not this
   should be provided as the language tooling-level element, but... as
   many will know, the programming (cool) text editors support
   microsoft's Language server protocol. Perhaps, such linting may be
   useful.

   The second major type of comments is a comment that clarifies the
   behaviour and a particular choice of data-structure or function, or
   the particular function that a non-trivially interpret-able blob of
   code can have. Example:

   #+BEGIN_SRC python
     def find_symmetric_subgraph(graph):
         _symmetric_subgraph = defaultdict(set)
         inverse_graph = defaultdict(set)  # adj.T
         for player in graph.keys():
             try:
                 for visible_player in graph[player]:
                     if visible_player in inverse_graph.get(player, set()):
                         # Bingo! We found a bidirectional edge.
                         _symmetric_subgraph[player].add(visible_player)
                         _symmetric_subgraph[visible_player].add(player)
                     inverse_graph[visible_player].add(player)
             except KeyError as e:
                 # Strictly speaking there could be concurrent
                 # modification, which could also cause KeyError to be
                 # raised. However in that case, as in this case, this will
                 # signal data corruption of a specific sort.
                 raise UnknownVertexInAdjacencyList(
                     {"message": "A name appeared in the visibility list, but"
                      "does not appear to be a vertex of the visibility graph.",
                      "unknown_player": e.message()})
             return _symmetric_subgraph
   #+END_SRC
   So this is sort of a dialogue with the programmer. There is
   reference to language specific constructs, that is not being
   checked. I could have refactored the code, and changed the name of
   things in the comment. So the simplistic attitude that a comment is
   something that the program compiler/interpreter has nothing to do
   with is... simplistic.

   In this particular case, our language should allow for inserts of a
   particular kind. This is a documentation string, but it's
   documentation of a different kind.

   In the previous example, the programmer was only interested in the
   inputs, the contracts (or the constraints) associated with the
   inputs, and the output. Here we are dissecting the source code of
   the function, and should be able to see what the code really does
   under the hood. In this particular case, there are two comments: one
   that signals that the desired behaviour is reached, another that
   signals that during the process of construction, the inverse graph
   is going to be the transpose of the adjacency structure of the
   forward graph, and one more comment stating that a specific choice
   may not be the best.

   For one, I would argue that the adjacency comment is an
   assertion. Sure, we don't want that assertion to run all the time,
   and so we deactivate it. The comment about the desired result can be
   communicated, but I'd argue that if the desired result isn't
   reached, in an overwhelming super-majority of cases, then that is a
   bug. So this is not quite a "useless piece of communication with the
   programmer", kind of comment, this is a "should be a language
   feature" kind of comment.

   Lastly, the block comment right before the raised exception is not
   entirely useless either. If the exceptional state arises, the end
   user should be made aware of this comment, i.e. if a back-trace is
   produced, the block comment should be included. The issue here is
   that the conventions on exceptional states in python are not
   entirely clear.

   Almost everything that can appear in an except clause is a value
   Error. Ideally, there should be a field in the =ValueError= type, that
   says 'comment', and allows the author to (in prose) express the
   sentiment about this particular exceptional state. My issue with
   (even this) solution, is that the comment may refer to the exception
   being raised in a particular section of the code, and not
   necessarily have to be carried through the call stack.

   The final type of "comment" that I'd like to cover is the
   deactivated snippet. Normally, because most text editors have the
   nasty habit of not having good registry management, and finding the
   right piece of code that you have deleted a day after your current
   session may be intractable, the programmers more often than not,
   comment out sections of code to deactivate them. Not a bad
   practice. But the usage of comments for this purpose has the
   following consequences. The deactivated snippet is indistinguishable
   from a doc-string example. Moreover, suppose there was a sweeping
   change in a graph theory code-base. Someone might have thought, that
   using the word table instead of Matrix, is not the best idea, so
   they replaced every symbol's name (refactored rather), that has
   table with Matrix. A =HashMatrix= is a normal type, an
   adjacency_matrix_from(adjacency_list) is a perfectly valid function
   signature, however, if said snippet was commented out, most
   refactoring tools would probably not touch it. Sure, users of
   QtCreator would say, ``mine does'', but the overwhelming
   super-majority does not. You should, as a programmer not be at the
   behest of your tooling. Or rather, the tooling that permits the
   usage of the language should provide you with the option to perform
   the refactoring according to its internal logic, rather than
   external opinion.

   More importantly, the macros in C and C++ (up until recently) have
   had the intention of allowing to programmatic-ally activate or
   deactivate snippets. So why on earth would you have two ways of
   doing the exact same thing? Well, for one, because the deactivation
   using macros is verbose. You can achieve the same effect with proper
   configuration of your text editor, e.g. instead of comment on
   =Ctrl+/=, surround with a false =ifdef=. But it is a luxury that only
   the languages cursed with Macros can enjoy.

   So, our language does not support comments. Instead it supports
   anonymous free-floating strings, that act as documentation, and
   syntactic conditional activation. So if you want to test two
   functions of the same name, you can put deactivation tokens that
   look like the following.

   #+begin_example
   factorial::Int->Int
   factorial n = n * factorial (n-1)

   /*
   factorial::Int-> Int
   factorial n =
      "This is an example of an explanatory doc-string.";
      "While this definition is certainly true, it will not compute. ";
      factorial (n+1) / n
   */

   /* named_deactivation
   factorial::Int->Int
   factorial n =
       GammaFunc (n+1)
   */
   #+end_example

   Which is what is needed in the source code. Of course the tools
   would allow you to deactivate the factorial name altogether, by
   using the =--deactivate fun:factorial= switch, and re-activate the
   two disabled ones, using the =--reactivate name:named_deactivation=
   and =--reactivate line:4= and/or =--reactivate file: example_file=,
   to re-enable the second and third.

** The troublesome boilerplate.

  The expressions follow the similar pattern to many languages, where
  the function's referential value is the final statement that is not
  terminated by the semicolon.

  Thus, instead of having mandatory boilerplate that has no
  significance, we provide mandatory boilerplate that we use in the
  language, that loses its significance.

  Why do I have a problem with the mandatory semicolon in most
  languages? Well, mainly because the boilerplate has no informational
  significance. Sure, the semicolon tells the compiler that the
  statement has terminated, but there's a slightly subtler point. Can
  the programmer stop thinking about where to put the semicolon. If
  the answer is yes, then the odds are that the programming language
  had offloaded some of the overhead from the compiler onto the end
  user. Worse, it could have been done away with in almost all
  circumstances, added as an option, you might suggest, but was kept
  mandatory. The only circumstance where I would find this acceptable,
  is if I could put something else there, that would have meant
  something different, and the semicolon is there to clarify.

  Consider, what would happen if I instead used a comma.

  #+begin_example
  factorial::Int -> .infer
  factorial n = print n, n, factorial n-1; n* factorial (n-1).1
  #+end_example
  The result of the factorial function would not be an Int. It would
  instead be a structured ordered datatype. (factorial 1).0 (access to
  the zeroth element), is the result of printing. Normally, you can
  think of this as the string, but I would argue that a structured
  datatype would be more useful. (factorial 1).0.0 would be the
  value 1. so would be (factorial 1).1. The value at (factorial 1).0.1
  would be the beginning location of the output in the console stdout
  buffer. So if you needed to adjust this value dynamically, you
  could. Interestingly, the value (factorial 2).2 is a funny little
  thing, it's the value 2*1 = 2, not the value 1. Of course, the value
  (factorial 3).2 would be wrong. it would be 3*2 and no 3*2*1.

  So the comma means, add to the structured return, and the semicolon
  means remove from it. OK, this sounds interesting. So we have options.

  But that's not all. We can use curly braces too:
  #+begin_example
  factorial n =  {n* factorial(n-1).0, GammaFunc(n+1)}
  #+end_example
  What this means is that the collection has no implicit
  ordering. What this also means is that the return type will also
  contain either the =GammaFunction=, or the recursive
  definition. Whichever is done first, is put in the 0-Th place,
  assuming the start is mostly parallel.

  The semicolon is useful here too: one can say that it is the most
  useful here.

  #+begin_example
  factorial n = {n* factorial(n-1).0; Gammafunc (n+1)}
  #+end_example
  The structure will have only one index, zero, which will be filled
  by the first of the two possible results. Which is the first? The
  one that computes the fastest. With the general consensus being that
  the order of evaluation of operator expressions should generally be
  right to left, excluding parenthesis, and operational ordering, this
  is consistent with the semicolon in the previous case. That is
  remarkable!

  So we've turned a chore of putting an unthinking semicolon into a
  general decision. It's not that you have to put a semicolon at the
  end of a statement because you have to and it makes the compiler
  read more easily, it's that if you don't, then he compiler will do
  something else.

** Built in Lists arrays and structures.

   Now you might think that since we reserved (implicitly)
   parenthesised collections for tuple-like structured objects, and
   curly braces for disordered collections, shouldn't we then reserve
   the square brackets for lists? Well... see for yourselves.

   Let's suppose that we have a list of names, alphabetised and kept
   in a structure, call it =namelist=.

   #+begin_example
   namelist::List(String)
   #+end_example

   This is a concrete type, with a specific parametrisation. Suppose
   we transmitted it over the network. Do we want the collection at
   the other end to be ordered the same way it was ordered here?  One
   would say, that we should go out of our way to preserve the
   alphabetised ordering, because the end user might want to have it
   ordered that way.  Another one, might argue that the ordering might
   not matter if the consuming process does not need the collection to
   be alphabetically ordered.

   Fortunately, the programmer doesn't much need to worry about this,
   if the consumer function has the requirement <Alphabetised> then
   the transmission will also require this at the receiving end, and
   will require the serialisation library to provide a way to
   reconstruct the alphabetical ordering, or will alphabetise the
   collection itself. So the decision is really up to the API and the
   constraint system, not the programmer.

   But what form on earth will the list literal have? Well, who told
   you that we would even have such a thing? OK, how about a tuple? No
   again.  Let me explain. When programming in a language such as
   Standard ML, Haskell, Python or any other language that has a
   built-in syntax for a list, what we're actually doing, is finding a
   simple and reliable data-structure that can keep an ordered and
   fixed set of elements (a tuple), and a flexible and ordered set of
   elements (list). The specific needs that these meet might not be
   quite as straightforward, as what the data-structures were designed
   for. Moreover, it is quite possible that the programmer had used a
   tuple in Haskell for a heterogeneous flexible data-structure, and
   since returning a tuple versus incrementing a list, from the SLOC
   point of view is similar in nature, doing a potentially linear
   increase every time a list element is to be added is better than
   adding an extra line of code to an overdue project. In the case of
   functional Languages, where almost universally the data-types are
   all algebraic, because the list =[1,2,3,4]= is much easier to
   interpret than =(Cons 1 (Cons 2 (Cons 3 (Cons 4 (Nil)))))= which is
   mostly noise, a certain syntactic shorthand was preferred. Python,
   widely regarded as one of the wisest languages in the world, has as
   usual copied the design without understanding the issues for why a
   specific design decision was made. Perhaps I'm being too harsh on
   it. Perhaps I realise that Python objects, being what they are
   could have just as neatly fit into a re-sizeable =arrayList= that
   doubled in size.

   Other languages that did not have to contend with immutability, or
   at least not to the extent to which functional languages did, had a
   natural data-structure of choice. An Array is the default for
   storing both old (C-style) strings, and a type of data-structure
   that is built into many languages. Despite the well-developed
   standard library and a slew of built-ins that only operate in on
   the low-level outside the code, Java has support for arrays beside
   =ArrayLists=. I would hesitate to appeal to Java's authority, as
   mistakes have been made, and attempts to make things better had
   been mostly unsuccessful, but if Java survived making this decision
   then there must have been a good reason for it.

   But our language does not believe that the low-level description is
   necessary. Yes, you can hand roll a loop that is as efficient as
   the C-generated code, and yes, you can make it work as reliably,
   but it would take you far too much time to do so. What seems to be
   the driving force for adoption is a balance of language "goodness"
   versus infrastructure. Rust is being incredibly successful, because
   it was built to support a single, efficient and standardised
   infrastructure. Python, is successful because the ease with which
   new code can be written, and the infrastructure of moderately
   well-behaved libraries, that outweigh the terrible design of the
   language. When I was writing my Bachelors computing project, most
   people chose python because writing things in it was easier. When I
   was writing my masters' thesis, I had to use Python because
   everyone else was using it, despite being more than capable of
   writing FORTRAN or C that did the same things, more efficiently and
   more reproducibly.


   Our language shall take a different approach. In most of the
   aforementioned languages, the architecture of the datatype needs to
   be presented explicitly, and the choice cannot be made
   automatically. Haskell cannot deduce the fact that I need a fixed
   length array, because I have a constant that I have no plan to
   modify, and instead uses a list. The API for collections is
   under-defined and often overdeveloped. In Python there is no way of
   getting a random key value from a dictionary. You are forced to
   choose from popping and putting back, iterating and immediately
   breaking, or worse, converting the dictionary to a list and taking
   the first element. Finally, a graphical application is not
   bottle-necked by performance, it is bottle-necked by experience and
   scheduling. Most time is spent reading and dragging the mouse,
   which can be used for maintenance tasks and decision making, while
   the user is busy trying to click a tick-box with his meat joysticks.
   In other words, there is incentive to spend that downtime trying to
   figure out the best and most fitting data-structure on the fly.

   So We relieve the end user (the programmer) of the duty to choose
   the data-structure. Instead, we ask them to point out the relevant
   operational features. So when you are preparing a collection of
   values for use in a function, but not as a collection of symbolic
   name, but rather a structured datatype, you specify them in
   parentheses. Whether or not, the underlying representation is a
   record, a linked list, or a complicated tree is irrelevant, because
   the ordering of the elements is the only thing that needs to be
   preserved. Similarly, if one simply needs a bag of holding, that
   doesn't have to preserve the ordering of the data, it can be shoved
   into an unordered data-structure. So let's say that we have
   Dijkstra's algorithm and we need to pre-heat the oven so-to-speak
   with a Fibonacci heap? Well, ask yourself, is the ordering of the
   elements preserved, i.e. do things swap places at any place other
   than popping off of the ends? Well, they do. Hence, this is an
   unordered structure. The way you would initialise it, would be.

   #+begin_example
   {1,2,3,4,10, 5,6 }::FibonacciHeap<Queue<Priority>>
   #+end_example
   Moreover, picture if you will, a program where you need a list only
   to collect the function parameters one by one. If you really wanted
   to avoid currying, (which our language supports), you could do
   something like

   #+begin_example
   args= (arg1, arg2, arg3)::List
   args.add(arg4)
   args.add(arg5)

   function (*args*)
   #+end_example

   where the comment symbol from standard ML is a curried explode,
   designed specifically for expanding an ordered collection into a
   curried function call. It can also be used for partial application.

   What do we do with a list of lists? The same as we would otherwise:
   #+begin_example
   (list (list, of, lists))::List
   #+end_example
   And let's not forget that the List datatype actually has very
   little in terms of concrete behaviour. It doesn't have to be a
   =LinkedList=, but if the usage suggests that it might be useful, it
   will be. But there's also the option that the compiler can
   constrain the usage of the list to be as an immutable collection of
   mutable cells. Surely you could save some memory this way. And best
   of all, the programmer doesn't need to lift a finger.

** Choice and concurrency

   The eagle eyed will notice that we've used parentheses to identify
   options in a constraint selection: the question mark operator. It
   stands to reason that the parentheses have their usual meaning, and
   that the choice of parentheses is somehow significant. Indeed it
   is, as using the curly braces relaxes a few of the requirements,
   but are also less predictable.

   So, an expression like this one:
   #+begin_example
   {a | b | c}?
   #+end_example
   implies a selection between function a, b or c, but no guarantees
   are made of the order of evaluation.

   While we're at it, we should also note that the pipe symbol also
   has special meaning. It is a strong guarantee that the constraints
   are non-overlapping. The strong guarantee needs to be expressed as
   a constraint partitioning. If no such statement is found, this is a
   compile error.

   Of course, it would be incredibly frustrating if we only allowed
   the use of this operator for partitions. So we can allow relaxing
   the constraints if we have constraints that mix and constraints
   that don't, we can specify them as follows:
   #+begin_example
   (odd &| prime)?
   #+end_example
   or
   #+begin_example
   (odd &| prime &| even)?
   #+end_example
   which is equivalent to:
   #+begin_example
   ({odd | even}? &| prime)?
   #+end_example
   but not
   #+begin_example
   (odd &| prime | even)?
   #+end_example
   the reason being that 2 is a prime.

   This also maps onto natural language, where by default every =or=
   is actually an exclusive or, and an inclusive or is usually
   represented with an and/or.

   Moreover, there is an important point, there is a difference between
   #+begin_example
   (odd &| prime &| even)?
   #+end_example\
   and
   #+begin_example
   {odd &| prime &| even}?
   #+end_example
   namely, that the latter reads, it can be odd prime or even all of
   which can overlap, while the former implies that a thing can be odd
   and/or prime, and it can be prime and/or even, but not odd and
   even. This also leaves room for semantic misinterpretation of:
   #+begin_example
   {odd | even &| prime}?
   #+end_example
   because on the one hand, the order doesn't matter, but on the
   other, reordering the terms in the expression changes the position
   of ors and and/ors. In this particular case, this leads to
   ambiguity, and hence needs to be resolved manually: the programmer
   has to write this out explicitly as:
   #+begin_example
   {{odd | even}? &| prime}?
   #+end_example

   The reason for these specifications is twofold. First of all, the
   compiler can safely assume that the operations within the curly
   braces are possible to re-order preserving the semantics. The
   implication is that data-structures that might not have the same
   layouts, but the same types of fields are equivalent. Secondly, if
   an operation can be done efficiently, but would result in
   re-ordering, the compiler is told to do this freely. Thus a
   particular case of
   #+begin_example
   {a + b +c}
   #+end_example
   implies that the operations can be re-ordered to be more efficient,
   and the result would still be valid. And while the compiler is
   always free to do such things, when it is told
   #+begin_example
   (a + b + c)
   #+end_example
   it is effectively restricted to re-orderings that would preserve
   the result[fn::while it is common to assume that the floating point
   numbers are commutative, this is sadly not the case, and
   re-ordering operations may lead to different approximations. ].

   This has a profound implication on what the compiler may assume:
   everything within curly braces, and some things withing parentheses
   can be done concurrently. So a Fibonacci sequence function of the
   form:
   #+begin_example
   fib n = {fib (n-1) + fib (n-2)}
   #+end_example
   is free to spawn threads at each evaluation, though is (thankfully)
   not likely to do so.  Even if the program is impure, the
   differences in value are presumed to be within acceptable limits,
   thus allowing more fine grained control over optimisations.

   Sadly this may lead people to falsely expect that a familiar block structure like, e.g
   #+begin_example
   fib n =
      {
      if n == 1
      then 1
      else
        fa = fib n-1;
        fb = fib n-2;
        fa + fb
   }
   #+end_example
   would compile. It won't. The order of evaluation of things within
   the construct might not matter, but within the else block, there is
   a strict dependency of =fa+fb= on the names bound as =fa= and =fb=,
   which contradicts the programmer.

   Unlike most compilers, and similar to LaTeX, our compiler shall
   propose that the things that can be reordered be re-ordered, and
   wrapped in parentheses.

** ``Is a'', vs ``as a''

   What this does, is promote an attitude towards values, where what a
   thing is, is less relevant than how a thing behaves. Some of the
   constraints can be specific enough, so that we can reduce the
   possible data-structures to 1. They can be too tight, at which point
   the programmer has to think which constraint based optimisations
   does she need to sacrifice. And to some extent the compiler is also
   free to choose the data-structure without having to ask the
   developer. That does not however, mean that the developer is
   removed from the conversation entirely.

   If I wanted my code to behave exactly as I want it to behave, I
   would write it out explicitly.
   #+begin_example
   a = 5 ::Int<BuiltIn(8)><Unsigned>
   #+end_example
   which can be abbreviated:
   #+begin_example
   Int<BuiltIn(32)><Unsigned> .abbr Uint32
   b = 3::Uint32
   #+end_example
   If need be standard abbreviations can be made standard. However,
   the programmer has access to the original constraints, and can see
   that =<Unsigned>= .implies =<Positive>=. Instead of using an obscure
   header, where one needs the Wikipedia page to find out which
   numeric limit means which thing, the built in tools, provide all of
   the constraints on the values. So a =UInt8=, would be
   =<LessThan(256)>=. Neat, huh?

   Now if you are trying to squeeze out as much memory out of your
   machine, and you knew that your tool had mainly used the values of
   12, 16, 18, 24, and 32, you could say: Hm, I ought to use a smaller
   int, but do I necessarily have to do that explicitly? Well, safety
   and concrete types dictate - yes, you should be explicit. However,
   if our language is possible to implement, you would be wasting more
   space. See, because the language is constraint based, the internal
   state of that enumeration does not matter. For all we know these
   may be represented with five constants and a counter, that simply
   indexes the array of constants.

** The mass exodus ab class ad concept.

   An important point is to note that the representation of a value or
   type is a distinct entity vs belonging to a type. When we look at
   an object, in our natural tongue, there's the notion of primary
   belonging, and subsequent classification. So, we see a Panda, a
   Human, a Police Officer, and subsequently we classify them as Male,
   Female, On-Duty, Off-Duty and so on. The somewhat unfortunate, I
   would argue problematic history has lead many object oriented
   languages to confuse object orientation with class-related
   hierarchy. Allow me to elaborate: Objective C, which is an
   evolution of Smalltalk is a language that was based around the idea
   of messaging and obfuscation. I myself am a little sketchy on the
   details of what exactly was considered part of the idea initially,
   and which sprouted as an offspring; to my eye, late binding is a
   signature of object orientation, as is the ability to signal an
   object in multiple ways.

   Every language used to program supports objects, for Excel these
   are confined to cells, and for C++ there are objects that are
   literally instances of a specific class. Not all languages that
   support classes are object oriented, and not all object oriented
   languages have to have classes. Lisp doesn't have that as a
   keyword, but thanks to a combination of fortuitous coincidences,
   you can write something which steers exactly towards object
   oriented code. By virtue of a few unfortunate decisions, being
   carried out in a systematic fashion over the span of its entire
   existence, Python is a counter-example. While it's true that you
   have inheritance, and that you can have obfuscation if you feel
   like it, an object is a thin shell for designating tangled webs of
   state. One might expect that I would be critical of JavaScript, but
   despite its best efforts, it does maintain one of the key points of
   OOP. The objects are a sharp delineation of singular state. you can
   weakly couple them to external state, but it is non-idiomatic,
   hence not standard practice.


   So why was OOP invented? It was an attempt to impose stricter
   requirements on code state. You can have state and that state can
   mutate, but only in a specific fashion, and according to specific
   signals. One takes most of the time to delineate the permitted and
   excluded signals. If C++ had not had a facility to designate
   classes; object orientation could still be achieved with closures
   to restrict scope and accessibility, and signalling via the strong
   typing, albeit instead of a neat: things in → things out, system,
   we would have ordering ambiguity.

   But why classes and inheritance? It seems like most OOP-centric
   languages universally have this concept, but for some unknown
   reason it is discouraged. As Bjarne Stroustrup likes to point out,
   every language that uses class as a keyword is a descendant of
   Simula, which was used primarily for scientific
   simulations. Science, and Physics, in particular, relies on rigid
   and linear tree-like hierarchical classification. A Panda is also a
   Mammal, a Dog is a mammal, but not a Panda. And anything that isn't
   a Mammal, cannot also be a Panda either, which allows you to neatly
   carve out islands of influence; Mammals and only mammals, hence the
   name, can feed their young with Milk, or birth their young after an
   extended pregnancy. Same, can be said of Planets, Stars, and many
   other things. This Language level support for classification
   allowed people to manage the dependencies, the behaviours, and most
   importantly interactions in a predictable fashion.

   Or at least that's what happens at a low level of
   complexity. Consider a Platypus. Is it a mammal? Are you also sure
   that a Panda is a bear or an overgrown Raccoon. Is an
   Australopithecus a Human? Is a Gorilla a descendant of the
   Australopithecus? And if not, why do so many people believe that
   humans evolved from the modern day apes? The shortest answer is,
   because the system doesn't quite describe the world as it
   should. Instead it describes a mostly idealised strictly
   hierarchical version. The problem is that even the natural simplest
   systems don't quite work that way. The main reason for why a class
   oriented object-orientation isn't working, is because, and given a
   bit of thought it's true, some classes are sufficiently broad that
   in order to guarantee that every instance of it has the same
   behaviour, the behaviour needs to be confined to single function,
   sometimes with caveats. It's not only impossible to project single
   behaviours onto the domain of the problem, but it is also sometimes
   impossible to project the presence of a property onto the object.

   Different languages have taken different approaches to this
   problem. Some like C++ have hoped that the programmer is clever to
   find a way to look at the domain of the problem and map the
   behaviours onto a neat system. Surely you would never be asked to
   solve a problem that involved any run-time modification to the
   structure of the class instances? Unfortunately, introspection has
   its uses, as does the ability to dynamically change the layout of a
   structured type. So the programmer had used a hack, that mapped
   strings onto object types and used this to emulate the
   fields. Syntactically, there's little difference between you typing
   =a.m_('field')= and =a.m_field=. This is a very good hack, that became
   standard in Qt. It is also the main source of frustration when
   implementing a =QAbstractItemModel=. Because the runtime lookup is
   handled through the facilities not in the core language, it is up
   to the library developer to expose the information, often two or
   more times. The upshot is that it works, and works remarkably well
   when it works. The downside is that it is effectively the proof
   that the object oriented features of C++ are insufficient to carry
   out true object orientation. Signals and slots, which are stand-ins
   for methods and interfaces also double a facility that's already
   there and essentially ignore it. You gain the ability to do what
   you want, but the very reason why these features: inheritance,
   polymorphism, and many many others were originally introduced and
   made the language as complex as it is, have largely gone unsolved.

   Java, as is common in the corporate world, thought that they
   understood the problem, and very quickly came up with a
   solution. The issue with C++ stems from the fact that it supports
   class orientation, but it doesn't fully utilise it to its fullest
   potential. Templates for some insane reason work no better than
   macros, why do you have a system that resolves overloads and does
   dynamic polymorphic virtual function resolution, when you ignore
   all of that in your generic data-types. Why not do type erasure?
   Long story short, this didn't work out, and caused many more
   problems. The issue was that you essentially asked your type system
   to take 5, and did a lot of dirty work unsupervised. Problem was,
   that humans are error prone, and the type erasure seemed like
   something that you could easily dose off to during the lecture. So
   people weren't even on high alert when doing the dance with the
   flaming chainsaw juggler. What can I say, clowns don't seem too
   threatening, until they are the scariest thing in the world by a
   long shot. In the end, because of Backwards compatibility, Java
   designers had to take the walk of shame of and adopt syntax that's
   essentially the same as C++, but worse, and a feature that does the
   same as the C++ templates, but far worse, in a language where
   everything is a class.

   Thus, we effectively end up with two different languages, with ab
   initio very different philosophies and purposes, converging on the
   concept-driven design.

   Java introduces Interfaces to express the abstract idea of
   conforming to a collection of assumptions, and Interfaces are the
   only datatype for which the declaration on the left doesn't match
   the constructor on the right. C++ came to a very similar system,
   where the template arguments can and should be restricted, but
   because the current system that utilised template meta-programming
   was too bulky, to reduce the overhead on the programmers, they
   decided that it wasn't such a bad idea to make the requirements
   explicit and easy to understand. It is also coincidental, that
   whenever a heterogeneous collection of objects is to be stored in a
   single data-structure in Java, the interface representing the
   collective set of constraints is given in angle
   brackets. Similarly, in C++, the template arguments and concepts
   mostly reside within angle brackets.

   This brings most people to a Eureka moment for why the syntax for
   constraints in our language is such as it is.

** Interlude programming conventions vs programming by convention

   The issues don't stop there. Java is enamoured with the remarkable
   tidiness of a hierarchical type and object and module system. When
   you need to import the standard library, you have to dig into the
   library's path, with unique identifiers, the reverse domain name,
   and many many other things that seem like a good idea in a
   corporate environment, but adds very little in the long
   run. Conceptually speaking, the programmer is faced with a
   stumbling block: If I need say the =ArrayBlockingQueue=, which is
   more a description than a name, they not only have to remember the
   name of the data-structure, which is far longer than it needs to be,
   but also to remember the path to it. "Is it =Java.Util=" or
   "=Java.Lang="? Hm. I don't have an issue with this
   system. Remembering the path isn't a big deal. I use this to
   exemplify how deep our fascination with hierarchical organisation
   goes, and how central it was to Java's philosophy.

   The result of such fascination is that the program's source code
   was no longer designed to be written by a lowly human being in
   something that's little more than a text editor. Now, your code was
   to be intelligently generated by an integrated development
   environment, and the language was designed to be easier to read by
   the IDE, in some areas, rather than a human being. Tooling over
   language features is rather economical, the tools can be designed
   and tested separately, they don't and cant' break existing
   code-bases (not intentionally), they are only used as guidance, and
   as long as the human driving the highly sophisticated autopilot,
   the autopilot can do most of the work. Make no mistake, this is not
   critique, not in the slightest, as I think that offloading most of
   the repetitive tasks onto the machine, while preserving a semblance
   of human readability, is an important factor. The issue with Java
   is that the language had a standard, while the tools did
   not. Because of that, some code-bases have sprouted
   adaptations. Since you couldn't guarantee that all fields of a
   class had been highlighted differently than non-field variables
   (inside e.g. functions), the fields had to follow a naming
   convention. The trouble is that the naming convention had to follow
   the personal taste of the team leader: some would say that it has
   to be =mName=, someone else might insist on =m_name=, and some
   would prefer the objective C approach of =name_=.

   Are these sorts of conventions bad? Yes and very much so. Consider
   the convention to reserve capitalised names for types. Some IDEs
   may warn you about following the wrong convention, some might
   not. So there's no a single guarantee that a library is written in
   any way that is appropriate for use: the developer may have missed
   one such case because the IDE hadn't warned them, or it may be
   because they were too tired, or they dislike the convention, and
   the code is a way of protesting against the exponentially more time
   consuming convention. In Haskell, if you use a lowercase name in
   the name of a type, it simply won't compile. So a convention that
   is not enforced on a language level is like a mushroom that's 95%
   safe to eat. Even though the chance of it being poison are slim,
   because you cannot afford to die even once, eating such a mushroom
   isn't a good idea. If the purpose of a guideline is to enforce a
   particular style, then the ability to break a guideline and the
   sheer amount of pedantry required not to break the guideline are
   simply casualties in the war against chaos, that started with
   friendly fire. These guidelines seem moot, in the instances where a
   stricter approach would have been much better. And the answer isn't
   always in *new* language features: the =m_= convention can easily
   be replaced with the =this.= convention: because the only context
   under which a field can be confused with any other variable is
   within a member function, which can typically use =this.=, and have
   the added benefit of enabling the same infrastructure in the
   tooling that checks for misplaced attributes, to check if a field
   had been added to the object. Moreover, I cannot for the love of
   god, create a local variable with a similar name, and confuse
   anyone who might be looking at the code, into thinking that
   =m_name= is a local variable of the function, because if
   =this.name= isn't declared as a field, every instance of
   =this.name= is a reference to an undeclared identifier.

   Programming by convention is one of the cardinal sins of modern day
   software architecture. Repeated patterns are ingrained into the
   human brain, and people who do programming as a supplement to their
   main speciality, do have a nasty collection of habits that make
   one's life miserable, and other's lives worse than living death.
   Our language, thus needs to be able to address these issues at the
   language parsing level, before they arise. So, we shall adopt the
   following conventions as syntactic rules.

   1) Data-types are all capitalised. Similar to Haskell.
   2) All functions are implicitly curried like in Haskell.
   3) All variable names start with a lowercase letter, and cannot
      begin or end with a number.
   4) All member access and sub-scripting operations are done with a
      dot between a name of a structured type, and the identification
      of the field. For example, the number of the entry in an ordered
      collection of elements is an identification. A pure function
      that takes a single (non-function) argument of the type that is
      precisely the datatype, and returns a single value of that type,
      can be considered an identification.

      EXAMPLE:
      #+begin_example
      max :: Pair of Int -> Int
      max Pair(a, b) = (a::<GreaterThan(b)>|b)?

      Pair(1, 2).max
      #+end_example
   5) No space is allowed between the dot and the identification.
   6) Single quoted strings are non-literal, and accept substitution,
      character escapes, along with any code substitution/formatting.
   7) Double quoted strings are literal. Everything inside them except
      for the double quote is rendered exactly as written, including
      newlines, offsets and tabulation. If a literal double quote is
      needed, it's prudent to use string concatenation:
      #+begin_example
      "this will contain the word *max* verbatim.
      This will be on a new line." " " "this string will be concatenated to the previous."
      "This string contains a double quoted quotation" '"' "This is the quotation" '".'
      #+end_example
   8) numerical prefixes are implicitly multiplication, while
      numerical post-fixes are implicit subscripting. If the following
      identifier has no operation separating it from the numerical
      post-fix, it's a warning, that can be silenced by making the
      post-fix explicit.

      #+begin_example
      stuff = (1,2,3,4,5)
      .assert stuff1 == 1
      .assert 2stuff2 == 4
      .assert 3stuff == .undef
      #+end_example
   9) All constructs are in uppercase. [fn::TODO retroactively patch every construct to be uppercase]
      #+begin_example
      IF x>3 THEN
      .assert x>3
      #+end_example
   10) Mathematical operation symbols have their C-style usual
       meaning, except right and left binary shift operations. Power is denoted by ^.
   11) Spaces have to surround the usual addition and subtraction
       operations; logical or, but have to be omitted for any higher
       precedence operation except &. Equality test is == and is surrounded by space.
       #+begin_example
       (1 + 2*3 + 4^5) - 6 | 1024 & 1 == 1
       #+end_example
   12) Expressions too large to fit on a single line can be broken
       down into components. descriptive names are not necessary.

       Breaking any of these rules will result in a program that
       doesn't compile. However, if the compiler is clever enough to
       suggest a solution, unlike the C/C++ compilers, it doesn't just
       suggest the line and ask the developer to fix the error, it
       does that for you, in a single prompt.

       The justification for this, is very simple. Fixing a file is
       easy. Code generation shall be included as an indispensable
       part of the compiler, for reasons that shall become apparent
       very soon, so there are few compelling reasons not to ask the
       programmer if they want to have an obvious mistake fixed,
       rather than waiting for them to fix it themselves. It saves a
       few seconds, but pre-disposes the programmer to work with the
       compiler, rather than against it. They also know that if a
       compiler doesn't suggest an easy fix, then the problem isn't
       trivial, and requires a thinking person to fix. This is also an
       instance where a modal prompt yes/no/manual is a permissible
       and in fact the best possible design choice.

** On data and structures.

   It is uncommon for a description of a language to dwell so long on
   means of classifying data, without introducing a unified means of
   structuring it. In Java, the first thing written is a class. In C++
   structures are given before classes, which are given before
   templates, and usually concepts are reserved for the more mature
   audience. In Haskell, wherein =TypeClasses= are more of a way of
   representing different kinds of data, rather than different data,
   but still, they are introduced after the algebraic data-types.

   We, however have only superficial knowledge of how to do algebraic
   data-types in the language, and have a very slight semblance of what
   a structured datatype is.

   The important difference here is the philosophy. We have started by
   showing that Algebraic data-types are useful for declaring
   connections between organisations of data. They are useful to
   define recursive objects, such as a Linked List, or a natural
   number. Combined with a way of organising data into ordered
   structures, such as tuples one can do pretty much everything they
   want. One may think that they need not have a record type, for
   indeed to have one, it is enough to have a mapping from integers to
   names and vice versa, and use the integral subscripts for look-ups
   in tuples. The added benefit is that lists can equally well be
   used, and they can be extended indefinitely, albeit later elements
   will require longer access.

   However, for reasons of making the quality of life of the
   developers better, most functional languages add strictures on top,
   and provide record types. In functional languages, rarely if ever
   are you allowed to have function overloading, without first
   producing a data-class.

   As we have shown, however, we can define a function to be
   generic. By sacrificing some of the flexibility of the type
   inference system, we gained the option to have overloading based
   upon constraints. We can have an identity function that returns the
   object, whatever the type of the object may have originally been.

   Effectively, what we have created in the language of C++ is a
   definition of the following manner.

   #+BEGIN_SRC c++
   template <class T>
   T& id(T& obj)
   {return obj;}
   #+END_SRC
   Except the semantics of the language and the deliberate separation
   onto references and values makes the C++ function more narrow in
   scope, and semantically different for the case where we don't want
   the reference to the value to be returned.

   Ignoring this subtlety, the functional purist will object to
   overloading. It is a bad idea. Almost always, and almost
   universally, ad hoc overloading is not working. The languages that
   enforce this strictly, like Haskell, offer the idea of a datatype
   being possible to overload over, if the datatype is a
   type-class. Thus every instance that *is* within the type-class is
   acceptable.

   Our type system permits defining a type-class as a
   constraint. Everything that satisfies the constraints of the
   function, can be accepted as input. But where the Haskell type
   system allows one to define the type-class only via the presence of
   a specific function in the definition and the user has to
   explicitly provide proof that a type is part of a class, our
   constraints have a number of creature comfort advantages.

   For one, we can define a type class to be contingent not only on
   the type, but also on the value. For the type, an equivalent to a
   type-class instance proof, is the compile time check for the
   existence of several functions. If one wants the proof of the
   existence to be explicit, the check can be extended to a proof
   function, that the user of the library will have to supply. The
   issues of overlapping constraint resolutions is handled via layers,
   whose precedence is determined by the library writer.  We relax the
   requirement that only a single function can be defined for the same
   type, but instead make the requirement that a single constraint be
   defined only once. If the constraint is contingent on the value, an
   interesting situation may arise. if the value is known at compile
   time, then the check can also be made at compile time. The same is
   true of the template meta-programming language of C++, where some
   conditions can be checked at compile time and one can create a
   source file whose error message is the solution to
   FizzBuzz. However, we provide a simple and understandable syntax,
   such that both compile and run-time code can be understood by human
   beings. If however, the check can only be done at run-time, the
   compiler shall be smart enough to include the check at an
   appropriate location. The functions in the code don't have to
   correspond to the jump locations in the end binary, so the compiler
   has the choice between using the conditional internally or
   externally. Depending on the situation, this choice may go either
   way.

   So, we can use Algebraic data-types. Hooray. But what should we do
   if we have a datatype, whose representation requires the storage of
   too much information? This is where the fun begins, so to speak.

   #+begin_example
   Data TooManyFieldsForADataType = Record Int Int Int String String (Recordd String String String)
   #+end_example
   This is a nested record with three strings, inside a structure of
   two strings and three integers. Here we have had to improvise to
   use different constructor name for a Datatype that very much has
   the same signature.

   Now, looking at this naively, and remembering that I've said that
   this language promotes the *as a* as opposed to *is a* approach to
   Type inference, we could argue that the Internal record is
   unnecessary. It can be grouped into an ordered collection. So can
   the three integers, and the two strings. So what we can do is we could say:

   #+begin_example
   Data BetterButStillBad = Record (Int, Int, Int) (String, String) (Recordd String String String)
   #+end_example
   So if we needed to access the Second int, we could do
   #+begin_example
   case data of Record a _ _ |-> a.1
   #+end_example
   This is better, but still not great, we have to do the pattern
   matching. So how about we just use a nested tuple
   structure. Dispense with the type, just keep it simple. It would
   work, but the function signature will become rather clunky, and the
   worst part is that the function will also accept tuples of similar
   structure but different interpretation.

   So, we reach the conclusion that using the Record type isn't such a
   bad idea. But the trouble here is that record types in functional
   languages, and Record types in our language have to be different:
   after all, we want to support some form of mutation, which we shall
   come back to later. So what can do we?

   A good starting point is to recognise that much like we had not
   made a syntactic distinction between built in lists and many other
   data-structures, a record type is also a generic idea. It is a
   collection of fields, that are not ordered by number, but by
   name. So, instead of trying to come up with new syntax, we can
   extend the one we already have. Recall that a single equality sign
   is not an assignment, but a binding. Typically, a variable can be
   bound to a symbol, but we can also bind to other elements. We know
   that the semicolon in a structured expression mean ignoring the
   particular value, and the value of binding of a thing to a symbol
   is the binding itself. So:
   #+begin_example
   myRecord = (
      hello=123,
      world=456,
      temp=2^10;
      perm=2^10*temp,
      tempp= 2^30;
   )
   .assert myRecord.hello == 123
   .assert myRecord.perm == myRecord.2
   .assert (myRecord.3| PANIC)?
   #+end_example
   Here we have the first instance of a failed assertion and an
   example of how exception management is done. The bindings
   terminated by a semicolon are confined to the scope of the value
   that's being created. They can depend on the previous values, and
   shadow any values from the outside[fn::speaking of which to avoid
   the issue that plagued Emacs Lisp, scope is lexical. ]

   So we can create free-form records. We obviously cannot modify them
   yet. Specifically, if you pay attention, the collections
   initialised in such a way, are bindings of values to names. Nothing
   is said of how the value is stored, if it is stored at
   all. Moreover, if we had a hypothetical function that accepted this
   as an input, and produced a record type as output, we have no
   guarantee that the output record will retain any of the
   information. This has a number of advantages, namely that to pass a
   collection of data, one does not need to have an obscure datatype
   that serves one purpose. A familiar pattern when trying to refactor
   code from relying on global scope, to local scope, is that the
   collection of external data needs to be provided in a single
   data-structure, which is often named context. The issue with this
   design pattern is that multiple people can have the same need, and
   would most likely collide in naming and in functionality. If the
   sole purpose of your data-structure is to carry structured pairs as
   if in a dictionary, this approach of a named tuple can be enough.

   But suppose that you want to limit the kinds of tuples that you can
   pass, and not only on the basis of present fields, but on the basis
   of conforming to an API, that may have its own set of checks and
   tests and assertions. In such a case, though this ignores the
   benefits afforded by the constraint based typing system, one may
   create an Algebraic record.
   #+begin_example
   Data AlgebraicRecord = Record of (typeof myRecord) | Null
   #+end_example
   This now has the fixed structure that one needs, albeit the
   constraints on the values are rather loose: typeof simply
   restructures the data, and ignores any constraint other than the
   type of the data; it sees 123, which can be an Int of infinite
   precision, then it returns an Int.

   For reference, it is good to know that
   #+begin_example
   typeof myRecord = {(Int, Int, Int), (Binding, Binding, Binding)}
   #+end_example
   which itself isn't a list, nor is it a tuple. It's importance shall
   become clear later, when we create a custom version of a mutable
   Record-based list.

   This new type can be constructed from a suitable tuple:
   #+begin_example
   myAlgebraicRecord = Record myRecord
   #+end_example
   And subsequent function can make use of this:

   #+begin_example
   prettyPrint:: AlgebraicRecord -> .infer
   prettyPrint record =
      (print record.0, record.world, record.perm)
   #+end_example

   A more important factor is how structured binding works:
   #+begin_example
   dummyRecord:: Int -> AlgebraicRecord
   dummyRecord a = Record (a, a^2, a^3)
   #+end_example

   Which compiles to the same as
   #+begin_example
   dummyRecord :: Int -> .infer
   dummyRecord a = Record (hello=a, world=a^2, perm=a^3)
   #+end_example

   Now, to avoid issues of non-determinism and the compiler behaving
   arbitrarily, if both of these definitions of =dummyRecord= appear
   in the code-base, not even necessarily within the same source file,
   the compiler will complain and prompt for action. Only one of them
   can be active at a time. So the user is prompted to deactivate the
   one they like the least.

   But of course, there is a different route, using a rewrite rule,
   which we shall explore shortly after.

   Now, knowing that we can make Algebraic data-types that depend on
   one another recursively, one can have links between record
   types. Let's show that on a concrete example.

   Suppose the program that we were writing, was TODO list manager,
   (the hello world of front-end design). We could define a named
   record of the following type.

   #+begin_example
     Data Todo = Todo of
     typeof (
       Definition :: String = '',
       addedOn :: Date = today,
       due :: Date<NotEarlierThan(addedOn)> = today,
       parent :: Todo = None
     ) | None
   #+end_example
   This is the best practice approach to creating the record. While
   there are values that are being discarded, being able to provide
   good defaults at construction time is a good idea.

   Now, we could have taken a different approach:

     #+begin_example
       Data Todo = TopLevel of
       typeof (
         definition :: String = '',
         addedOn :: Date = today,
         due :: Date<NotEarlierThan(addedOn)> = today,
       ) | SubTask of
       typeof (
         definition :: String = '',
         addedOn :: Date = today,
         due :: Date<NotEarlierThan(addedOn)> = today,
         parent :: Todo = None
       ) | None
     #+end_example
     Now, did we need to provide the None constructor? Reason
     suggests, that yes, we should have, otherwise how can we
     construct the =parent=? The issue is, however, that we now made
     the state of having a top-level sub-task represent-able.  We want
     for invalid states of the system to not be possible to represent
     within the language. So with our third attempt we shall try to
     provide a full set of possible states.

     #+begin_example
     example = (
       definition :: String = '',
       addedOn = today, "The inference will be fine";
       due :: <NotEarlierThan(addedOn)> = today, "Should be fine too";
     )

     Data Todo = Toplevel of typeof example
     |  SubTask of typeof (*example*, parent::Todo = _)
     #+end_example
     where we've used the wildcard to indicate that the value taken is
     irrelevant. It's different to if we used =.infer=, because in
     that case the value would have to be possible to determine from
     the constraints, and since there's not an inference graph for
     this type (yet), the value cannot be inferred.  We've also
     re-used the familiar (\*explode\*).

     Can we get to feature parity with C? The answer is a resounding
     yes. Functions are first class citizens of the type (or rather
     kind) system. If we wanted to, we could simply have record fields
     that are functions. Moreover, since the uniform function call
     syntax allows the fields to be subscript-ed by name, we recover a
     semblance of familiar syntax.

     #+begin_example
     instance = Class (hello='hello', world='world', function=function)
     instance.function (arg1, arg2, arg3)
     #+end_example
     The only non-trivial piece here is that the function has to
     accept an ordered collection of three arguments. If it were a
     curried function, we'd have to use a construct. Moreover, because
     some ordered collections are bound by name (anonymous records),
     we can use this to allow keyword arguments into class
     methods. [fn::thought for the day, are we creating a convoluted
     version of Python? ]

     What do we gain over allowing the =class= keyword? First of all,
     we gain the option to not name a class. We get tuples and named
     tuples for free, and the constructor syntax is just an evolution
     of the ordered collection syntax. Another important point is that
     at this stage, the only information carried with the instance of
     a class is what's directly inside its constructor. All other
     polymorphic behaviour is divorced from the class, and hence can
     be optimised away. The class is explicitly immutable, but every
     internal field can be accessed. So we gain the read-only for
     free, but pay for it by allowing reads on every single
     instance. Or do we? How can one implement a =private= element of
     a class?

     #+begin_example
     exposedInstance = ExposedClass (1, 2, 'String')

     privateField::ExposedClass<IsEqualTo<exposedInstance>> -> String
     privateField a = 'HiddenString'

     .assert exposedInstance.privateField == 'HiddenString'
     anotherInstance = ExposedClass (1,2, 'string')
     .assert (anotherInstance.privateField|PANIC)?
     #+end_example
     So the other instance doesn't even have that piece of
     information. The only issue here is that to track this much
     external state, would require a lot of tracking by value.

     However, this process can be automated, by creating a custom
     constraint [fn::to avoid creating a custom constraint for every
     private type of value, consider nesting constraints in such a way
     that the dependencies and implications form the right
     expression.], parameterising it on the data that we want to
     attach, and tracking it through (exercise)*. The added benefit of
     such an approach is that by not exposing the private data, we can
     prevent its corruption by outside translation units.

** Guided inference.

   Many languages pride themselves on the ability to do type
   inference, C++ 11's auto was a headlining feature and for good
   reason. Meticulously tracking through the type of something is a
   tedious job that nobody wants to do. Not many languages however,
   can pride themselves on function inference. Can this even be done?
   Should we bother?

   Well, to put it simply, it may be an interesting avenue to
   explore. It seems like a solution looking for a problem, but
   considering how much time programmers spend writing trivial
   boilerplate, this may seem like a worthwhile investment.

   Let's begin our discussion by talking about the possibility of
   function inference, and how it might work. Consider a function that
   takes a value of type a and all of the constraints on it, and
   returns something of the same type and the same constraints.

   #+begin_example
   fun :: 'a<*> -> 'a<*>
   #+end_example

   It should work for any type, and preserves all of the constraints,
   regardless of what those constraints are. If for more than a second
   you doubted that any function besides the identity can do this, you
   are incorrect. The best way to see it, is that the value can have a
   constraint =<isEqualTo(value)>= which has to be preserved. The only
   way that this constraint is lost, is if the address of the thing to
   which it points to has changed, but that would relax the
   constraint, as the thing passed in cannot simultaneously represent
   the value and its address. So we can, in this case write the answer
   as fun = .infer.

   But can we do this in other ways? Suppose that we have a function
   that preserves only a certain constraint,

   #+begin_example
   fun :: 'a<isGreaterThan(b)> -> 'a<isGreaterThan(b-1)>
   #+end_example
   Well, you can obviously see the issue. For one, this implies that
   the type can be ordered, and that =(b)= is of the same type, and it
   can be added, so we already have a lot of implicit
   constraints. These do not help however, because of one simple
   reason. Suppose the function is the identity. It satisfies the
   constraint. Suppose that the function is
   #+begin_example
   fun a = a-1
   #+end_example
   It would satisfy the constraint also. If we had asked the compiler
   to infer what we meant just from the constraint operations, we
   would expect for it to be able to maintain the property both
   ways. But we would prefer it used the case where the new constraint
   is the tightest, this is the first thing that comes to mind after
   all. But the compiler is not as smart as I made it out to be
   throughout. All it can do, is form machine code based on
   successively refined rules. Both the definition of the rules, the
   implications and interactions of the rules, as well as the
   inference modes are something that library developer has to do on
   their own.

   Much like the C++ standard library delegates the creation of
   standard collections and standard templates, we shall have a
   standard library that shall include some syntactic sugar, some
   basic constructs that would otherwise be too tedious to write out,
   and of course a few inference rules. However, anyone who wishes to,
   should be able to contribute to the inference process.

*** Value inference.

    Before we tackle the infinitely more difficult task of
    constraining and running inference on functions, we shall consider
    the far simpler task of running value inference.

    #+begin_example
    a :: Int<IsEqualTo(3)>
    #+end_example
    The answer if we wanted to infer it, is obvious. And no other
    value is possible. So, we can write this as:
    #+begin_example
    <IsEqualTo(n)> .suggests n
    #+end_example
    This means that if we ever see a value with this constraint, we
    build up a candidate list. This candidate list is populated via
    such =.suggests= rules. At the end of the process, an element from
    the list of candidates is selected. In this case, as there's only
    one element, the thing that gets selected is =n=, the parameter.

    #+begin_example
    a :: Int<GreaterThan(2)>
    #+end_example
    Here, we have an obvious candidate.
    #+begin_example
    <IsGreaterThan(n)> .suggests n+1
    #+end_example
    The trouble here, is that the constraint of GreaterThan, extends
    to floating point numbers, and so while we fixed this answer, we
    have biased the same answer onto the case of floating point
    numbers. To undo this, we could say either
    #+begin_example
    Integral .and <IsGreaterthan(n)> .suggests n+1
    #+end_example
    Or equivalently, say
    #+begin_example
    <FloatingPoint> .excludes n+1
    #+end_example
    Arguably the second one is better, because every instance of when
    the +1, has special significance in the context of deducing a
    value, the 1 is an arbitrary unit and has no special significance,
    so in every inference on FloatingPoint types, we have removed n+1
    from the candidates.

    Now consider whether or not we want the inference to go the same
    way, based on other constraints. Suppose the context is such that
    we want to have an <Even> number, that's greater than another
    number.

    #+begin_example
    a::Int<Even><IsGreaterThan(2)>
    #+end_example
    By our reckoning, the previous candidate isn't great. Rather than
    removing it, we can make it take a back seat: Add another
    candidate and promote it.

    #+begin_example
    <IsGreaterThan(n)> .and <Even> .suggests n+2 .priority 1 - (n /mod/ 2)
    #+end_example
    So we shall have a priority 1, if the number is divisible by two,
    and priority zero otherwise. The priority is a floating point real
    number, which if not specified defaults to 0.

    Now, if we had a situation that looked like this:
    #+begin_example
    a :: <Positive>
    a = .infer

    IF even a THEN print('{a}')
    #+end_example
    So what does the compiler do?  First of all, it sees an infer
    statement, so it looks at the known constraints: so far it's just
    that it's a positive number. Positive implies GreaterThan(0), so
    that is also a constraint to keep in mind. It then walks down the
    suggestion lists, nothing for positive, but there is a good chance
    of this working with GreaterThan(0), so fine! One candidate: n+1,
    because we don't have the Even as a constraint. But we haven't
    inferred the value yet. At this point the compiler looks forward,
    and sees the familiar constraining IF construct. Now the
    constraint on a, wasn't satisfied beforehand, and if this had been
    a variable that we input from the user, it would have been a
    compile error. However, we ask the compiler to infer the value, so
    can we? We constrain it to be even, and back-propagate the
    changes. This means that the inference has another check, and all
    other constraints are checked for whether or not they conflict
    with even-ness (if they did, this would be a compile error). After
    we've established that the value could have well been even, we
    revise the list of candidates, and their priorities. This adds the
    candidate n+2. We then continue, find that a has to be printable,
    this is implied by its type, so no extra constraints. We reach the
    EOF, the candidate list is in its final state, and the value is
    being inferred. The most suitable value is 2. During compilation,
    the user is prompted if they want to replace the =.infer=
    statement, with the value that we've inferred, and the user has
    the option to never re-prompt for it in the future (reversible).
    This is how the value inference system can save the person from
    having to solve a constrained optimisation problem. Of course, the
    compiler didn't do most of that work either. The benefit here is
    that writing such libraries can be a collection of coding
    exercises, that people might be interested in doing in their spare
    time, that might actually prove useful.

*** Type inference.

    This is a similar situation to where most languages find
    themselves. However, the constraints are part of the type
    information carried within the program's source code, and this
    information needs to be carried through. As before let's start
    simple.

    #+begin_example
    a :: .infer
    a = 3
    #+end_example

    Now. The situation here is radically different. =3= is a literal
    value, representing multiple types. Languages like Standard ML
    would require that this literal be only representative of an
    Integer, citing that mathematically, two different types are two
    different types. However, a point that I found exceptionally
    frustrating when writing code that had to deal with such a
    paranoid type system, is that mathematically, real numbers
    encompass both Integral and non-integral values, so the
    distinction is moot. The real reason for sidestepping the
    inclusion of Integral as well as floating point representations is
    the fact that the value is represented in memory with two
    conceptually incompatible representations and the type system is
    really a method of assigning the representation.

    I believe that such an approach is unnecessarily limiting. If one
    can get away with not annotating the types of inputs and outputs
    of a function, one can surely be safe in freely mixing implicit
    and explicit type conversions. This is almost true, of almost all
    applications. In 3D graphics, rounding errors are rarely visible
    beyond the errors in manufacturing the display and the lighting of
    the room, however in Scientific computation, unaccounted for,
    systematic errors are unacceptable. So implicit conversions are
    not a solution. However, what is a solution, is the type inference
    that we shall perform.

    The value is bound once, and never used again. So while we could
    state that the type of it could be an integer, or could be a
    floating point... It very well, might just be represented by a
    string. The type of the value is irrelevant, because it is not one
    of the names that is being exported, and no other uses of it can
    be found. What we thus want to do, is design a set of rules that
    can see that, and disable the type inference altogether.

    The best way to do that is to say:

    #+begin_example
    <Unused> .suggests _ .priority 100
    #+end_example
    meaning that the type doesn't matter, and can be ignored. We shall
    pretend that there are no other candidates with that priority.

    Take 2:
    #+begin_example
    a :: .infer
    a = 3
    print('{a}')
    #+end_example
    Hm. Same situation, except we have an external side effect, hence
    we cannot draw much of any conclusions from what this is. We don't
    care about the type, but we do care that in all cases the
    representation is the same, and the representation is the only
    thing that matters. We don't care what type it is, as long as it
    prints the same way as any other type.

    #+begin_example
    <PrintOnly> .suggests _
    #+end_example
    Now the situation is very different if we have an arithmetic
    operation before the printing.

** Example: the assignment operator and Functional programming.
* Perspective, or why Python is a valuable lesson.

Python is a remarkable piece of human ingenuity and abject failure in
almost every conceivable way. Allow me to explain. Python has had a
very strict and very specific policy on what it was and what it wasn't
since day one. There is something that is called the Zen of
python. Here it is in full:

- Beautiful is better than ugly.
- Explicit is better than implicit.
- Simple is better than complex.
- Complex is better than complicated.
- Flat is better than nested.
- Sparse is better than dense.
- Readability counts.
- Special cases aren't special enough to break the rules.
- Although practicality beats purity.
- Errors should never pass silently.
- Unless explicitly silenced.
- In the face of ambiguity, refuse the temptation to guess.
- There should be one-- and preferably only one --obvious way to do it.
- Although that way may not be obvious at first unless you're Dutch.
- Now is better than never.
- Although never is often better than *right* now.
- If the implementation is hard to explain, it's a bad idea.
- If the implementation is easy to explain, it may be a good idea.
- Name-spaces are one honking great idea -- let's do more of those!

In principle, this is a remarkable set of common sense ideas that
dictate the design of a good language. The trouble here, is that it
has very little in common with modern python, and even less in common
with the soon to be deprecated version of Python 2.7. For example:
Beautiful is better than ugly, except the only way for preventing the
user from accidentally passing a keyword argument to a function
position-ally, involves boxing and unboxing a dictionary. It's fine,
but it's ugly. Explicit is better than implicit. Hence why there are
so many implicit conversions taking place, and the whole type system
revolves around making the types (which things in Python clearly
have), implicit. Simple is better than complex, hence why you can't
point to a location on the hard disk, you instead need to work with
the brittle system of modules. At the very least, the language of
Python is hypocritical.

But my complaints don't end there: if an implementation is hard to
explain, it doesn't automatically mean it's a bad idea, just that you
don't understand it. It's a common test in teaching of Physics, to see
if you could explain the concept to a three year old, without
contradicting experiment. If your hypothetical child can't possibly
get it, or you contradict what you know, then you don't understand the
concept. The feature needs to go back to the drawing board, perhaps
abandoned altogether: don't release code that other people wouldn't be
able to use, because it's too complex and brittle. The main issue
here, is that the design of an interpreted **only** language is the
last thing that can prevent you from doing that.

So how did it come to this? In my humble opinion, it's the problem of
the committee being simultaneously too free in many ways not set by
the zen, but also encroaching on the same principles. As a result of
the decision making process that did not include introspection and had
no room for critique, what was better for Python as a commercially
viable language, was the last thing that Python as a programming
language needed.

** Why is it good and successful

   The appeal of the language also has nothing to do with the
   principles listed here. Most have started programming in Python
   because of the robust infrastructure of poorly designed but
   well-maintained libraries, that allowed for relatively modest
   programmers to do things that were previously too difficult to
   express. Python is easy to get into: the compiler isn't a compiler,
   so you don't have to deal with cryptic messages like you do in C++
   or Java. You spend less time producing noise, than actually coding,
   which is a plague on most tooling-oriented languages like
   Java. Moreover, you have the right amount of righteously exquisite
   syntactic sugar in Python, that lets you be exceptionally
   productive, rack massive amounts of technical debt and be bombarded
   by GitHub issues, that you will ignore. Notice that at no point
   have I complained about Python's performance. It doesn't and
   shouldn't matter, but what does, is the fact that some design
   patterns are too simple to express, obfuscating the inner problems,
   while some other complex are downright impossible to do correctly.

   Python has the appeal of lacking things that cannot be immediately
   understood. If you were to write a program that printed out 'hello
   world' in Java, consider what you would need to do. First
   everything is a class, which is not an easy concept to understand
   fully, so if you're unlucky enough not to have been taught this at
   University, you would go down a rabbit hole of dismay at words like
   contra-variance and encapsulation. Then, you have the conceptual
   hike to explain what a public, static and void function even is:
   you have to know that everything in Java has access restriction,
   dictated by the severity of the word. After all of that you need to
   dig into the namespace of =system.out.println("hello world")=,
   invariable be puzzled as to why it doesn't compile, and understand
   that you have to put a semicolon at the end of every statement. You
   understand why that is a design decision only when you see how the
   compiler is made, see the mess that both UNIX and DOS have left,
   and understand that this isn't necessary for the language, it's
   just easier to write a compiler for; something that no good
   language should have.

   By contrast, in Python 2, you have the command =print "hello
   world"=, which doesn't give you too much trouble. This property of
   not conceptually overwhelming the user is remarkable, and something
   that Python should have. In fact, I can forgive it for not
   packaging print into a namespace as the final step of the Zen would
   suggest, because name-spaces are a solution to a problem, if you
   don't have the symptoms of said problem, you don't need it. In
   fact, because you have no need to remember if dict was in
   =Python.utils= or =Python.awt.utils= or otherwise, you have, at the
   basic level the "preferably one obvious way" to do stuff. If you
   wanted collections, you have them built into the language, and
   these collections, as a matter of utmost importance, conform to our
   intuitive understanding of how a collection would behave. Contrast
   this to C++. If you wanted more than one integer, you could just
   create several of them in a comma separated list. But say you
   wanted to save the inputs of the user... Hm, in that case, you
   would use a vector. The first problem here is that the word
   ``vector'' is heavily loaded. Anyone familiar with maths, would
   expect for this collection to be of fixed length, kind of like a
   tuple, and yet this one you can grow. On the other hand, you have a
   built in feature, the array, that acts like the vector would for a
   mathematician. There simply isn't an intuitive way to do those
   sorts of things, not to mention that the semantics of how to create
   those damn things can also be head-scratching. I remember, when I
   was learning this on my own, I found it incredibly frustrating that
   not only could you not create an array of variable length, when
   programs clearly need to be able to do that, but also, that the
   number needs to be known beforehand. Starting from a language where
   all of these internal rough edges are gone is much more appealing,
   and is conducive to understanding of the process of programming.

   But Python has more. It has no curly braces, which are not given
   any significant explanation early on, until you learn about
   scope. It has a ``neat'' syntax, where indentation, which is how
   humans determine if something is nested in something, also has
   significant meaning for the syntax. This economical design
   contributes to *very* readable code, without any training. At some
   point, you will reach the same, if not better understanding of
   blocks in any of the C-family languages, however, because those
   languages need to be gotten into first, and people are lazy, Python
   gets a disproportionate number of people getting into it.

   Lastly, we shouldn't underestimate the power of
   infrastructure. There are many languages that are better than C++
   at what C++ is doing, like D, or Rust, but the compelling reason to
   use C++ is that it's also the language of pretty much everything
   else. Unreal engine uses C++, Qt and every other GUI toolkit in
   existence uses C++. Porting such code to Python is no easy feat,
   but it's comparatively easier to do than any other
   crossover. Moreover, as opposed to many other languages, despite
   its corporate success, the platform is egalitarian and open: anyone
   can write python packages, anyone can publish, and it will run on
   any platform for which there is a python interpreter. Zig, a
   promising language that is in many ways superior to both C++ and C,
   has the problem of being in the early development: there aren't
   many code examples to look at, there isn't much support. On the
   other hand, the no-frills attitude of Python lets people write code
   that is intelligible from the get go, so you can write literate
   documentation.

   There are languages that share some of those traits: if I needed to
   train my children to program, I'd go with Haskell or standard ML,
   because they have few distractions and very simple (conceptually)
   ideas, that they will no doubt have seen in Maths at School. Sure,
   I don't expect them to stay on Haskell, and despite the fact that
   it is superior to Python in every possible way a Language can be
   objectively superior to another, simply because of the projects
   that are written in Haskell, they will likely move on. Writing a
   library in Haskell is complicated, because it relies on constraints
   that other languages with which it might interface, do not
   have. Moreover, the flagrant buoyancy of complicated concepts in
   Haskell, is still a thing: I cannot let my children forever live in
   the prelude environment, so I will eventually need to teach them
   about Monads, which look like an over-complicated version of
   imperative programming. A language that supports this from day one
   is better. So Python is comfortable. So is Julia, (and it shares
   many of the issues of Python). Competing with these languages is
   like trying to convince the US army to adopt a gun that's only 5%
   better than the M1 Garand. Sure it *is* technically better, but to
   change over from where you are to that, you need more than a 5%
   improvement. Not to mention that Python's fluid perception of what
   should and shouldn't be part of the core language, allows for those
   ``killer features'' to be back-ported into Python.

** The Ugly side of convenient.

   McDonald's is the cheapest form of fast food. If you think about the
   rationale of why most people eat it, there's a simple answer: the
   food looks appealing, it's cheap, accessible, reliable and above
   all, predictable. Python has most of these features. So knowing
   that you don't give fast food as the panacea, and suggest everyone
   to eat it, why are most online schools starting to teach
   programming from the Computer Science equivalent of a
   drive-thru.

   My perception is that the people are not allowed the freedom to
   identify. Whilst we can choose to not eat junk food, we cannot
   choose not to use Python, if we are employed in any serious
   capacity. Sure if you write code as a hobby, you are spoiled for
   choice, but if you want a career in game development, you're stuck
   with C++, and none of the languages that have a better model. The
   reason why big-mac code is slowly edging its way to becoming the
   norm, is that more projects are based off of academic research,
   where the versatility of Python, and the ability to bodge together
   a paper give it an advantage, rather than become the drawback. The
   reason why academics don't use Haskell, is that to express a
   concept properly takes time, and gives little immediate positive
   reinforcement. It feels like a puzzle game, rather than a rhythm
   game.

   So Python is slowly failing upwards, because all of its ugliness
   works to its advantage in most circles. It doesn't matter if you
   bodge things together, if you can bodge them reliably. So the
   guiding principles have no bearing on its success, the incidental
   minutiae have a far greater influence. Hence, why the guiding
   principles have little in common with what we see
   today. Intelligent design succumbs to evolution if you allow both
   to coexist, and survival of a language is dictated by the number of
   projects that use it. I'm locked into Python because I have
   =matplotlib=, and no other library can generate plots. I'm locked
   into python because almost all of the code is written in
   Python. Most of my public works are written in Python, because the
   circles in which I am use that to the exclusion of everything
   else. In order for that to change, I need to provide a
   compelling-enough reason, and in academic or scientific
   computation, the only such reason can be performance, which
   amortised for the complexity of the tools, the abundance of
   computational resources and the general preciousness of the
   programming time over the run-time of the application, means that
   Python and its ilk are doomed to succeed.

   So what is the corollary? For one, the design principles listed
   conveniently on a piece of paper can guide the development only
   initially. For the language to be good in the long term, it needs
   to have a policy that prevents it from becoming ugly, when subject
   to stress. There will be people who will want ``Feature X'' despite
   it being completely contradictory to the spirit of the
   guidelines. And because a large-enough number of people will insist
   on these features, there must be either a mechanism for veto, or
   for fluid adjustment. The experience with the expression-based
   assignment operator and Guido Van Rossum's retirement showed that
   the veto is not a functional solution. Instead of fighting the
   current, we must develop ways of redirecting it.

** The guiding principles of good language design.

   The programmers time is the most important performance metric.
     - If they need to take time to make the program faster, it's our fault.
     - If they need to dig through documentation it's our fault.
     - If they need to take time to refactor it to conform to what the
       language needs, then we've turned them into an inefficient part
       of the tool-chain.
     - If they need to take time to write boilerplate, it's our fault.
     - If they need to fiddle with third party tools to make writing
       code easier, we wasted our time.
     - If they need to convert tabs to spaces or spaces to tabs, then
       you've turned them into a worse version of =ed=.
     - If they spend time searching for documentation on an error
       state, we've wasted time.
     - If they need years of training to get good at the language,
       they need to somehow make those years back.z
     - If they spend hours upon hours searching for how to install
       your library, instead of that being done by you, you wasted
       their time.
     - If they spend hours figuring out what your message means, you
       wasted their time.
     - If you don't have a consistent guideline on a best practice, it
       shouldn't matter.
     - If you have a unique guideline that's true in every instance,
       and you allow your programmer to not follow it, you are
       effectively offloading the parsing onto them.
     - If you told the programmer about a mistake that has a unique
       and obvious correction, and there is no decision-making
       involved in fixing that mistake, then you have wasted no only
       their but the end users' time.
     - If you encourage bad design, you cost decades to those
       unfortunate enough to have used your language.
     - If the type and error checking takes time, and has nothing to
       do with solving the problem, then you've asked your programmer
       to do a captcha, and threw away the result.
     - If you've allowed for erroneous states to be represent-able,
       then you have wasted all of the time taken to find this
       mistake.

     So. Quite a bit to unpack. Notice that the Zen of our language,
     is very different to Python's. It is all a value judgement, but
     it encompasses all of the possible situations. It is a set of
     constrained optimisation problems that if solved correctly will
     lead to the programmers and you being on the same side fighting
     problems, rather than you and the problem fighting the
     programmers.

     Every design decision that we've made up until now has been
     dictated by these guiding principles. The constraint based type
     system is a mechanism, that allows for violated constraints to be
     easily identifiable. The result of the constraint typing, which
     allows for both type and value inference, allows the library
     designers to ease the process. Fixing common mistakes by creating
     substitution and optimisation rules, allows one to both have an
     easy to understand implementation, and optionally, understand how
     it is being optimised. This layered structure lends itself to
     good documentation.


     # TODO

** interlude: associative structures, keyword arguments and symbols.

   It is instructive to look at a very intriguing design decision made
   in Python at a very early stage, one that I'm immensely fond of,
   and unfortunately one that we have to improve upon, as it is
   critically flawed, and a somewhat weak point of our language design
   so far.

   This is not something that can be implemented solely as a
   construct, albeit it is possible, but sub-optimal and potentially
   dangerous to do so. I am referring to keyword arguments and
   associative structures. We have reserved the curly braces to mean
   objects for which ordering doesn't matter. Thus we have left open
   the potential for using that same syntax in a somewhat odd
   circumstance. Suppose that we wanted a hash table, to look up
   certain values, i.e.
   #+begin_example
   {
     key_1: value_1,
     key_2: value_2,
     key_3: value_3
   }
   #+end_example
   So far nothing out of the ordinary: the curly braces signal that
   the objects can be reordered, and the colon is rarely used in our
   language, as we use the Haskell/FORTRAN-like =::= to indicate
   =type\_of=. But there is a problem. The obvious one, is that the
   language specification deliberately disallows underscores as part
   of the name, but that's hardly an issue in Python, since the values
   could be interpreted as look-ups within a three element ordered
   structure =key= and =value=. However, the less obvious one comes in
   when we consider passing this structure as a list of function
   parameters.

  The interpretation of underscores as indices versus the traditional
  square bracket approach is debatable. This debate, however is very
  important for the point made in this section. On the one hand, since
  there can be no ambiguity between a list in square brackets, and a
  slice, or index, the square brackets are a remarkably good
  choice. However, there is a certain amount of neatness in using the
  underscore syntax, and while it may seem that this is breaking the
  convention, it actually falls neatly into our established notions.

  For starters, most markup languages understand underscores as
  indices, so in LaTeX, you would probably be able to write =a\_n=,
  which will render to \(a_n\). This notation may be inconvenient for
  programmers in other languages, because at first glance, they
  sacrifice a part of the language that they know: the ability to name
  things using underscores, for something that they haven't used
  before. Is this sacrifice worthwhile?

  In my opinion, it is not a sacrifice at all, and the reason shares a
  common thread with why we shouldn't replicate the conventions of
  Python.

  Consider that our language doesn't stand convention that is not
  enforced by a syntactic and semantic difference: even the semicolons
  terminating expressions with side-effects are meaningful, they
  indicate the return value is not to be preserved. So the convention
  for type identifiers and value identifiers are both =CamelCase=. Why?
  you can quote me on that this is a highly subjective, aesthetic and
  opinionated decision. So much so, that I don't want for my
  programmers to revert to a different convention out of habit: if you
  try writing
  #+begin_example
  hello_world :: Int -> Str
  #+end_example
  you would get a syntax error. So you will get source code that
  *always* follows this convention, because breaking it without having
  first defined the structure =hello= and looking up =world= in it,
  would simply not pass the syntax checker. The second thing that you
  accomplish, is the fact that you can define the operator of lookup,
  as you know it's both infix and has the symbolic name =\_=. You
  don't have to guess whether that's =\_\_index\_\_= or =\_\_at\_\_=,
  since there's no need to guess.

  If you want to do some arithmetic, you can parenthesise it, but the
  lookup will have the regular form of =\_(= for the opener, and of
  =)= for the closing parenthesis. We don't need to sacrifice one of
  the most important resources, a pair of standard delimiters, in
  order to get the same effect, and every syntax check we can run on
  the pair of square brackets we can run on this regexp.

  But hold on; notice that I said, *if* you didn't define =hello= and
  tried to lookup with the value =world=. What if you did? How would
  you go about that? Well, one way could be
  #+begin_example
  hello = {
    world: 42
  }
  #+end_example
  But notice I haven't defined the meaning of =world= yet, or rather I
  didn't bind that symbol to a value. Yet this seems natural to write,
  doesn't it? I suppose, we should then go the Python route and do:
  #+begin_example
  hello = {
   "world ": 42
  }
  #+end_example
  except, whenever one copies design elements from Python they have to
  be judicious not to copy the mistakes. OK, but then what do you do
  if the string contains a space. Strings can differ in white-space
  only, but we are using white-space as a terminator for
  identifiers. So should the actual lookup syntax should be
  #+begin_example
  hello_"world "
  #+end_example
  which is (in my opinion) an improvement in readability over
  #+BEGIN_SRC python
  hello['world ']
  #+END_SRC
  at the very least there's less redundancy in termination. But
  there's a far deeper corollary which we'll touch upon shortly.

  Coming back, if we want for =hello\_world= to be syntactically
  valid, we must provision for =world= to have a type. Indeed it has
  one: it's a symbol. In other words it's an intrinsic part of the
  source code, representing the abstract concept behind the object
  that we bind to the symbol. It is *not* the same as a string. So
  there is a similarity to the =hello.world= syntax. However, as we've
  mentioned previously, the =*.world= part is a representation of
  calling the function =world= which has the first argument
  =hello=. World /is/ a symbol technically, but whenever you see a
  dot, you are sure that there must be a function named after what
  follows the dot. However, that is related to the /type/ and not the
  /value/ of an associative structure. Creating the look-ups might not
  map neatly onto a function, hence the subtle but important
  difference in invocation.

  We also get an additional bonus: the symbol names have to be
  acceptable as identifiers, so you can be sure that white-space won't
  make a difference. If you pass in a capital case identifier, its
  value can only be a type, so this would allow for type arithmetic to
  be performed using associative structures, which might be very
  important in applications such as databases, or when interfacing
  with other languages with different type bindings.

  Now consider if we had both:
  #+begin_example
  world = 21
  #+end_example
  and
  #+begin_example
  hello = {
    world: 42
  }
  #+end_example
  is hello mapping from 21 to 42, or from =world= to 42? Well, for
  consistency's sake, the preferable answer would be from =world=
  to 42. But we do not want to lose the ability to do substitutions in
  associative structure definitions, so we want to be able to, in
  addition to string formatting be able to do literal formatting. We
  will get to that in a moment.

  As a result of these decisions, we can safely say that certain kinds
  of associative constructions can be passed to functions as keyword
  arguments: only the symbol names shall have any significance, and it
  is up for debate (for now) whether the other elements should be
  ignored, or indeed if heterogeneous associations should be allowed
  altogether.

  However, we've made a decision that makes such usage a little
  inappropriate: all of our functions are implicitly curried
  one-in-one-out type of affairs. We allow for structures to be passed
  in either anonymously: by specifying an (dis)ordered collection of
  the arguments, which will be bound to the names implicitly, or a
  named data-structure, provided it passes the duck-typing. Keyword
  arguments do not fit well into this.

  We do not want the choice of implementation style to affect the
  usability of a function beyond what's reasonable. So let's talk. By
  our constructions, the function defined as:
  #+begin_example
  max :: .infer
  max (a,b) = a if a> b else b
  #+end_example
  can accept its arguments in any form: a list, an array a
  structure... anything that preserves the ordering. Which of these
  functions is to be used, depends on the type it is invoked on. A
  function that has the signature:
  #+begin_example
  max :: (Int, Int) -> Int
  #+end_example
  is less generic, but it has the same hold over what the concrete
  type of its arguments can be. However, the function
  #+begin_example
  max :: Pair(Int, Int) -> Int
  #+end_example
  is unambiguous and only accepts one type of argument. We do not want
  for this function to accept a =FunnyPair(Int, Int)=.

  However, the reason why Python has the =*args= =**kwargs= as acceptable
  inputs to a function is borne out of the observation that the entire
  function call syntax is just a wrapper around the fact that one can
  specify the function invocation as either an ordered or disordered,
  but named collection of information. Currying can be employed to
  serve as part of the first purpose: just pre-apply according to the
  ordering of the collection and collect: e.g.

  #+begin_example
  applyArgs fun args =
     nextFun = fun args_1;
     applyArgs nextFun args_[1-->len]
  #+end_example

  Here we have assumed that the slicing mini-language that is similar
  to Python's but has crucial differences. The collection args is
  automatically constrained to be an ordered collection, so that the
  application of =applyArgs id {1,2,3}= would result in a constraint
  resolution (compile-time) error, because the slicing syntax that has
  =>= in the name implies the presence of ordering. Another subtle point
  is that we do not have an explicit base case, for an empty
  collection. We can do that, because we have adopted a similar syntax
  for a =unit=, i.e. an empty collection =()=, and an empty
  list/array/structure. Any function that has no inputs, is to be
  eagerly evaluated, when it sees =()=, so this version not only
  applies but also evaluates all of the functions.

  However, here we also see why the same methodology cannot be applied
  to dis-ordered =**kwargs= style invocations: the reason why currying
  works is because all such functions are in fact functions of a
  single argument that accept one input and produce a single
  output. The illusion of multi-parameter functions comes into play
  because the return type is a function that accepts the type of the
  next argument, and returns a function that accepts the one after
  that. If we try to apply these functions out of order, we would not
  get something that makes much sense, and doing this for keyword
  arguments that could be provided out of order, or far worse, be
  missing, is not something that we can do outside the language
  design.

  If we want to support these sorts of wonky application schemes, we
  want a clever scheme that permits for the arguments to be applied
  out of order and still generate a string of lambdas that compile to
  very much the same code. Herein lies the solution.

  A first step is to recognise that the representation within source
  code, of the kinds of things that we want to use as keys must be
  preserved. In other words, a symbol is not part of the documentation
  of a library, it is part of the library itself, or rather part of
  its representation. This is not a wild concept, C-based ABIs usually
  retain the presence of something function names, our addition is
  that we might want to keep the names of the parameters as well. In
  fact, what we propose is a new, somewhat clunkier but much more
  resilient ABI, but we shall have to touch upon that a little later.

  What this also necessitates is a clear delineation between what a
  function a subroutine and a procedure really are. So without further
  ado, the next section shall touch upon the necessity to have such a
  distinction and contrast the lessons that we learn from Python to
  lessons learnt from other languages.

  But to give a final answer to how we would do that: we would have a
  method of defining module-public functions, that shall have all of
  their parameters declared with a name and a documentation
  string. When such a function is defined, what actually happens is
  that the Cartesian product of all intermediate currying schemes that
  can result in function evaluation are being bound to the same name,
  so that one can use a convenient syntax to bind and curry any
  parameter directly at any stage of evaluation. Which gives us
  something that looks like this:

  #+begin_example
  atan2 :: .infer -> Real< -\pi --- \pi >
  """
  Evaluate the angle and quadrant of a vector given by {x}
  and {y}, returning with the sign, the arctangent of \frac{y}{x}.
  """
      x : 1.0 :: Real
      """ The x-like coordinate of the vector"""
      y :: Real<.if .equal(0, x) .then NonZero>
      """ The y-like coordinate of the vector"""
  = ccall "math.h" 'atan2' (y : 'double', x : 'double')
  #+end_example

  Which exposes a named object that can be invoked both as a function:
  #+begin_example
  atan2 1.0 2.0
  #+end_example
  as well as with named bindings:
  #+begin_example
  atan2 (x: 1.0, y:2.0)
  #+end_example

  The power of this approach also comes into play when we do things like:
  #+begin_example
  atanNormalised = atan2 (x: 1.0)
  #+end_example
  and then invoke:
  #+begin_example
  atanNormalised 2.0
  #+end_example

  The under-the-hood implementation is that every permutation of the
  function parameters is being bound to the same name. When ambiguous,
  the order of the parameter definition takes precedence, however
  other patterns of binding are possible. In fact, the type of partial
  binding is the opposite of what we would expect. Now one might
  object that the generation of \(O(n!)\) functions for each public
  function is too much. To them I say that \(O(n)\) parameters should have
  been structured better. Moreover, as the symbols are a permissible
  datatype for the language, there's not a single reason preventing
  you from simply requiring a single associative collection as input.

  The benefit of this approach is that the default values for some
  parameters are also handled by the appropriate partial binding. The
  only trouble is that one needs to specify the type of binding
  beforehand using the keyword =.defaults=; so you would have something
  of the sort of
  #+begin_example
  atan2 2.0 .defaults
  #+end_example
  but
  #+begin_example
  atan2 (x:2.0) .defaults
  #+end_example
  is a partial application with a single overridden default, and
  #+begin_example
  atan2 (y:2.0) .defaults
  #+end_example
  is the same as the first example.

  Combining this with the =applyArgs= function we get the ability to
  code in the familiar decorator style that is commonly used in
  Python. To avoid several thousands of hand-rolled applyArgs
  implementations, syntactic sugar will be applied, and the usual
  asterisks shall explode the collection into the appropriate binding
  pattern. The only significant difference here shall be that the
  double asterisks are no longer required to delineate associative
  structures.

* Piece on the experience of Haskell.

  Haskell is named after a computer-science legend Haskell Curry. You
  may be interested, as to why it's named after the first rather than
  the last name. It's a humorous story, related to a certain actor who
  happens to share the last name.  Haskell is not a top-chart language
  unlike Python, and unlike Python it deserves to be one. Similar to
  the aforementioned workhorse of Data-science it has syntactically
  meaningful white-space, an interpreter, and a particularly
  easy-to-use prelude. This is the exhaustive list of major
  similarities. Otherwise, the two languages couldn't be more
  dissimilar. So Haskell is a worthwhile orthogonal language to orient
  our decisions with respect to.

  Haskell is a remarkably terse and incredibly easy-to-reason-about
  language. It owes all of these properties to a single principle of
  that uncontrolled behaviour is hard to track. Thus, all sources of
  behaviour that cannot be controlled in normal imperative languages
  (normal not being used in any positive light, being a racist is also
  shockingly normal for too many humans), are being controlled via the
  type and the kind system. To understand some of the design choices
  that we have just accepted up to this point, we must take a brief
  detour into what those concepts mean under the hood.

  Before we do that let's recap what Haskell is: a strongly typed
  purely functional language. The strongly typed, means that it cares
  about what the information is beyond the bit pattern that represents
  it, and it will not allow the programmer to misuse or misinterpret
  the information. The purely functional means that it has a very
  strong opinion on what the side-effects are, and where they're
  allowed to appear.

  It is now important to note what side effects are: when a piece of
  code is being read, it shall have effects, otherwise, why would you
  write the code. Normally, this is a binding of a value to a name, or
  an invocation of a function call. A function itself has an effect,
  that can be controlled: it has an input, and can produce one single
  output, both of which have a definite type. A side effect is
  something that is not declared or not properly explained.

  The whole idea behind the two major programming paradigms:
  functional and object oriented stem from the idea of controlling
  effects, and mitigating the unintended side-effects. Consider that a
  function call in Haskell can only produce the effects that are
  permitted by the type of the call: if a function is =Int -> Int=,
  then the function can only behave in one way, produce a result that
  only depends on the global constants, and the value being put
  in. In, say, Java, the method is allowed to do a little more, it can
  affect only the public state of the function parameters, and the
  internal state of the parent object, if there is one. It is also
  permitted to throw exceptions but only if the signature permits it.

  As a result one can clearly reason about a few invariants of the
  program: in Haskell, you can be sure that the **only** location
  where new values can be created, are within functions. New values
  can only be created, but you cannot re-bind to the same name, this
  is sometimes phrased as immutability, but I think that this obscures
  the conceptual intent; why is Haskell a purely functional language,
  rather than an immutability-oriented programming language?

  The important similarities between our fledgling language and
  Haskell are many. But there is a crucial difference.

** The Genius of Monads.

   The unfortunate issue with Haskell and many other functional-like
   languages is that one cannot quite agree on what side-effects
   really constitute. Haskell, subscribes to the mathematical concept
   of a function: it has inputs and has outputs, and for the same
   inputs it's guaranteed to produce the same outputs.

   As a result no Haskell function can depend on any external mutable
   state. So a function cannot for example read from a pseudo-random
   number generator. One thing that gave me considerable food for
   thought, was why I couldn't assign Date object in Haskell a default
   value that was the time at the point of execution.

   Moreover, it cannot cause any mutation of any state. This is not
   strictly necessary on its own, but it is a logical conclusion of a
   few compromises that will have to be made. If you want a language
   that operates on complex data-structures using functions, you can
   either require that the data-structures, however non-trivial are to
   be fully copied each invocation, or that the data-structures cannot
   be mutated after being accessed by a function.

   That last part has far reaching consequences: the biggest and best
   one, is that the concept of where something is becomes utterly
   irrelevant to obtaining the right answer. No longer are we forced
   to do what many call the Von-Neumann type of programming, where we
   model our machine as a finite collection of boxes where one can put
   and take things out of. Unfortunately this also means that arrays
   are no longer the most practical data-structure. You not only cannot
   dynamically adjust their size after instantiation, but you can only
   create one if you know the exact values of each cell. This doesn't,
   thankfully mean that the compiler cannot make use of efficiently
   packed data-structures such as arrays, just that exposing them to
   the programmer is no longer a good idea. Hence why your MLs and
   Haskells and Lisps expose a default implementation of a linked
   list, despite this particular data-structure being outclassed by
   arrays in every case where the access is to a collection of
   elements, and not a particular pattern of linking as in e.g. a
   Fibonacci heap.

   But is input/output a side effect? This question differentiates
   within functional languages more than it differentiates functional
   from other languages. A purely functional language such as Haskell,
   recognises that the input and the output both constitute external
   state, in one case we ask for the state to be read in, and in the
   other - to be altered. Strictly speaking, if we permit I/O as a
   non-side-effect, none of the internal state of the program can be
   safely assumed to be side-effect free, unless such bubbles of state
   dependent and state-independent, pure functions are being tracked.

   As a result, languages are either not-strictly functional, or
   strictly not-functional. Almost everything that we'd want a useful
   program to do is going to involve one form of side-effect or
   another. Standard ML as an example of a not-strictly functional
   family throws in the towel and states "Well, we had to resort to
   imperative at some point, let's introduce the assignment and
   dereference operators". Indeed the operators are not allowed to mix
   with functional operations of binding and evaluation, hence one can
   at a glance isolate the imperative idioms and tell if a function is
   pure or not. However, the issue here is that ML is a
   functional-first, but still imperative language. C++ has all of the
   facilities to produce similar functional code, and =const=
   correctness with making almost everything that can be =constexpr=,
   a compile-time constant, makes it a viable platform to produce
   purely functional code. Lisp, while promoting the functional style
   in one of its dialects, does not enforce one, so it too, sadly,
   falls into the realm of not-strictly functional. The other
   alternatives are all of the interpreted calculators, and some of
   the command-line applications. Purely functional languages rarely
   have much utility beyond being REPLs, unless they do something
   exceptionally clever. They are not-functional, in that one would
   not want to use them to design a program, even if one strictly
   could. In other words, one cannot fully eliminate side-effects as
   they are what most programs are designed to produce.

   Notice, however, that we have omitted Haskell. It fits neither
   category, because it is a language that has features of both and
   weaknesses of neither family. The workaround is known as the
   *monad*, the single most important concept in Type theory. All one
   needs is an Algebraic Datatype system, and one can build into the
   compiler a method of controlling the kinds of side-effects that one
   would want to produce.

   Haskell has a monad for Maybe, symbolising the side-effect of not
   producing an answer, or an answer of a different type. There is a
   monad that represents input/output. There is a monad that
   represents the possibility of mutable state. But is this any better
   than the "F*** it, let's just put Algol 68 notation for
   imperative".

   As I've mentioned previously, we cannot eliminate side-effects, but
   that is not sufficient justification to let them roam free within a
   code-base. What Monads do, is they recruit the typing system to help
   control the propagation of side-effects. In a Standard ML code-base
   it is impossible to know how many functions are pure based on
   signature alone. In Haskell, you know that =main= is an impure,
   state-ful side-effect permitting function because you know that it
   returns an =IO=. Every function that uses the result of an impure
   function is itself impure, hence the programmer is acutely aware of
   the propagation of side-effects.

   But Monads do come with problems of their own: there is no
   consensus currently on how to safely control the propagation of
   top-level mutable state. The whole point of the language design was
   to make this impossible so it is no surprise that it is difficult
   to represent the undesirable. Another issues is of that different
   types of side effect can cross-contaminate the functions. That may
   become problematic once one realises that =IORef Option x= is not
   the same as =Option IORef x=.

** The genius of the Option monad and exceptions

   As we have mentioned previously, the presence of the choice
   operator and value-dependent typing allows us to completely ignore
   the need for constructions not unlike exceptions. However, because
   the types can differ drastically, and unlike Haskell we can have
   functions that differ only in output values, how do we track that?

   The answer is simplicity itself. Suppose we have a function that
   returns the division of one integer by another.
   #+begin_example
   div:: Int -> Int<!equals(0)> -> Int
   div a b = a/b
   #+end_example
   From the cursory glance all seems fair, but as soon as one tries to
   use this function, they encounter a problem:
   #+begin_example
   constraint !equals(0) of the second argument is unsatisfied.
   #+end_example
   So one either needs to have an =IF secondArgument!=0= or use a
   choice operator. That is mostly very correct if you are the
   immediate user of the library that provided the function to you,
   but suppose you are a middle-man.

   In Haskell, Standard ML and Rust, the standard way is to wrap the
   answer in an optional type, and simply return =Nothing=, if the
   divisor =b= is zero. This is certainly possible and acceptable in
   our language, as it only takes the presence of algebraic data-types
   to do, however, constraints offer a better solution.

   #+begin_example
   div :: Int -> Int<equals(0)> -> .error(DivideByZero)
   div a b = () |-> .panic(.function_s, (.functionArgs), (),  "Cannot divide by zero. ")
   #+end_example
   This is a standard pattern to be adopted within the language. First
   of all, note that because of the dependently typed language, we can
   indeed have overloads that have =Ints= but of different values. The
   compiler will replace the resolution of the function with a
   run-time conditional, and either follow one branch or the
   other. However, the usage of our function =div= becomes a delight:
   one no longer needs to ensure that the exceptional circumstances
   never arise, but can use it normally.

   First of all, obtaining an error value does not immediately lead to
   a panic, trying to use it or assign to anything will though. A
   parallel must be drawn with the =null= pointer. The presence of
   such within the code is normal, but trying to dereference one is an
   issue. What this pattern permits one to do is to handle the error
   in some way, by unwrapping the error and using the contents of the
   panic constructor to do something useful. Of course, there are
   instances, where we want to fail fast, and panic as soon as an
   exceptional circumstance occurs.

   Secondly, whenever a panic takes place, we have the option of
   receiving differing levels of information. The nominal way that
   Python and Java have elected is to provide the full call
   trace-back. The programmer will see the file and the line of code
   from where the error message was thrown, but not know the
   circumstances under which this has taken place. More importantly,
   it will not know which combination of inputs raised the exception,
   just that it took place in this call-stack. Unless the file is
   poorly organised this information is utterly useless to the
   programmer, while the name of the function from which the
   exceptional state originates, *and* the input values, particularly
   if what we deal with is a pure function are very important.

   Also, just like we mentioned throughout the text all of the
   provided information, while conventional can be substituted and not
   just passed by rote.

   The first argument of the panic, is the symbol that is responsible
   for the erroneous state. In this case, this is the function in
   scope. If we wanted to, we could delegate that to the enclosing
   function: =(.function_p)_s=. Recall that the underscore is an index
   operation. For each value, there is a =_s= which means a symbol's
   symbolic representation (note representation, not pretty print or
   string value), =_p= which means the parent(s), and =_c= which means
   the children. It is also possible, if some of the optimised and
   constrained versions no longer preserve the calls to other
   functions to artificially inject the name of the symbol. It is
   frowned upon to do so in manually optimised code, but permissible
   for automatic optimisations.

   The second argument is the ordered collection of the input
   arguments by value. Why do we specify it? Because while in this
   particular case we've been dealing with a pure function, a general
   panic can be raised from within a procedure, and that procedure can
   depend on global mutable state. It is possible that an exceptional
   state results from an incoherent external state: and the programmer
   should be notified of this issue. The format is that of an
   associative collection: so external state shall be represented as
   #+begin_example
   .panic (.function_s, (arg1_s: arg1, arg2_s: arg2, externalState_s: externalState), (), "")
   #+end_example

   The unit in the third argument is the partial result that the
   function has produced, if any. Sometimes, even a failed calculation
   is recoverable, to an extent, and while it is not a great idea to
   rely on this pattern for all intents and purposes, it is well
   acceptable in some ranks. So if the matrix diagonal-isation function
   found that the matrix is singular, the third argument can contain
   the singular value decomposition, or a lambda that when evaluated
   amounts to the SVD, as it stands to reason that this expensive
   calculation may be unnecessary.

   Finally, the error state must be explained. Ideally, you want a
   reference to what can be done, and some reading material to get up
   to speed as to why this is an exception. So, in our example a web
   page explaining why dividing integers by zero cannot be simply
   swept under the rug with a single value called =Infinity=, would do
   well.

   Finally, how do we use this function? People familiar with rust,
   will already have noted a few similarities to their best practices,
   namely in the language and the use of the concept of =.panic=. The
   choice operator here makes a return in one last context.

   #+begin_example
   q ?= div a b
   #+end_example
   Our call to =div a b= is ambiguous, because we haven't constrained
   either =a= or =b= visibly within this scope. It is worth
   remembering that the type of a value is not utterly meaningless,
   and while return types with different constraints can fail overload
   resolution but can be safely bound to a single name, different
   types *need* disambiguation for assignment. The return value of div
   can have different values, that are not included in a generic class
   or algebraic overload, so the assignment is ambiguous. The question
   mark in this context, signifies that the subsequent treatment of
   the two values is to be branched, and in both branches, the return
   of the thing on the right, must be bound to the symbol on the
   left. It is a form of making use of the generic programming
   features, without having to define a named generic.

   In this particular example, assigning the quotient to q if the
   quotient is an =Int=, is fine. However, the alternative is a lazy
   panic.
   #+begin_example
   p = q
   #+end_example
   will not do anything spectacular. It will not fail, immediately,
   just have the type of =Either(Int|() -> .Error(DivideByZero))=, which can
   be inferred. However, subsequent function calls shall have to
   branch depending on the value,
   #+begin_example
   x = power 2 p
   #+end_example
   if there is an overload that includes an Int where p is, that will
   be the branch that will be boxed into x, however, it stands to
   reason that not many functions will have .Error as input type. This
   will result in an un-handled code branch, which in case of errors
   has the helpful message
   =Overload resolution failed. No error handler bound to power :: Int -> .Error(DivideByZero) -> *. Use a choice operator=.

   If you follow the advice, you can do
   #+begin_example
   x = (power 2 p, p)?
   #+end_example
   That last bit, will eagerly evaluate the lambda, and unravel the
   panic.  This will immediately stop the application.

   For completeness, we shall add the following syntactic sugar:
   #+begin_example
   q ?= (div a b)()
   #+end_example
   will eagerly evaluate both branches, mapping the Int branch to an
   Int, and every lambda to their return type. The question mark shall
   generify the return type of div, and the bang -- implies eager
   evaluation. The first set of parentheses is needed to priorities
   the evaluation of the function and to apply the evaluation token to
   its return type, rather than
   #+begin_example
   q ?= div a b ()
   #+end_example
   which is equivalent to
   #+begin_example
   q ?= div a b
   #+end_example

   This is not quite the syntax used in =Rust=, but very similar. A
   worthwhile sacrifice in my opinion.

** The shortcomings of Haskell

   Haskell has very narrow adoption for many reasons. The main reason
   is that it offers an improvement only if you have a team of
   mathematicians that want to program simple things, and have not yet
   learned how to do that in C or Python. But to wean people from the
   proverbial fast-food, requires considerable advantages, and being
   healthy is not immediate or appealing enough on its own. One knows
   that biscuits taste better, even thought their nutritional value is
   dubious. And while C is akin to tofu, in that in the right hands it
   can produce remarkably healthy nutritious food, Python is similar
   to ready-made fast food, in that each =pip install= proliferates a
   bit of cancer.

   As we've mentioned, the workloads in which different kinds of
   Monads have to be inter-mixed are not particularly well-suited to
   fast, iterative production of code.

   Haskell has a lot to make it appealing in the long-term
   perspective. Software written in Haskell is much more predictable,
   and hence predicting bugs is easier. It is remarkably terse, in
   that even for very complicated projects, one can keep the
   complexity down to a few files and pages of code, with very high
   signal-to-noise ratios. Software is compos-able, in that when
   writing a large project, chances are you could strip out and
   publish most of your maintenance code and someone else might find
   it useful. Because most of the advances are purely theoretical, it
   lends itself well to iterative refinement and high degree of
   polish.

   As if those things weren't enough on their own, Haskell's
   infrastructure is quite good: you have a wiki, and a considerable
   amount of pre-made libraries, a robust build and project-management
   system, and tons of libraries. Stack, GHCI and many other things
   don't seem to produce the enthusiasm for everyone to use Haskell at
   their workplace and in their project.

   So what gives? Why is Haskell, this wonderful, wonderful language
   not at the top? For starters, it doesn't offer an immediate and
   obvious improvement; in order to make use of the terseness, one
   first has to migrate to Haskell. Another issue, is that one cannot
   gradually wean the code-base away from mutable state; if one doesn't
   have lots of =constexpr= statements in C++, I doubt that simply
   migrating it to Haskell, would make adding those statements any
   easier. And unlike the migration to the Unix-conforming shells,
   there isn't a convenient tool that lets you move a project that
   *could* and *should* be purely functional, to become purely
   functional. The advantages of Haskell barely outweigh the downtime.

   Not to mention that Haskell is supposed to encourage the correct
   design from the get-go, rather than improve what's already
   available. So if you have a team that works in C++, C, Rust or even
   OCaml, you stand to gain protection against the bugs that you've
   already encountered and fixed.

   So, if our language is to achieve wide adoption, one has to be able
   to address these issues in a convenient-to-the-end-user way.

** A segmented distribution model.

   We shall start by observing that the common text-only distribution
   model of most programming languages is sub-optimal. There's room
   for rules about tabs vs. spaces, about aligning and restructuring
   the text file of the program, to be more readable. What is worse,
   these rules are merely suggestions, and valid code that compiles
   can all fit on a single line with no white-space. Moreover, sweeping
   changes such as refactoring the name of a function can break the
   entire program, while changes to the documentation that do not
   correspond to the reality can rarely be caught. There's need for
   extensive testing that is almost always done via a wrapping library
   that effectively produces a decorated function or even program that
   does nothing other than what the programmers themselves could do
   within the confines of the language. As such, tools that can be
   standardised at most include a compiler and maybe a package
   management system.

   However, we must do better.

   We shall borrow the idea of a distribution model that does not
   revolve around a text file, from a lesser-known language Unison.
   The source code of a program is actually going to be the AST, that
   is being generated every time your favourite lsp-supporting text
   editor "saves" the sham text file, and a supplementary
   representation file. So all of the syntactic sugar is kept separate
   from the actual optimised source code, and as such, simple
   refactoring such as renaming is trivialised. Also, multiple
   representations of the same program may coexist within the same
   code-base, to represent different approaches one may take to
   formatting, whilst keeping such politics out of the internals of
   the language compiler.

   Another idea we shall half-borrow is that the library should
   persistently include snapshots of different incompatible
   states. Instead of making the entire code-base immutable and
   persistent as is the case with version control in Unison, we shall
   allow the code-base to be mutated, but keep the original
   states. Every modification to the code-base has to have a specific
   purpose, it could be a refactor, a bug-fix and an optimisation, but
   mixing many of them isn't usually a good idea. So instead of
   allowing the users to modify the code and then retroactively commit
   the state of the code-base into version control, the programmer
   requests the permissions needed to do a specific modification and
   only facilities necessary to make that specific optimisation are
   provided.

   If one for example is optimising a database, it stands to reason
   that functions will not be merely renamed, but their behaviour must
   be fundamentally changed. In the optimisation mode, it is prudent
   not to allow the programmer to commit a refactor that changes the
   name of a function, but to allow the change of the behaviour to
   take place. As a result, one does not have to rely on the good will
   of the programmers in question to remember to retroactively commit
   only the necessary changes, or to bunch together modifications that
   are not necessarily linked. You know, rigidly, that only an
   optimisation must preserve the public API of the library, and this
   is enforced at the language level, because unlike traditional
   text-file based distributions, the programmer can only modify the
   code base in a way that is coherent with the goals. In other words,
   if you change the public API while optimising the behaviour, it's a
   compile time error, that should not be allowed to commit.

   Similarly, refactors should not result in changes in the outwardly
   visible constraints. Ideally, a refactor, should only change the
   outwardly visible API itself, but not the behaviour or the
   constraints. Non-public functions are allowed to be modified to an
   extent, but if the constraint propagation results in a different
   set before and after a refactor, then this is hardly a
   refactor. Restructuring a code-base, however, can create a change
   of the public API, and this means that only in cases where a
   library has had a change that is flagged as a restructure, do we
   actually need to review the behaviour of our programs.

   A parallel to version control systems is not at all
   accidental. Barely anyone ever forgets to do conflict resolution
   whenever a newer version of their code is being pulled by
   git. However, many do forget to update the libraries if the public
   API may have changed. Moreover, it is frequently the case, that
   upon upgrading, one has to choose between either the old or the new
   version of the library. One relies on the fact that the features to
   be removed shall be signalled as deprecated long before the
   removal. However, what we want to do is to live in a world, where
   upgrading to a newer version of a library leaves you with the
   option to not break the system that you already have, but will
   constantly nag you to update the one that you have. Effectively,
   because the library is shipped as an AST, and all previous states
   of the AST are present, one always has the option to use an older
   version for the program to work, while labouring to resolve the
   broken API and providing an update that does not depend on an older
   public API.

   The only price one has to pay, is that for all of this to work,
   each library must be shipped not only with its source code, but
   with the source codes of all of its predecessors. This is an
   opt-out feature for the user: much like one can do a shall clone of
   a version control system, one can strip the library to the bare
   minimum that is effectively the same as what binary-only
   distribution of modern dynamic libraries comes down to. However,
   one has the option to produce a shallow, but special version of the
   library that carries with it the information needed for constraint
   resolution, and a deeper yet clone that includes the current and
   one earlier version of the public API. It stands to reason that the
   extra size on disk is a small price to pay for such flexibility.

   In the world of embedded software, one must of course resort to
   highly optimised builds, and that is what we intend to have. In the
   world of servers, avoiding downtime is one of the driving factors
   for outdated software in use, which compromises security. Having
   the option to always safely upgrade, knowing that the software
   shall revert to the state that is consistently working and only use
   the new public API when all of the programs are ready, makes the
   task of maintaining a server much easier. Finally, the end user has
   one less variable to worry about: a deep clone of each library
   pulls in the entire history and compiles as needed the version of
   the library that suits the software already installed. A user no
   longer has to track down =libsdl-legacy=, but can rely on the fact
   that =SDL= contains all the code needed to run both the very early
   code and the very latest, just takes a bit more storage
   space. Because some people might take issue with the extra space
   taken up by source code, (and the source code does add up to a lot
   if every library is distributed this way), one can opt to shallow
   clones that have the extra safety in the constraints, and warn
   about the need to include deeper clones as needed, but not
   globally.

   The benefits of not distributing text files, but keeping in and
   integrated version-controlled AST are not limited to stability and
   maintainability. Today, most optimisations are completely opaque to
   the programmer, as the kinds of constraints that lead to certain
   optimisations are not well-known and usually related to the parsing
   of the program source code. If the programmer wants to know what
   the compiler optimises their source code to, the only and best way
   to figure that out is by looking into the assembly or using a
   debugger. However, this makes reasoning about any particular
   snippet of source code remarkably difficult. What one wants to be
   able to do, is to have an AST that represents then naïve input, and
   the AST which is the effective optimised output, to cut out the
   need to optimise the same code each time a source is being
   compiled, but also to make it easier to reason about the program.

   Because of the presence of the constraint based typing, the
   abstract source distribution, certain patterns of optimisation can
   be extracted and suggested for distribution as a development
   library, that can be plugged into other programmers workload.

   For example, let's say that we have a code-base that has a sorting
   function that uses a temporary object for swapping two
   objects. Suppose that we have said that we were going to optimise
   the program, by adding a specialised sorting function that uses
   binary exclusive or to swap two integral types. Ordinarily for that
   optimisation to be available to other uses of our language, someone
   like myself should spend the time implementing this explicitly in
   the compiler's optimiser. It's laborious to do so in every
   conceivable instance, but the information on what needs to change
   is readily available. In our design, the programmer has clearly
   delineated the constrained specialisation as an optimisation, and
   the types of modifications available to them will likely not seep
   beyond the aforementioned sorting function. So instead of keeping a
   team that produces the optimiser modifications, we keep an
   infrastructure that will, during compilation, suggest this
   optimisation to be published and submitted to the standard library.

   As a result, the standard installation of our languages' compiler
   shall be lightweight, and highly modular: only relevant
   optimisations are searched, and applied explicitly in a way that a
   programmer is able to audit. As a result, a programmer can either
   use a standard optimisation fetched from a large repository of
   development modules, or modify it slightly to suit the purposes of
   their program. Moreover, because the parsing of the source code
   into an AST is not necessary and high-level behaviour optimisations
   are applied within the AST in situ, the compiler has relatively
   little work to do: a parser tool handles the syntax validation and
   translates the textual representation that is made available to the
   programmer back into the AST, but the translation is carried out
   each time the code-base is modified. Because the language tools
   don't rely on humans breaking the project down into behaviourally
   isolated object files, but rather does so based upon the concrete
   implementations committed to the code-base, the linking can be done
   in an eager fashion. The compilations should be relatively fast,
   which allows, combined with the opt-out distribution of the source
   code with the libraries, allows one to adjust the code-base to
   static linking or break a static object into multiple shared
   objects based on external criteria. Things that ought to be
   separate can be separated out, and functionality present in any
   particular program can be exploited by any other program, without
   having the developers of either programs to make any provisions to
   begin with. One is also no longer at the mercy of common decency to
   give credit for the work: if your program uses a function of
   another program, your source code's representation shall contain
   proper credit to the author of the program that you use as library.

   We have also converged upon a version of the UNIX philosophy that
   does not rely on the presence of crude tools such as command
   shells, e.g. fish, bash and =zsh=. Instead, if you know of a program
   that can has a function that plays sound files, you can use the
   exact implementation of that function directly from your source
   code, provided both programs were written in our language. So
   instead of the command-line interface being the only enforceable
   /lingua franca/, every user facing facility must be documented in
   source, and can be presented as source code documentation. So, you
   could for example use an implementation of an audio player's DSP in
   your own player, without having to run that program in full.

   This is the power of many languages, but specifically and most
   profoundly felt in lisp. Because emacs lisp package are obliged to
   expose all of their functions to the public, it is possible to
   reach unimaginable heights of complexity. Unlike emacs lisp, our
   language is to be compiled to an efficient abstract representation,
   requiring no interpretation at all, if no behaviour changing
   modifications are to be made. As a result, the entire collection of
   all the programs written in our language becomes a repository of
   functions and behaviours.

** Custom Primitive types

   Java and Haskell are very upfront about a clear distinction between
   compound and structured types and primitives from which such types
   can be made. The types in the area of primitives are very much set
   not by the programmer or even the compiler, but by the machine on
   which the program is to be run. As a result, While one can assume
   that every non-primitive type can be built algebraically out of
   primitives, there are very few cases when a primitive type can be.

   C++ has a much more subtle distinction in that all the types are
   stored as bytes, and it is taught to perform some operations
   bit-wise, but because inline assembly and overriding of basic
   operations like =+= or copy assignment is a thing, we can
   effectively construct algebraic data-types that are in many
   indistinguishable from primitives of a different structure. Our
   language, despite the syntactic similarities to functional
   languages, is much closer in spirit to C++.

   Now is an important time to clarify: a type carries information
   about how data is to be interpreted, not anything intrinsic to the
   data. A dependently typed system allows the interpretation to be
   more fine-grained. Because we can attach an arbitrary constraint to
   an =Int=, we can effectively create custom types that are
   represented in the =Int= space, but interpreted differently.

   But when we say =a::Int=, we have quite a bit of information
   implicit to the primitive. Normally, there would be a between
   constraint that would warn about unchecked narrowing
   conversions. Unlike =C++=, where one rarely gets told of potential
   loss of precision when the sum of two =short= s is assigned to
   another =short=. The constrain system, would recognise that
   integral types can indeed be added together regardless of whether
   or not their lengths are the same, but a narrowing constraint that
   is trowing away values is not something that the system is just
   going to let slide. In this, we can still allow implicit
   unambiguous conversions, provided that the output can implicitly be
   estimated. But the output is not always an =Int=, even if it is a
   32-bit integral type. What if it's not a two's complement? What
   if it is unsigned.

   The constraint system ensures that the signed-ness and
   unsigned-ness can be determined and safely assumed from the
   constraint and type inference, just like in C++, the "almost always
   auto" principle can save the programmer a lot of thought. But one
   still wants to be able to call specific types of objects with a
   name, rather than a torrent of constraints.

   So effectively, the primitive's names similarly to the algebraic
   data-type's labels, are a collection of implicit constraints that
   are carried by the compiler wherever it needs to interpret any
   data: so why not, as we have done previously expose the mechanism
   to the user?

   I do not encourage lots of primitives to be defined this way. There
   is a reason why I believe in not adding a built in list
   implementation, but rather to infer whether something is a
   heterogeneous list, or a homogeneous fixed array or a structure, at
   compile time. I want the compiler to be able to make a few
   decisions on behalf of the mountain of experience that we have
   accrued over the century of programming. We know which
   data-structures are good in which cases, and while we can't tell
   everyone on stackoverflow that they should be using arrays and not
   lists, we can make it so that concrete data-types are set based on
   the usage, rather than by asking the programmer to guess correctly
   in the first instance.

   However, such a system would make it impossible to create
   extensions that support exotic character types, new standards and
   exotic integer representations. So, the piece that is missing is
   the =.typedef=.

   #+begin_example
     .typedef Int
     Int :: <Integral>
         :: <fixedLength(4)>
         :: <Signed>
         :: <SpecialNumericDisplay>
         :: <Between(-2^31, 2^31-1)
         :: <TwosComplement>
   #+end_example
   Here, we can clearly see, that there is no underlying type system,
   there are no real primitives, and no deep distinction between a
   boolean and an integer. =Int= is interesting, in that it is also
   very solidly defined and can map directly onto a value in machine
   memory. When we spawn an =Int= we know that it takes 4 bytes, and
   is of fixed precision, and more importantly that it has some very
   specific numerical two's complement interpretation. We can define
   an IEEE floating point in a similar fashion, and we can include
   some of the switches for non-standard floating point behaviour into
   the definition. This is the only major drawback that one can think
   of, but exposing this information to the compiler in a clear way
   (unlike std::numerical_limits), we can and do obtain a much clearer
   system.

   Contrast this with the standard definition of a boolean:
   #+begin_example
   .data Bool = True | False
   #+end_example
   here, it is merely an algebraic enumeration. We know what needs to
   be represented conceptually, but unlike many low-level programming
   languages e.g. C++ or java, we have no semblance of the need to
   actually keep this value in a represent-able form.

** Constraint oriented typing for optimisations

   When I first studied programming, in C++, we were told that the
   declaration necessitates the allocation of memory, and that an
   =int= is always four bytes, that a ===bool= is always four bytes,
   because it has to be addressable, and that a =char= is a single
   byte, because evidently you can address individual bytes for
   characters but can't address individual bytes for Booleans. This is
   rather confusing, and was an artefact of my teachers not quite
   being computer scientists, but people who just learned how to solve
   problems with =Microsoft Visual Studio=.

   What I have found, though, is that in practice, even in less
   optimised C++ code, most if not all of the data that you allocate,
   will be optimised away. Local variables within functions,
   especially Booleans can be done away with by implying the avenue
   taken by the compiler. Disparate branches of an if, with copies of
   branch less code works just as well as boolean and checks, and
   sometimes far better, in case of loop unrolling. The trouble is,
   nobody knows exactly in which precise circumstances does this
   optimisation trigger. Moreover, people might not know if it
   triggers for all equivalent formulations, or if one actually does
   need multiple return statements in a function to ensure that no
   temporary value is allocated.

   This is why I'm so opposed to so called "best practices". Sure it's
   easy to say, don't use =std::endl=, and to parrot that advice to
   each stackoverflow post in existence. It is better to explain the
   concept of a buffered output stream and the fact that =std::endl=
   is actually a misnomer that nobody cared enough to fix, in that it
   also flushes the buffer. But often people say that "you shouldn't
   do that, because it flushes the buffer". That's not the point! The
   point of this advice is in something that is not readily apparent
   from looking at the source code of the standard library. The issue
   is that the whole point of buffering the output is that a system
   call to produce output is expensive, so it is worthwhile to save as
   much information into a buffer as possible, and only write whenever
   a buffer is full, or when the programmer signals that the
   information is to be produced as output. There are situations where
   you might actually want to do that, e.g. for logging.

   Even so, this relies on the ability of the programmer to attain
   information outside the source code of the standard library to
   understand the code that is laid out, so providing a model that
   does not have any pretence of doing one thing only and then doing
   something else is a detriment to our ability to reason about
   code. And reasoning is what we do most of the time, we don't write
   everything from the ground up, we reason about snippets of code
   that do close to what we want, and adapt it to serve our needs. We
   want for the source code to be as representative of what the
   program is actually doing, rather than what we /think/ it's doing.

   The biggest issue I can see in this, is that it is assumed that the
   programmer's specification and what the program actually does are
   one and the same. But the trouble is, because most of the
   optimisations are done on a very low level, the programmer can look
   at two identical programs, one having a more intelligible but less
   efficient spec, i.e. source code with recursive function calls and
   the other with a =for= loop and say, the latter one is faster,
   despite both essentially doing the same thing, possibly unrolling
   the loop altogether.

   The trouble with LLVM and GCC is that the programmer can only use
   trial and error to guess when each optimisation shall be
   triggered. I cannot program relying on the tail call optimisation
   to kick in, because I have to look up a forum post that states
   whether the particular version of the particular compiler for my
   particular language recognises that the definition that only
   recursively calls another function and carries no information from
   the stack other than the end result can be unrolled into an
   iterative loop and possible even further than that. I know how to
   trigger it in C++, but a different language can have implementation
   dependent idiosyncrasies.

   I want to take the guessing out of the game. As stated in the
   previous sections, what we want is for the source code itself to
   already be optimised, so that the final step is of linking and very
   cursory particular-case irrelevant data types of optimisations. We
   want for the programmer to know what kinds of changes to the
   algorithm the program will undergo, so that they don't spend the
   next week adding an optimisation that is already there, but
   hard-coded into the specification of what the program must do,
   adding nothing to the performance, but making the source code far
   less intelligible. I purport that if the programmer could
   explicitly state the optimisation conditions, and apply them
   automatically, they can both keep the more understandable
   mathematical definition of a Fibonacci sequence as a recursive
   definition, but also allow it to be replaced with a more efficient,
   but less mathematical imperative. As we've talked in the previous
   sections, we also want for these constraint oriented optimisations
   to be made available to the public, and be re-usable. So when
   someone else writes a recursive sequence, that has the next number
   depend on the three previous numbers, the compiler simply says
   "hey, I noticed that your function here is following a common
   pattern with a Fibonacci sequence. This guy found that if you can
   set constraints =<A>=, =<B>= and =<C>=, that you can make this an
   iterative function that computes in no time. I will try to
   automatically see if I can do that, but I might not, so you might
   want to consider how you can constrain the data, or if you can do
   that at all, without breaking what your program is supposed to
   do."

** The new programming workflow.

   In Haskell, or in Rust the common workflow isn't to create a text
   file and try to immediately hack at it, and then pass it directly
   to the compiler. Haskell has both Stack and Cabal, which offer the
   option to have them create a sane directory structure and organise
   your files in an acceptable, standard fashion. There are however,
   some fundamental issues with how this is normally done. A lot of
   the heavy lifting is left to the editor plugins, that need to make
   sure that the right things at the right time are being suggested
   when the right file is in the active editor buffer. But consider
   how much work needs to be done to refactor a single function to be
   named differently.

   First, the worst efficiency should be given to trying to do that by
   hand with search and replace. No syntax awareness, and the human
   being has to process the jump to a new location, parse the context
   and figure out if the thing is acceptable to rename. Then do that
   for all files. I'm sorry did I say human? That is also how a
   computer would do it.

   Think about syntax highlighting. The text editor has to work hard
   at re-parsing the file each time a modification is being made to
   the context. Moreover, even if your text editor was sure that the
   symbol under your cursor was indeed a constant, the compiler may
   have a differing opinion. The LSP does not require that the same
   tokeniser and parser operate, and worse yet, the results of the
   tokenisation at compile time do not affect the parsing of your
   editor, and the hard work that your editor has done is being thrown
   away too.

   How about, we do things differently. We've already established,
   that the source code contains the AST of the code-base, segmented
   into independent modules, and textual names, constructs and
   bindings attached to the AST called the Human interface
   representation. So as in the example, we want to change the name of
   a single function.

   Both in the LSP world, and in the world of our language, we can
   have something very efficient. The command to change the name of a
   function really touches only the area of the HIR that's responsible
   for that function. LSP will do something less efficient, and
   probably touch every file where a syntactic unit of that name,
   i.e. a symbol is present. Similar to LSP, the editor would probably
   not need to re-parse the entire file to get up to date syntax
   highlighting, only one function name changed, and the LSP is aware
   that the highlighting doesn't need to. However, in our language, we
   have one more benefit: the change is non-destructive, and the
   version control system is able to track the specific change leading
   to a much smaller diff.

   Let's look at a slightly more complex example. Suppose we want to
   alter the behaviour of one internal function. In a normal language,
   you just =grep=, (in FORTRAN you =grep -i=) and edit the files that
   have the name in it. Perhaps you're adding an extra function
   parameter. After being done, you =git commit= and say =optimised
   function=, because it's the end of the day, you're tired and you
   can't even be bothered to remember the name of the function or why
   you needed to optimise it.

   By contrast, in our case the approach would be very different. The
   first step is that you need to explicitly say what you're going to
   do with the code-base, before you're allowed to modify it. You will
   say something like =optimise reverseAppend=. What this will give
   you, is a direct buffer in a compatible text editor, not a text
   file. What you will see immediately, is that the only thing you can
   change, is the internals of the function, everything else is greyed
   out as non-modifiable, i.e. you couldn't, even if you wanted to,
   modify anything else. That's because optimising involves preserving
   the behaviour, but improving the run-time characteristics. Adding
   an extra parameter at the public API level is not exactly an
   optimisation, it's a restructuring.

   Now assume that we did want to optimise the function, and that the
   tools simply reminded us that we shouldn't touch the public API of
   that function. Now we've added a private sub-function that has an
   extra parameter, and does what we want, and we only use the old
   function as a wrapper for the behaviour. Then, we hit save, and the
   modifications are committed to the AST and the HIR, however, they
   are not finalised and the code-base is not in a final state. The
   compiler must first try and see if there are errors beyond
   syntactic ones, e.g. did we break any of the previous contracts
   with our new implementation. Then, when we know that the code
   works, we will be shown a list of possible optimisations: tail call
   optimisation, for instance. We shall apply those manually. Finally,
   we shall have a commit message, which is the final thing we must
   do. Except, now that we have a syntax aware version control system,
   the compiler already has a suggested message "optimised
   =reverseAppend=: added sub-function, optimised via tail call (HASH of
   the tail-call-optimisation)". You can edit this, but it's the end of
   the day, and the commit message is quite accurate.

   Afterwards, you get another prompt: make optimisation public?
   Because you are a nice FOSS abiding programmer, you know that the
   only reason why you are allowed to use a network connection to
   fetch the most appropriate optimisations is because when you make
   one, you can make it public and the one you used, was made public
   by someone else. You opt in, because it stands to reason that you
   might not always want to, if you feel that the optimisation is not
   quite ready, or generalises well enough, or if you feel that this
   is best kept a trade secret (SHAME, SHAME, SHAME). You publish the
   optimisation and call it a day.

   Next time, when you boot your compiler, it flags that your
   optimisation has a bug. It says that user =DarthVader609= has found
   that the optimised function produces different behaviour to the
   un-optimised one. You then, can reverse the optimisation in your
   code-base. You say =revert optimised reverseAppend=, and publish the
   optimisation as buggy, citing the original discoverer:
   =DarthVader609=. You refactor the code-base: do =rename
   veryBadFunctionName -> iWasTiredOk=, and then, you are given a flag
   that the tail-call optimisation was to blame for the regression. A
   few moments later, you find a =bugFix revApp: forgot to check the
   base case.= and the option to roll back in both the changes that
   you made yesterday, and the optimisation that caused the
   regression. You opt not to apply the hot-fix to the tail call
   optimisation but keep the helper function with two parameters. You
   then find that =DarthVader609= has their own version of tail call
   optimisation that you decide to try. As a result, your function
   works, and you don't have to rely solely on your knowledge of the
   language or inspect the optimisation, but you simply know that the
   author =DarthVader609= has written much of the standard library
   optimisations, though the optimisation is in the community limbo
   for now, it will become standard rather soon.

   If this sounded too scary because of the kind of social interaction
   that you might have to do, recall that everything here is an
   opt-in. You can apply your own optimisations and a collection of
   well-tested optimisations will be put into a segmented
   repository. There will be a version that's pretty much the standard
   library of optimisations and there will be one that is the language
   user repository with dank memes.

   It shall now become increasingly obvious why keeping comments
   strictly out of code is such a necessity. The optimisation can
   carry explaining comments that shall have to transform with the
   AST, to reference the current context of variable names. We cannot
   allow the comments to be unchecked plain text, for the same reason
   we don't want to have to do textual replacement every time we want
   to change the name of a function.

** Assembling it all together.

   We are now at a point, where one is reasonably suspicious if one
   can make such a system at all. It is conceivable that a constraint
   resolution system can be made to work. In fact, it is very similar
   in scope and spirit to the feature recently patched into C++,
   called concepts. An algebraic type system can also be made to work,
   one only needs to look at the parsers for open source compilers for
   Standard ML or Haskell, to see that it is not itself a complicated
   ordeal.

   The issue is that the intermediate representation that Haskell made
   use of, =SystemF= was built to support a functional language first,
   and expects the optimisations to be done, as is tradition before
   producing a linked and compiled binary, but not at all in a form
   that can be exposed to the programmer. We cannot similarly
   trans-pile to C, or use existing infrastructures like LLVM, because
   we would then concede to the traditional form of opaque
   optimisations that are eliding the source code completely.

   We don't want that. We don't want to depend on any existing
   convention, because programming by convention is the one thing that
   we're trying to avoid. Moreover, while it is true that today almost
   everyone has a C compiler, that might no longer be the case several
   years from now; it might so happen that Zig takes over the world,
   or that java-like byte-code distribution becomes standard. So, we
   must have a mechanism to transform the language that we have in the
   high level into a language that can be optimised at the low level.

   The only solution that I can think of is that our language, in
   addition to providing a new high-level grammar, should have its own
   flavour of assembly. So what we want is to have a mechanism of
   implementing high level functions in a very, very low level form.

   Compare
   #+BEGIN_SRC haskell
   square :: Int -> Int
   square x = x*x

   main = do
     print $ square 2
   #+END_SRC
   which is a fairly straightforward thing to optimise. Unfortunately,
   the assembly to which it corresponds is anything but compact:
   #+BEGIN_SRC asm

     Main_$trModule4_bytes:
             .asciz "main"
     Main_$trModule3_closure:
             .quad   ghczmprim_GHCziTypes_TrNameS_con_info
             .quad   Main_$trModule4_bytes
     Main_$trModule2_bytes:
             .asciz "Main"
     Main_$trModule1_closure:
             .quad   ghczmprim_GHCziTypes_TrNameS_con_info
             .quad   Main_$trModule2_bytes
     Main_$trModule_closure:
             .quad   ghczmprim_GHCziTypes_Module_con_info
             .quad   Main_$trModule3_closure+1
             .quad   Main_$trModule1_closure+1
             .quad   3
     Main_main2_closure:
             .quad   Main_main2_info
             .quad   0
             .quad   0
             .quad   0
     Main_main2_info:
             leaq -24(%rbp),%rax
             cmpq %r15,%rax
             jb .Lc3lh
             subq $8,%rsp
             movq %r13,%rax
             movq %rbx,%rsi
             movq %rax,%rdi
             xorl %eax,%eax
             call newCAF
             addq $8,%rsp
             testq %rax,%rax
             je .Lc3lc
             movq $stg_bh_upd_frame_info,-16(%rbp)
             movq %rax,-8(%rbp)
             movq $c3ld_info,-24(%rbp)
             movl $ghczmprim_GHCziTypes_ZMZN_closure+1,%edi
             movl $4,%esi
             xorl %r14d,%r14d
             addq $-24,%rbp
             jmp base_GHC.Show_$wshowSignedInt_info
     .Lc3ll:
             movq $24,904(%r13)
             jmp stg_gc_pp
             .quad   0
             .quad   30
     c3ld_info:
             addq $24,%r12
             cmpq 856(%r13),%r12
             ja .Lc3ll
             movq $ghczmprim_GHCziTypes_ZC_con_info,-16(%r12)
             movq %rbx,-8(%r12)
             movq %r14,(%r12)
             leaq -14(%r12),%rbx
             addq $8,%rbp
             jmp *(%rbp)
     .Lc3lh:
             jmp *-16(%r13)
     .Lc3lc:
             jmp *(%rbx)
     Main_main1_closure:
             .quad   Main_main1_info
             .quad   0
     Main_main1_info:
             movl $ghczmprim_GHCziTypes_True_closure+2,%edi
             movl $Main_main2_closure,%esi
             movl $base_GHCziIOziHandleziFD_stdout_closure,%r14d
             jmp base_GHC.IO.Handle.Text_hPutStr2_info
     Main_main_closure:
             .quad   Main_main_info
             .quad   0
     Main_main_info:
             jmp Main_main1_info
     Main_main3_closure:
             .quad   Main_main3_info
             .quad   0
     Main_main3_info:
             movl $Main_main1_closure+1,%r14d
             jmp base_GHC.TopHandler_runMainIO1_info
     :Main_main_closure:
             .quad   :Main_main_info
             .quad   0
     :Main_main_info:
             jmp Main_main3_info
     S3lK_srt:
             .quad   base_GHCziIOziHandleziFD_stdout_closure
             .quad   base_GHCziIOziHandleziText_hPutStr2_closure
             .quad   Main_main2_closure
             .quad   Main_main1_closure
             .quad   base_GHCziTopHandler_runMainIO1_closure
             .quad   Main_main3_closure
   #+END_SRC


   it is crude, and uninspired, and the executable is screaming at you
   that it is compiled via the Glasgow Haskell compiler, and that it
   has a complex runtime associated to it that Haskell needs to link
   against. This output is something that is far too cluttered for a
   complex language on =-O1=, but this is the produce of =ghc= on
   =-O3=.

   An even more interesting observation, is that the similar code in
   Rust is far simpler,

   #+BEGIN_SRC rust
   pub fn square(num: i32) -> i32 {
    num * num
   }

   pub fn main() {
    print!("{}", square (2))
   }
   #+END_SRC

   #+BEGIN_SRC asm
     example::square:
             movl    %edi, %eax
             imull   %edi, %eax
             retq

     example::main:
             subq    $72, %rsp
             movl    $4, 4(%rsp)
             leaq    4(%rsp), %rax
             movq    %rax, 8(%rsp)
             movq    core::fmt::num::imp::<impl core::fmt::Display for i32>::fmt@GOTPCREL(%rip), %rax
             movq    %rax, 16(%rsp)
             leaq    .L__unnamed_1(%rip), %rax
             movq    %rax, 24(%rsp)
             movq    $1, 32(%rsp)
             movq    $0, 40(%rsp)
             leaq    8(%rsp), %rax
             movq    %rax, 56(%rsp)
             movq    $1, 64(%rsp)
             leaq    24(%rsp), %rdi
             callq   *std::io::stdio::_print@GOTPCREL(%rip)
             addq    $72, %rsp
             retq

     .L__unnamed_2:

     .L__unnamed_1:
             .quad   .L__unnamed_2
             .zero   8
   #+END_SRC
   It does have the somewhat difficult to interpret elements in the
   main function, but the square function itself is remarkably
   simple. Moreover, it is presented as a separate entity in the
   assembler, so if you wanted to, you could in principle make a
   correlation between what was written in the source code and the
   assembler output. The Haskell output, on the other hand, is very
   hard to correlate to the assembly, and hence difficult to reason
   about. Moreover, here we can clearly see, that the square function
   is not being used at all to produce the output, but that the
   program is clever enough to inline the usage.

   With C, which was originally designed as portable assembler, we
   have an even neater story:

   #+BEGIN_SRC C
     #include <stdio.h>

     int square(int num) {
         return num * num;
     }

     int main()
     {
         printf("%d", square(2));
         return 0;
     }
   #+END_SRC
   which compiles down to an even smaller assembly:
   #+BEGIN_SRC asm

     square:
             movl    %edi, %eax
             imull   %edi, %eax
             ret
     .LC0:
             .string "%d"
     main:
             subq    $8, %rsp
             movl    $4, %esi
             movl    $.LC0, %edi
             xorl    %eax, %eax
             call    printf
             xorl    %eax, %eax
             addq    $8, %rsp
             ret
   #+END_SRC
   Here, not only can we tell that the program is very optimal, but we
   can see that the value of square 2 had been in-lined, and
   printed. If C had a facility to produce the optimisations
   efficiently at the source code level, the actual program to which
   this corresponds to would look like the following:

   #+BEGIN_SRC C

     int square (int x){
       return x*x;
     }

     int main(){
       printf("%d", 4);				/* Opt. square(2) */
       return 0;
     }
   #+END_SRC

   Which is slightly easier to reason about. At the very least, we
   know that the function square is not used directly, but rather that
   the optimiser is made aware of what square(2) entails and figured
   that it's a compile-time known value. Because the overhead to
   calling a function is large for this particular platform: x86_64,
   it was a good idea to inline. There might be a platform in the
   future, where calling functions can be done faster: whether because
   there will be massive parallel compute capabilities, because the
   functional revolution will take place, or because of sheer luck. It
   does not matter, what does, is that the prudence of the decision to
   inline a function is platform-dependent. What is even more
   interesting is that the format string, despite being equivalent to
   calling a function overload to printing an integer, is instantiated
   in full, and so we can expect that the particular program can be
   made even more efficient. Given a slightly different standard
   library of functions, there might exist a version of =puts=, which
   is much faster than =printf=, in that it is non-variadic. An even
   better architecture in my opinion would have been preparing the
   string in the program and requesting the OS and the standard
   library just do the printing. In fact, this is what the Rust
   version is kind-of doing, it's just that the calling conventions
   are very different, and pad out the rust version.

   So can we have robust high level optimisation that is transparent
   to the user, as well as low level assembly optimisations? I firmly
   believe that this is at least worthwhile to explore, if it doesn't
   work, the high-level language can be isolated from the low-level
   primitives, much the same way as C does it. It is, however, prudent
   to attempt.

   We shall start by noting that there is a third kind of thing that
   is normally referred to as a function: in addition to a procedure,
   we have subroutine.

   #+begin_example
     \square :: .subroutine
     \square =
       .asm x86(
         movl %edi, %eax
         imull %edi, %eax
         ret
       )

   #+end_example
   Notice that unlike most languages we do not even pretend for a
   second that the subroutine has a designated input or a designated
   output. They do not, on the hardware level really have any kind of
   distinction between registers. The only languages that are honest
   about this ground truth are the ones where the distinction between
   ins and outs, e.g. FORTRAN:
   #+BEGIN_SRC fortran
     function func(i) result(j)
       integer, intent(in) :: i ! input
       integer             :: j ! output
       j = i**2
     end function
   #+END_SRC
   note the pesky intent(in). As opposed to a possible intent(out).

   Also, there can be no name collisions, because subroutines' names
   are prefixed with a backslash, and only subroutines can have names
   prefixed with a backslash.

   One may notice that Haskell isn't the sole language where the
   double colon is used to designate types. Indeed, we shall be
   borrowing heavily from FORTRAN, as it is in many ways a much more
   suitable baseline of comparison for highly perform-ant code. It
   definitely lacks in some facilities needed for systems programming,
   but those are things that one can go around using other features of
   our language.

   With all of that in mind, we have to note that the usage of the
   subroutines is also borrowed from FORTRAN: we do not apply
   arguments to them, but rather we =CALL= them.

   #+begin_example
   square :: Int -> Int
   square a = .asm x86 CALL(
     %edi= a;
     \square\;
     %eax
   )
   #+end_example
   Let's go over this line by line. First of all, the =.asm= is a
   construct. There are primitives that one can use to build up the
   construct, and have the level of sophistication that this one
   has. Suffice it to say, the parsing requires a few tricks that we
   haven't covered yet. Next, the definition is given an integer =a=,
   and the value of the register is bound to =a=. What happens in each
   particular case, depends on whether or not a, has the constraint
   =<BindableToRegister(32)>, which =Int= has, by virtue of being a
   =32= bit primitive. The subroutine is called via a terminating
   backslash. The output is bound to the only returned object, the
   EAX, that is being substituted for the correct representation of
   the contents of the register. We cannot even in principle infer the
   type of the register in general, so the type annotation of the
   function cannot be replaced with =.infer= or omitted, unless an
   explicit annotation to =%eax= is given.

   Why use a construct? You want to have a place to write pure inline
   assembly. That's the =.asm= language intrinsic. We want a place
   where we could apply some of our language sensibilities, but also
   interface with assembly, and we don't want the assembly conventions
   to seep out into the code-base. Thus, the best solution ends up
   being a custom construct, for each supported type of assembly. It
   is straightforward to write the =CALL= construct, assuming a much
   uglier version of it in the form of a procedure, also exists, but
   does not require multiple lambdas be provided, and can rather work
   with just the bodies. That general function is the one that will
   handle all kinds of supported assembly translation, not just
   x86. Because some registers may not exist for some types of
   assembly, packaging all of that information into a construct seems
   neater.

   A fair and accurate question can be raised, is =square= necessarily
   a function? We've worked hard to preserve the distinction
   notation-ally in what the procedure can alter and what is to remain
   untouched, and any odd assembly chunk might have leaky state, and
   therefore square might not be. Here the best we can really do is to
   say that the system will have to do a bit of checking of the
   assembly on our behalf to make sure that the function really is a
   function. There are ways of doing that efficiently: the subroutine
   is free to modify the registers, as they are transient anyway; load
   and store information into memory, as long as segmentation is
   respected. It might be a good idea to not let subroutines access
   any particular memory location, but rather to require that all
   locations that are to be accessed, first be bound to local scope
   symbols. In other words, if the programmer is writing to a location
   it stands to reason that that location is going to be read, and
   even if temporary, binding it to a variable name is not too much to
   ask.

** =malloc= and =free=

   So while up until now, most of our information was stack-based, we
   can now allocate on the heap. We can now write the assembly
   equivalent of =malloc= directly as a function name =malloc=, that
   can even have some semantic meaning.

   =malloc=, being passed a symbol, simply binds that symbol to the
   return location of the C =malloc=. Of course the type has to be a
   different kind, we could adopt a special syntax for it, or have the
   user create a construct, but instead, we want to have heap
   allocation as something that every programmer can patch in. The
   type of the return of =malloc= in C, is always a pointer, even if
   allocation failed. Suppose we do want to have the integrated check
   and panic if the allocation failed; which can be formulated as a
   constraint on the pointer: left as an exercise.

   We can thus have a subroutine that allocates =Int= s, =Double= s,
   and even non-primitive objects, such as C =structs=. We already have
   a mechanism for allocating algebraic data-types, right? Why would we
   want this?

   Suppose that you had an API, that provided semantically the same
   thing as one of our objects of algebraic record type, but in the
   form of a C =struct=. You can now write a binding module that allowed
   our language sensibilities to apply to that =struct=, and even
   determine the name of the corresponding type, according to some
   rules. But there's another, deeper need for this. While we have
   talked about how our structures must behave, it was assumed that
   the exact type of back-end data-structure is magic. The high level
   optimiser can look at our usage of a data-structure, and say that it
   doesn't need to exist in full, or that it doesn't have to be one
   data-structure, but several arrays with a common size and
   index. However, at some point we will need to instantiate a version
   of a complex structure, and we don't want for that to be opaque,
   thus we need to provide a language-intrinsic low-level version of
   =struct= allocation. We can do that on the stack, simply by
   pretending that the several adjacent memory locations are one
   object, and by reading from or writing to them. But even then,
   eventually we will run out of stack space, and will be forced to
   allocate things on the heap. When we will need to do that, the
   mechanism is there and it's the ability to call subroutines, which
   all talk assembly.

   Of course, introducing objects with non-trivial lifetimes is
   tricky. While we can assume that the stack allocated list of three
   numbers will be freed as soon as we exit the scope of its creation,
   as many know from years of C programming and memory leaks, that is
   not the case with C.

   Because we talk assembly, our language has the unfortunate
   consequence of being manually memory managed! Did we just create
   FORTRAN but with extra steps, and mildly less annoying syntax?
   Moreover, how can we claim that the compiler knows best what type
   of data-structure to use, when it can't even manage them without our
   help. OK, fine, it can be trusted to call =malloc= upon binding, so
   we can at least avoid null dereferencing.

   But how can we free memory? Will the compiler ever be able to do
   that? Will we be able to do such acrobatic miracles as we can in
   Haskell, where we can build an infinite Fibonacci sequence, and
   find its 21st element by simple subscripting?

** Garbage collection

   "I don't mean that the enterprise should be hauling garbage, I mean
   that it is to be hauled as garbage. " -- ST:TOS Klingon.

   "I don't mean that Garbage collection is the act of collecting the
   garbage in memory. I mean that a system of garbage collection is a
   collection of garbage ideas. " -- yours truly.

   Such a harsh critique of garbage collection is in many ways both
   unwarranted and an understatement. It is relatively common in
   highly expressive languages, but mostly because the appeal of
   lifting the burden of memory management off of the shoulders of the
   programmers is a selling point for many languages. Let us see why
   Haskell needs a garbage collector.

   In functional programming there is no concept of the Von Neumann
   box, there are no memory locations, just values. However, because
   of that, to replicate the behaviours of highly complex systems, one
   has to come up with complex and potentially infinite
   abstractions. A list in Haskell can contain an infinite amount of
   elements and in one fell swoop one can define Fibonacci
   sequence. For that kind of expressive power to work, the programmer
   must relinquish all semblance of control over where the value is
   being held. One cannot have heap allocated or stack allocated
   values, because the compiler is to intelligently decide if the
   value is to be used or allocated at all. Because one has to deal
   with far more complex objects, the garbage collector must do
   considerably more work to maintain feature parity with languages
   such as C or C++, which permit mutation by default and allow the
   programmer to heap allocate things that cannot possibly exist on
   the stack.

   Garbage collection is often a necessary evil. Some languages like
   =nim= and =rust=, have eschewed many of the challenges of automatic
   memory management by clever design. In the first case, we have an
   efficient form of reference counting that uses some of the language
   features to provide tight constraints and remove heap-allocated
   objects at a more optimal time. =Rust= in particular, has found a
   way to track the ownership of complicated objects and use that to
   free memory as soon as one can release it. The trade-off of such a
   system is that it requires the programmer to use objects with
   finite lifetime and track in the change of ownership explicitly.

   However, the reason why this works in Rust, is because in addition
   to tracking ownership of an object, it also permits the mutation of
   an object, and allows borrowing objects in ways that cannot cause a
   race condition.

   Both approaches are valid, and provided one has access to the
   language, can build data-structures that support this to a small
   extent. We can have reference counted pointers in C++, which will
   be garbage collected. We do provide a mechanism to do that as
   intelligently as we do in Rust, except while in C++ it is possible
   to share a unique pointer, and that not causing a compilation
   error, rust's borrow checker will give you a massive headache if
   you try.

   So, to an extent, the constraint system allows one to apply these
   principles, but in a much more general fashion. But, in the spirit
   of mathematics, we need to sharply define the rules that govern the
   interactions. We have a language built around constraints, we built
   it precisely to be able to articulate these ideas in code, rather
   than in the compiler.

*** Bindings.

    Let's start with the basics of object lifetime. First, we have
    symbols and values bound to them. We shall adopt a convention
    common in Lisps, where ='symbol= means the symbol as a value,
    whereas the expression =symbol= refers to the value bound to the
    symbol. So, the first and most important example of objects with
    finite lifetime, are scoped values. Whenever a function is defined,
    a new scope appears, and it is by default, populated with bound
    symbols of the enclosing scope.

    This gives rise to a few well-known and debatably useful feature
    of modern programming languages: shadowing, and an unarguably
    useful feature too: scoping.

    #+BEGIN_SRC python
      def fun1(arg1):
         symb1 = 'val1'
         symb2 = 'val2'

         def fun2 (arg2):
            symb1 = 'val1 primed'
            print(symb1)         # val1 primed
            print(symb2)         # val2

         print(symb1)                 # val1

    #+END_SRC
    In this example we see both. The lifetime of the =symb1= to =val1=
    binding is the duration of execution of the =fun1=, a parallel
    binding exists within the body of the enclosed function, and when
    one requires to find the value bound to the =symb1= symbol, we
    recover the one bound in the narrowest enclosing scope.

    So the object with lifetime in this instance is not the symbol, nor
    is it the value, but the binding. But how do we even refer to the
    binding in our language?

    #+begin_example
    symbol = 3
    'symbol.bindings_self_0.val == 3
    #+end_example

    So each symbol, has a collection of bindings, that can be
    navigated. But why =self_0= and not *just* a numerical index? Well,
    if you have multiple closures, then you may have the same symbol
    bound to multiple values. This can take place both syntactically
    and physically. The syntactic case is obvious, multiple enclosed
    functions give you a tree, where the number of branches is not
    necessarily bounded, but practically no more than
    =O(n_\text{functions})=. However, if you have a recursive call that
    has the same symbol bound over and over, the tree gives way to as
    many bindings within the same apparent syntactic scope as there
    were iterations. The zeroth binding is guaranteed to exist if a
    value is bound within the current scope. However, if the current
    value was bound outside, the correct subscript is =p_0=. Higher
    numerical subscripts are not at all guaranteed to
    exist. Syntactically, in a linear program, =0= is guaranteed to
    exist, and no other can. Recursion populates higher indices, and
    only such a process can. The assignment that we hacked into our
    language a while ago, can increment the counter of bindings, but in
    the interest of preserving memory, it is advised to keep the
    binding counter to the number of times the binding expression is
    evaluated. Incrementing the counter may be useful for debugging,
    but for stack variables is tantamount to creating copies and not
    destroying them.

    But do not be alarmed, the entire binding hierarchy is not kept at
    runtime, and is rarely evaluated at compile time. It is simply a
    convenient interface to reference the bindings for constraint code
    that will use the concepts of bindings, which we shall address
    later.

    For now, we must note that the bindings can have other computable
    properties. For example, one can interrogate if a binding is
    actual: i.e. if the binding statement relative to the point where
    the query was placed, had already been evaluated. Consider:

    #+begin_example
    parent :: Int -> Int
    parent a =
       b = a;

       childOne :: Int -> Int :: a =
         b = a;
         'b.bindings_p_0.val == b;
         a.

       childTwo a;
       'b.bindings_c.count == 2;
       'b.bindings_c[:<actual>].count == 1;
       'b.bindings_c[:<actual>]_0.val == a*a;
       'b.bindings_c_1.val == a*a;

       childTwo :: Int -> Int :: a = (
         b = b*b;
         b
       );

       childOne * childTwo
    #+end_example

    The parent function has a single direct binding, so the parent of
    the child's binding tree has one element. The children of the
    =parent= function have a binding each, so the count of leaves on
    the binding tree is two. If however, we filter among those, the
    bindings that are actual at the time of invocation, we shall see,
    that one binding was not yet invoked, and hence only one is
    counted. However, the actuality is determined by control flow, not
    definition order, so the only actual binding is the one in
    =childTwo= and not =childOne=. However, the bindings in the tree
    are indexed in definition order, and not execution order, so the
    childTwo corresponds to the second child in the binding tree.

    The only other thing we haven't covered yet, is that within
    multi-line definitions, it is prudent to delineate the end of a
    function, with a full stop, or to parenthesise the entire enclosed
    function. Similar to python, we do allow enclosed definitions of
    objects including functions. Unlike Python, if we do that, we do
    not rely solely on the indentation to communicate where one
    function ends and another begins. Moreover, we want to remain
    consistent, so every value that is terminated by a semicolon is
    excluded from the output, every value terminated by the comma is
    included. If an enclosed function ends with a full stop, it's
    equivalent to implicit parentheses, and a semicolon at the end of
    the group. If it does not include a full stop, then it's a
    syntactic error.

    We're making progress, but not in the right direction. The actual
    bindings remain actual after the final usage of the value. What we
    want is a constraint that applies =free= after a =malloc='d
    object's pointer is no longer used in any scope. This is an
    oversimplification, as it wouldn't work if say the objects
    referenced each other, and would free things that could still be
    in use, before their time expires. But for e.g. unique pointers,
    that sort of primitive approach would work. But we do need a
    built-in form of usage tracking that has many of the same
    components of the binding tracking.

    Let's look back at the two-child function example. Suppose that it
    were a heap-allocated memory location that does not get cleared
    with the stack frame. Suppose that instead we have this scenario.

    #+begin_example
    parent :: Ptr<Int> -> Ptr<Int>
    parent a =
       b = a;
       b
    #+end_example

    Here, we have only *one* memory location to track, but two
    pointers, and crucially, the pointer which is returned from the
    function is not bound to the same name as the input. Here we make
    use of another feature: bindings between pure symbolic values are
    treated specially, as aliases, rather than ordinary bindings. So
    effectively you are indeed tracking the same value, but under
    different names. We can break this equivalence, because the
    =.relax= and =.constrain= operations apply only to the symbol, and
    only after the statement. But until then, the aliases are
    guaranteed to have the same constraints, and the same value (which
    follows from the constraints, really). The consequence of this, is
    that one can test a single value once, and the constraints can
    magically apply to the alias half a code-base away. This sounds
    like exactly the kind of thing that functional programming would
    want to avoid, however, I trust that most programmers can wrap
    their head around the fact that if you call the same person two
    different names: "Larry" and "Lawrence", that if "Larry" is
    married, so is "Lawrence". However, you can have the same
    hypothetical Larry undergo a divorce in one hypothetical
    (i.e. =.relax larry <Married>=), that might not necessarily apply
    to all other versions of him, in all possible hypothetical
    situations. Hence why if you want to maintain the status of alias
    between =Larry= and =Lawrence=, you must explicitly bind them
    together again.

    So for each binding there can be an alias. This makes our life
    both easier and harder, because on the one hand we *can* track the
    value and not just the symbols, but on the other, the aliases and
    bindings add a lot of noise. For this specific reason, the
    standard library shall contain a function that does that for you.

    #+begin_example
    UNIQUE VALUES (parent)
    #+end_example
    which returns a dictionary of answers:
    #+begin_example
    (
      in: (count: 1, symbols: (a), ...),
      out: (count: 1, symbols: (a,b), ...)
    )
    #+end_example
    This is still a bit clunky, but recall that constructs are a
    thing, and that their syntax deliberately allows for expressions
    like the ones in query languages, so it gives a good starting
    point, to the effect of making progress.

*** Usage

    It stands to reason, that we can, using the built ins, and a bit
    of trickery that can count the number of uses of a binding within
    each scope, provided as =USES OF ('symbol)=, that we can tell if a
    value has any presence outside a particular scope. So we can
    attach a constraint that tells us if a value is within
    scope.

    EXERCISE: write a constraint that is applied to values that are
    bound, within scope and is no longer satisfied when the value is
    outside the scope.

    EXERCISE: Write a constraint that acts as before, but also allows
    for manual freeing of memory, and if the memory is freed, every
    attempt to use the freed memory in a context other than
    re-allocation will cause a constraint resolution failure.

    # TODO solve that exercise myself.

    But knowing that we have a constraint, that tells us if something
    is after =malloc= but before free, how do we use that to make the
    thing free automatically upon exiting scope. Let's consider a
    concrete example. Suppose that we have the function that
    dereferences an integer pointer.

    We want for the constraint system to not work with freed memory,
    not to cause segmentation faults. We also want the pointer
    dereference to interpret the information according to the type
    attached to the pointer, and bring it into the world of value
    oriented functional programming, bound to a single value, with
    singular constraints. The greatest challenge, and one of the most
    useful sort, is that if the pointer is not used at any point
    afterwards, we want the memory to be freed.

    So, let's go over this. The first thing is easy to do: just have
    the input explicitly require that the pointer is within scope and
    not freed. If you did that using the first exercise, the function
    call will always resolve, because the use in the function is a
    use, and the thing can only be freed after its final use. [fn:: If
    you did that through the second exercise, the constraint
    resolution will fail, if the memory was freed, and will not fail
    if the memory is still actual. ]

    But what do we do with the memory location. The functional purist
    (after profuse nausea at the thought), will approach the problem
    as: if the function can modify the pointer, then this modification
    should be reflected in the output. So we shall have two procedures
    that have the following signature.

    #+begin_example
    deref :: Pointer<OfType(a*)><Scoped> >>  (a, Pointer<OfType(a)>)
    #+end_example
    We shall need to specialise this signature to a few cases. If the
    output has to be constrained to a scoped pointer, then what we
    need to do, is just to interpret the information at the memory
    location and do nothing else. That is:
    #+begin_example
      deref :: Pointer<OfType(a*)><Scoped> -> (a, Pointer<OfType(a)><Scoped>)
      deref ptr =
      (
        \interpSubrountine = a.\interpretFromPointer;

        .asm x86 CALL(%edi=ptr;
                      \interpSubrountine\;
                      %eax),
        ptr
      )

    #+end_example
    Here the asterisk signifies that the parameter =a= is used as a
    generic argument or a template argument in =C++=. Also notice that
    this style of dereferencing guarantees that the resulting global
    state is unchanged. In other words, deref really is a function.

    It is not the desired behaviour when there are no subsequent
    uses. So what we do then, is we call the same function, but
    instead, from a procedure.
    #+begin_example
      deref :: Pointer<OfType(a*)><Scoped> >> a
      deref ptr =
        a, dptr = deref ptr;
        .asm x86 CALL(%edi=dptr;
                      \free\);
        a.
    #+end_example
    In this case, we make use of the function that we have already
    defined, in this particular instance, the constraint resolution
    succeeds, as =dptr= is used within a subroutine. Afterwards, if
    there is a use, that constraint resolution will fail, because the
    value is no longer scoped. Moreover, we have the behaviour we
    want.

    Great! We can do better. If the pointer is invalid, we don't
    return it, because it's invalid. If the pointer, is however, valid
    after dereferencing, it is the same pointer. So we can simplify
    both functions, to remove mentions of the pointer.

    The issue now is, that we can have a mixture of manual and
    automatic memory management. As a result if the pointer is used
    only once in a free clause, it is still considered scoped, but is
    not used any more. So we can modify the constraint to exclude
    that. Except in this instance, the architecture we have cannot be
    re-used. We would, thus need to do some work and repeat ourselves
    a little, to make sure that the semantics are preserved.

    #+begin_example
      deref :: Pointer<OfType(a*)><Scoped> -> a
      deref ptr =
      (
        \interpSubrountine = a.\interpretFromPointer;

        .asm x86 CALL(%edi=ptr;
                      \interpSubrountine\;
                      %eax)
      )

    #+end_example

    #+begin_example
      deref :: Pointer<OfType(a*)><Scoped> >> a
      deref ptr =
          \interpSubrountine = a.\interpretFromPointer;
          .asm x86 CALL(%edi=ptr;
                        \interpSubrountine\;
                        %eax)
          .asm x86 CALL(%edi=dptr;
                            \free\ );
          a.
    #+end_example
    Now, however, whenever we use the deref, procedure, the subsequent
    free become a double free, but if we were careful, we wrote the
    free procedure in such a way, that it doesn't compile if the
    pointer is out of scope, i.e. we get a double free. So if we
    wanted to migrate to scoped pointers, we can do it this way, and
    it would not interfere with other cases of manual memory
    management.

*** Migration and motivation

    So what we have just done, is effectively reinvent the scoped
    pointer that people are already using in C++. Why would we do
    that?

    Imagine that we were porting Doom from C to C++. The first step is
    very easy to do, just rename every header, and every file to the
    proper extension and maybe change =<stdio.h>= to =<cstdio>=. Job
    done? In a way, you have the C code using the C++ compiler, but
    you are using exactly none of the C++ features, there are no
    objects, just structures that are manually allocated and
    de-allocated. Your functions accept pointers, not references,
    speaking of which, your code-base is riddled with raw pointers,
    and casting is done in the worst possible way for C++.

    IF you tried to do the same in Haskell, you'd have to work around
    the fact that not all features of C have a strict equivalent in
    Haskell. You'd have to worry about the fact that Lists are not
    random access, and most of your mutable code, while remaining
    within Monadic bubbles, would still be imperative code with
    mutable state. The amount of work needed to port the code-base of
    Doom to idiomatic Haskell would be highly non-trivial.

    So, what the constraint system allows us to do, is to shift the
    paradigms of our language gradually. There is always the risk of
    moving too fast, or too slow, where you either end up with too
    much modifications to the code-base, or a language that has some of
    the features that only work thanks to some constraints that
    haven't been set yet.

    What we can do in our language is the following. Step 1: parse the
    C code and turn it into function/procedure/subroutine equivalents
    in the target language. Verify if it works. Then you write a
    constraint: all variables that are allocated within one scoped,
    and never referenced outside that scope are scoped pointers. This
    constraint can on its own highlight the pointers that can easily
    be turned to scoped ones. So you can query the code-base, much like
    with grep, but based on the fact that some pointers cannot be made
    scoped. The manual intervention is needed, if you want to move
    to automatic memory management.

    The crux of the improvement over e.g. C and C++, is that the
    process can largely be automated. You have a library of
    constraints that one needs to apply iterative-ly to a code-base to
    move from manual to automatic smart-pointer oriented memory
    management. The move from smart pointers to an even more
    hands-free experience can also be made.

    One principle for the functional part of our language must be made
    clear, if it's not evaluated, it might not be allocated. It could,
    but probably isn't. When we talk about optimisers we shall talk
    about this at length. It is not a black box feature of the
    compiler, it is something that we shall build into the language
    ourselves, just like we built smart pointers. We can in principle,
    re-create the borrow checker, based on its outward behaviour.

    Last, but not least, if you build the constraints of the =nim=
    values into your code-base, you can verbatim copy the algorithm of
    the =nim='s excellent garbage collector. In cases where you would
    have to re-architect the code-base, the language itself will tell
    you what's wrong, and hint at how you need to re-structure.

** TODO On mutability by other means.

   We must first note, that our language is not functional. Because we
   can have the =.constrain= and =.relax= keywords, the type of a
   thing can change by entering a function. This is not necessarily a
   side-effect, but it can lead to one, if say the constraint that is
   being added changes the value interpretation of the data. However,
   =.constrain= is only meant as a means to speed up the compilation,
   not the run-time, and hence such keywords are to be omitted from
   production software. All other constraint semantics are effectively
   value-dependent type-classes, and even share a similarity with C++'s
   equivalent of type-classes: templates.

   So based on the similarities to the Haskell family of languages, we
   can indeed reach the conclusion that the Monadic encapsulation of
   side-effects is possible. However, we would inherit all of the
   drawbacks of such a system. What we want to do instead, is to have
   a language-level and singular interface to non-Monadic
   encapsulation. In other words, we want to make it impractical for
   people to hand-roll an IO monad, and instead use the language's
   built-in features.

   What we have to have though is a clear distinction between
   functions and procedures and subroutines.

   A function is a mathematical function. A procedure is a sequence of
   operations that may have side-effects. A subroutine is a procedure
   that has no return value.

   But first we must talk about how we're going to represent common
   side effects. As you might have guessed, the constraint system is
   going to feature prominently, so if you're a functional geek and
   assumed that all preceding chapters were retreading an ML-family
   functional language, then I'd suggest you give them a fresh read.

   The first thing that we must note is that immutability is an
   implicit constraint on any value that is not being mutated within
   the program. Of course there can be kinds of mutation, that must
   track the constraints present on the value. So if we had an
   =assign= infix function, then the constraints on the expression on
   the left must all be relaxed, while the constraints of the
   right-hand expression applied to the name afterwards.

   So if one wants to have mutability, the presence of mechanisms to
   =.relax= and =.constrain= must be present. As soon as functions
   containing these keywords enter the code-base, all functions that
   depend on them are no longer functions in the ordinary sense, they
   are procedures. =assign= is a procedure, and the lack of
   function-ness is made explicit in the signature:

   #+begin_example
   assign:: Symbol -> (* ->> Symbol)
   #+end_example

   Note that the signature is not simply that of the function, but has
   an idempotent side-effect arrow: =->>=. This is a very very subtle
   point: in the eyes of a functional programmer, every function that
   has a side effect is impure. A Monadic functional programmer would
   say that this a particular side-effect that can be wrapped in a
   Monadic type, but anyone else will realise that:
   - the effect of assign is dependent only the inputs.
   - The only external global state is that of the AST, hence the
     re-use of the symbol is a side-effect,
   - The external state is modified idem-potently, i.e. no matter how
     many times in a row we apply the assign procedure, the global
     state will be left in the same state if the inputs were the same.

   So if one sees many idempotent assignments, one can replace all of
   them with just one, provided the input parameters are exactly the
   same. However, it is a much trickier question if the same can be
   said if there is other potentially side-effect prone code in
   between assignments. These cannot be optimised with referential
   transparency, at least not in the general case. For a truly
   functional language that would be a serious issue, but not in our
   language, we never pretended to be purely functional.

   But suppose that we only had one assignment. The Monadic approach
   to functional programming tells us that one does not have to give
   up functionally pure behaviour of the entire program, just because
   we have *one* assignment procedure. Instead, every function that
   depends on such a value ceases to be referential-ly transparent, and
   only if used in a very specific context. One can unpack the value
   from the monad, and perform the operations as usual, preserving the
   functional purity everywhere except in the act of unpacking.

   While we can simply use Monads in our language directly, (as we
   have algebraic data-types, it is a matter of patching the support as
   an optimisation), we can also replicate the behaviour in the
   constraint system. After the assignment, the symbol no longer has
   the <value> constraint, and most functions will no longer accept
   it, while some procedures will (depending on where the procedure
   arrow lies, it might part of a curried pure function and thus
   unacceptable as input). However, if one wants to use a mutable
   variable in a function =func=, one can do the following:

   #+begin_example
   func:: InputType -> OutputType
   func = ...

   func:: InputeType<!value> >> OutputType
   func in =
     tempVal = in;
     func tempVal
   #+end_example

   Here it is apparent that the binding equals sign specifically is
   semantically a copy assignment, that can act, under certain
   constraints as a move assignment. Of course these constraints must
   be tracked and valid.

   Note, though that we no longer have an idempotent procedure. What
   this means is that the same symbol being passed in is not a
   guarantee of the same global state, unlike the case of assignment,
   where no matter how many times the symbol and value are passed in,
   the result is guaranteed to be the same side effect, here it might
   not be. While the symbol is the same, the value bound to it, may be
   different, and hence the result is not guaranteed to be the same
   for passing the same symbol as input. Note also that we have not
   taken the symbol as a parameter, but an actual =InputType=
   instance, which is not a pure value. This is an important
   distinction.

* The optimiser exposed
** Version control

   Like we said previously, we want for the optimisations to appear
   within the code-base verbatim. So in the case of
   #+begin_example
   square :: Int -> Int
   square a =
     a * a

   main :: Arguments -> Int
   main argv =
     printf("%d", square 2)
   #+end_example
   we would like to be able to see what goes on. So we have a couple
   of tools available. During the compilation of this session, we can
   command the compiler to optimise. Let's do that.
   #+begin_example
   :opt step
   #+end_example
   What this means is that the compiler is to do one compilation step
   and return the results as a text buffer. What we then get, is the
   following:

   #+begin_example
   optimisation 1: dispose of unused names.
   #+end_example

   so the file now looks like
   #+begin_example
     square :: Int -> Int
     square a =
       a * a

     main :: () -> Int
     main  =
       printf("%d", square 2)
   #+end_example
   Now if we called compile before the optimisation step, the produced
   executable would be much different, in that it would allocate the
   memory to the arguments, pass them to the registers in the correct
   way, and do everything up until the point of parsing. Similarly, if
   we evaluated and thrown away a result, unless the source code
   reflects that change, the evaluation would take place and thrown
   away. So if you needed to debug the naïve approach, you could rest
   assured that nothing that you aren't aware of is optimised away.

   Another step and you get
   #+begin_example
   optimisation 2: inline result of compile-time computation
   #+end_example
   and the value =4= replaces =square 2=.

   At some point we will realise that we are usually doing
   optimisations automatically, so let's do that.
   #+begin_example
   :opt auto
   #+end_example

   We get as a result, a block of optimisation steps. We don't see it
   by default, we just know that there were e.g. 20 of them. The last one being
   #+begin_example
   optimisation 22: Re-arrange for optimal layout.
   #+end_example

   As a result, the entire program compiles down to
   #+BEGIN_SRC asm
     .LC0:
        .string "4"
main:
        subq    $8, %rsp
        movl    $.LC0, %edi
        call    puts

        xorl    %eax, %eax
        addq    $8, %rsp
        ret
   #+END_SRC
   Which is much smaller than what we expect even C to produce. There
   is no reference to the function square, because we never needed to
   instantiate that template. Moreover, we are making use of the fact
   that =printf()= is general-purpose, output function, with lots of
   complicated machinery, on account of C's sub-par typing
   system. However, we know what we need to produce, a decimal
   representation of a compile-time known value 4. What about the
   source code? It looks like
   #+begin_example
   main :: () -> Int
   main =
     puts("4")
   #+end_example
   which is exactly in line with what the simplest program
   corresponding to the assembly would look like. But that is not the
   specification of what the program is. Sure all of its outward
   effects are that it prints 4, and exits. We want the optimiser to
   squeeze as much complexity out of this, but we don't necessarily
   want for the programmers to see the de-compiled source, but rather
   the original specification of the problem and the optimisation.

   So in this case, we want to apply the optimisation that gets rid of
   the arguments in main, but keeps =square 4=. We could do that
   manually, e.g.
   #+begin_example
   :opt manual 'main
   #+end_example
   or, we could just say
   #+begin_example
   :opt commit auto:1
   #+end_example
   What this means, is that we want to make the first optimisation
   part of the official source code. So the programmer, if they wanted
   to view the entire code-base (of two functions), could do
   #+begin_example
   :view spec
   #+end_example

   This shall produce the same view of the source code as after the
   first optimisation. But if they optimised, the first optimisation
   step shall be step 2, not 1. They can thus do:
   #+begin_example
   :opt step:-1
   #+end_example
   which will bring them back. And they can fast forward to the output 4 and exit source code, by going
   #+begin_example
   :opt auto
   #+end_example
   If you now tried to do something else in
   #+begin_example
   :opt manual 'main
   #+end_example
   you would get a step 3.1. And if you did that again starting from
   3, you would get 3.1.1, and so on. All the branches of all
   optimisations are preserved in a persistent data-structure. All of
   your optimisations can lead to the same AST state, eventually. The
   compiler is kind enough to find those for you, and then you would
   get optimisation 3.22/22. Fun! You could refer to that state of the AST as either
   #+begin_example
   :view optimisation 3.22
   #+end_example
   or
   #+begin_example
   :view optimisation 22
   #+end_example
   Which in both cases may be "print 4 and exit".

   At some point, however, the amount of intermediate branches may
   become untenable. Perhaps you do reach a leaf node, or perhaps you
   get so many branches for something very simple, that you run out of
   memory. The most crude approach is
   #+begin_example
   :trim * except spec
   #+end_example
   which means that everything except for the specification branch is
   removed from the AST. You can no longer roll back to the state
   where you had =Arguments= in =main=. The names for symbols that you
   might have used also get purged, as well as any unused constructs.

   That information is not lost, just separated from the code-base, and
   archived. We can do more fine grained trimming:
   #+begin_example
   :trim opt after 2
   #+end_example
   #+begin_example
   :trim changes except spec
   #+end_example
   #+begin_example
   :trim changes except spec and opt after 3
   #+end_example
   Finally, the two can be combined, so that there is a single archive
   corresponding to the trim operation.

   In each case, we get rid of some of the branches. In the first: all
   optimisations after 2. parallel branches are untouched if they
   diverged from 1, so 1.22 would still be in the tree, but not 5.22.
   The second example is what you would do, when you were iterating
   over multiple architectures, and finally decided on one, and wanted
   to keep the history of that optimisation, but not the other
   architectures.

** Optimisation process in detail.

   We have a rough idea of how the high-level optimiser works: it
   iterates on the entire code-base, and does obvious things. But what
   can be done depends on the context. For example, lets look at
   #+begin_example
   findAnswerToAll :: .infer
   findAnswerToAll world =
      doSomethingWithWorld world;
      world;
      42;
      42
   #+end_example
   Obviously, we're not told if this is a function or a procedure. If
   =doSomethingWithWorld= is a function, then we can be confident
   that no modifications to the =world= can be made. Moreover, if it
   is a procedure, only the things passed into it as parameters can
   be modified, if appropriately delineated. Either case, we can find
   out just by looking at the signature of =doSomethingWithWorld=,
   and some type inference. In the context of a function, the
   evaluation serves no purpose, and it is likely, that the
   function's result should have been kept. We can warn the user when
   doing the optimisation that this can either be a silly mistake
   along the lines of "I forgot to bind the value to something" or "I
   forgot that it's a function, and not a procedure". The other tow
   evaluations have no syntactic or otherwise effect, so one can
   safely throw them away.

   One could say, we can only do this in a function, and a procedure
   would be very different, but that is not exactly the case. The
   programmer never communicated that the world is a callable, and 42
   is an =Int= literal. The latter cannot possibly have side effects,
   and the former, if it could, is not requested by the
   programmer. So we can ignore the lack thereof. The only place
   where we /do/ have the potential for side-effects is within
   =doSomethingWithWorld=. But we can still do the same kinds of
   optimisations to every such procedure. And at some point we may
   realise that something *could* be a function, rather than a
   procedure. But in this case we do not suggest that to the user,
   unless they ask:
   #+begin_example
   :opt proc to function
   #+end_example
   The reason may be that the end user prefers performance with the
   negligible side effects being fairly manageable. But while our
   language is not built around the functional paradigm, as you have
   probably noticed it can support this style really well, so if the
   user wants to move away from the Von Neumann imperative, they can
   be pointed at what needs changing.

   But how does the compiler know to optimise the other way around?
   As a general rule of thumb, the function that is more specialised,
   is more suitable. Consider that when you're buying shampoo you
   first look for the purpose: then optimise for the constraints, and
   finally, if you found no "my special snowflake" shampoo, find the
   most generic one. So if there's a generic function written just
   for =Ints= and another one that specialises in =Int<Odd>=, you
   expect the latter version to be faster, or better in some
   way. That is not a guarantee, but to the extent of relying on the
   automatic resolution, you expect for that to be not a terrible
   guess. If there are two specialised versions of the same function,
   you either fail, specify the precedence of some constraints over
   others, or specify that one method is faster than the other.

   But there rarely is a situation where one function is in every
   conceivable way better than another. There may be one that gets
   you the part of the answer that you care about, but faster. One
   that isn't a function, but has manageable side-effects, and is
   much faster. It may also be the case, that you avoid the faster
   one because of memory constraints.

   Now suppose, that we're talking about sorting functions. We know
   that to sort a large collection in a stable fashion one can use
   merge-sort. That is after all close to the proper limit of how
   efficient an algorithm could be: \(O(n \lg n)\). This can be
   written easily as a function:

   #+begin_example
   sort :: Collection<Homogeneous><OfType(a*::<Comparable>)> -> Collection<Homogeneous><OfType(a::<Comparable>)><Sorted>
   sort collection =
       merge :: .infer
       merge (l, r) =
           if l_0 > r_0 then
               r_0 ++ merge (l, r_[1::len])
           else
               l_0 ++ merge (l_[1::len], r)

       half = (
         1: collection_[0:(len/2)],
         2: collection_[(len/2):len]
       )
       merge (sort half_1, sort half_2) .constrain <Sorted>

   sort :: a*<Comparable> -> a<Comparable>
   sort element = element
   #+End_example
   Here we haven't done much of anything fancy, because we want to
   illustrate a few points. Ordinarily, the function's return type
   would be guaranteed to be the same as the input type, but not in
   this case. The type of the answer will depend on the context, the
   only thing we know are the constraints. We shall discuss what
   happens to the types in the next section, for now, assume that the
   specification is correct.

   We know that merge-sort isn't the best sorting algorithm. It's
   fast, and better than bubble-sort, but has huge space requirements,
   that on some architectures, would lead to a large constant factor
   in the time complexity. Tony Hoare, whom I've had the honour of
   meeting, has come up with a remarkable solution to this problem.

   #+begin_example
   sort :: .infer
   sort collection =
       pivot = collection_0;
       sort collection_[:<LessThan(pivot)] +++ pivot +++ sort collection_[:<GreaterThan(pivot)] .constrain <Sorted>

   sort :: .infer
   sort () = ()
   #+end_example
   Here we have made use of a couple of tricky things, especially if
   you come from Haskell or Standard ML. In those languages, an empty
   list, a list with a single element and a collection with more than
   one element all have the same type. In our case they do not. A unit
   can represent an empty element in every collection that defines
   one. It's the leaf node in a tree, it's the =nil= in a list, and
   it's an empty array segment. If we wanted to, we could just as
   easily have made unit =False= for a boolean:
   #+begin_example
   .data Boolean ::= True | False == ()
   #+end_example
   though it doesn't take too much of a genius to figure out why that
   is a bad idea.

   The triple =+++= is a flattening append: the construction is needed, because
   #+begin_example
   1 ++ 2 ++ () ++ 3 ++ 4 ++ () ++ 5 ++ 6 ++ () ++ () == ((1,2), (3,4), (5,6))
   #+end_example
   while
   #+begin_example
   1 +++ 2 +++ () +++ 3 +++ 4 +++ () +++ 5 +++ 6 +++ () +++ () == (1,2,3,4,5,6)
   #+end_example

   In addition to being far more quick, (hence the namesake), this
   sorting algorithm is easier to write using the native features of
   slicing and appending. But we know that there are catastrophic
   failure cases, namely when quick sort of this kind is given a
   pre-sorted array/list it would effectively do \(O(n^2)\)
   operations.

   But we know that there are cases when this is definitely
   faster. Can't we check for the <Sorted>ness of the input? In
   general, such a check would be linear upon every iteration. But
   who is there to say, that we should only use the <Sorted> as a
   dummy constraint. We can infer quite a lot about what a sorted
   array is, in relation to its subsets.

   #+begin_example
   checksorted :: Collection<OfType(<Comparable>)>
   checksorted a =
       IF a.len <= 1 || a_0 < (checksorted a_[1:len])_0

   <Sorted> checksorted(a*)
   #+end_example
   This check is expensive, so we want a way to track the sorted-ness
   without checking for it explicitly every time.
   #+begin_example
   a::<Sorted> .implies a_[b*::<GreaterThan(0):c*<LessThan(len)>]::<Sorted>
   #+end_example
   Which is intuitive, so we can make a slice, and a sub-array of a
   sorted array is going to be flagged as sorted. The trouble is, for
   that to work, we need to establish that the bigger array was
   sorted. It is not a trivial task in general, but knowing that we
   constrain the outputs of sort functions, we can recover if some
   parts of the re-arranged sorted array are sorted. And we do know
   that merging two sorted arrays doesn't necessarily lead to a
   sorted array, but if two arrays are sorted, and the last element
   of the first is smaller than the first element of the second, then
   we know that the result is sorted. With this rule:
   #+begin_example
   checkMergeAbleSorted :: a*::Collection<OfType(<Comparable>) -> a
   checkMergeAbleSorted a b =
       IF a_[-1] < b_0

   <_> => checkMergeAbleSorted a b .implies (a +++ b)::<Sorted>
   #+end_example

   we no longer have to constrain the merge-Sort to produce sorted
   arrays. The result will automatically track this constraint. The
   added bonus is that if we merge the elements in the wrong order,
   the output constraint would be unsatisfied, and the incorrect
   sorting algorithm won't compile.

   Now, we have two functions that have the exact same
   signature. This is a collision, that whenever you use a function
   named sort would fail to compile. There is no way of
   disambiguation without specialising either one or the other. But,
   you see, here the choice depends on whether or not we're likely to
   get collections with large sorted chunks, or completely randomised
   data. But how do you convey probability in the case of
   uncertainty.

   Consider how you would make a rigorous decision in the comparable
   situation. Suppose that the merge-sort takes exactly \(T = A * n *
   \lg n\) where \(n\) is the length of the array. The =quickSort= is
   taking a different amount of time for each configuration, but for
   simplicity's sake, let's assume it takes \(T = B*n*\lg n\) for
   unsorted arrays and \(T = B*n^2\) for sorted ones. Suppose that
   you know this based on some benchmarks (which we shall touch upon
   later). Suppose then that we can expect the real data for which
   the program is tuned to be percentage \(C\) of sorted data. So the
   best algorithm is the one that gives the best average case
   performance: in fact is the quick-sort if and only if
   \begin{equation}
   B (C n + (1-C) \lg n) \leq A (\lg n)
   \end{equation}
   So knowing the values for A, B and C, we can make the right call
   in every single time. This can be known at compile time, which is
   the way we can currently decide which function to use, however,
   the constraint system allows us to dynamically choose the right
   strategy based on diagnosing pathological inputs.

   So let us do both:
   #+begin_example
   :opt 'sort static tune
   #+end_example
   where for tune to be an available form if optimisation, one has to
   have both functions defined.

   The first step involves bench-marking the functions on classes of
   representative data, which will be made part of the code-base.

   #+begin_example
   .benchmark 'short = (
      (2, 1, 3),
      (3, 1 ,-5, 24),
      (-15, 52, 47, 2000000, -5434343),
      (0xffff, -0xffff, 0xfffe, 0xfffc),
   )

   .benchmark 'long = (
       (arithmeticSeries 55 0 1) +++ (arithmeticSeries -32 1 3),
       (random (type: 'MersenneTwister, seed: 11111995, quantity: 1024)),
       (arithmeticSeries 0 1024 1).randomise(seed: 243632),
       (locale.countries),
   )

   .benchmark 'sorted (
       (arithmeticSeries 0 1024 1),
       (arithmeticSeries 1024, 0, 1),
       (geometricSeries 1 1024 2),
   )

   .benchmark 'parametric seed quantity (
       (random (type: 'MersenneTwister, seed: seed, quantity: quantity ))
   )
   #+end_example
   The first two are simply representative classes of data, when a
   benchmark is run, these set the baseline of comparison. Within
   each class, the benchmark establishes the likely performance
   leader. However, as this data can vary in many ways, the answer is
   more of the kind: average performance of this particular
   implementation was better for this class of tests. This is
   perfectly acceptable in many cases, especially if the classes are
   representative of radically different use cases.

   In our particular example the 'short tests are what's likely to be
   needed, if we're dealing with user facing data in a combo-box with
   few choices. The long benchmark is what might be needed for the
   somewhat larger combo-boxes, or table views. This is what you would
   typically want to have. And the same goes for sorted
   benchmarks. If we have largely sorted collections and the sorting
   algorithm is going to encounter them with some probability, it's
   useful to know how catastrophic of a failure that would cause with
   =quicksort=.

   Run the =sorted= class benchmarks.
   #+begin_example
   :bench run 'sort on ('sorted)
   #+end_example
   So suppose we ran all the benchmarks. The data will be saved and
   presented with several =.csv= files for memory used, the number of
   cache misses, the number of cache hits, branch mispredictions, the
   CPU clock time taken, excluding context switches, and so on. This
   data you can plot with =bokeh=, or your choice of visualisation
   software. You can draw conclusions and then make a few decisions.

   The first kind of decision that you might want to do is to
   manually intervene, and annotate all the sorts to be =quicksort= s.
   For that you would do:
   #+begin_example
   :rst function 'sort
   #+end_example
   Which will present you with the sources for all implementations,
   #+begin_example
   :resolve 'sort all
   #+end_example
   which will prompt you with every kind of collision that would fail
   to resolve based on constraints/type (in our case just two). We
   can't have functions with identical signatures. But we can have
   names like =sort= work as placeholders for concrete values,
   e.g. functions, which may be chosen automatically, but based on
   principles that are not built into the type inference. So, the
   first step is to give the two functions distinct
   names. Fortunately, as we have the sources laid out conveniently,
   and the optimisation is of a /particular/ kind, we can change the
   names of the functions. This should not break the rest of the
   code-base, because post
   #+begin_example
   :commit
   #+end_example
   the resolution "wizard" will ask us to make the sweeping change of
   figuring out which one to use and when.

   Here is where the fun begins. The easiest way is to just say we
   want all sorts to be =quicksort=.
   #+begin_example
   :resolve 'sort 'quicksort because "=quicksort= has quick in the name."
   #+end_example
   Which will replace every instance of scoped usage of sort with a
   header-line definition of
   #+begin_example
   "=quicksort has quick in the name="
   sort = quicksort
   #+end_example
   The code-base's AST is otherwise untouched, and this line appears
   where the definition of the sort function would be.

   Of course, the more fun option would be to say
   #+begin_example
   :resolve 'sort 'quicksort benchmark
   #+end_example
   This will create a view of each usage of the sort function, and
   ask you to annotate each usage with the kind of test-case that
   best represents the likely data. The best way to do that, is to
   annotate the function as follows.
   #+begin_example
   exampleuseofSort (sort arg::<LikelyRepresentedBy(sorted)>) blah
   #+end_example
   thanks to this annotation the same constraint resolution system
   that we employed all throughout this book, can very easily add
   annotations to the core functions, =quicksort= and =mergesort=, and
   use a choice operator to assign
   #+begin_example
   sort = (quicksort|mergesort)?
   "Based on included benchmarks."
   #+end_example
   So if the data is <LikelyRepresentedBy(sorted)> it will choose the =mergesort=,
   and otherwise =quicksort=.

   Of course, this is not at all scientific, and to do the kind of
   cost-benefit-analysis that we performed earlier with parameters,
   we can do something a little more advanced.

   Suppose that we have constructed exemplary representatives of
   totally randomised data, and a proportion with sorted data. We can
   say with more granularity, that any particular input has
   probability \(C\) of being sorted, and \(1-C\) to be totally
   randomised. Then we can do something along the lines of
   #+begin_example
   exampleuseofSort (sort arg::<DataProfile((sorted, c), (long, 1-c)))>) blah
   #+end_example
   This is going to resolve to one function, but is much more
   explicit about our assumptions. Likelihood isn't a =yes-or-no=
   predicate, it's a spectrum and there are circumstances where the
   probability of 51% is =Likely=, and when 99% is not sufficiently
   likely.

** The big-O notation

   This is something that we learn at university that is remarkably
   useful up until the point it isn't. For most applications, the big
   O notation asymptotic estimate is far from the expected
   performance. As we've seen in the previous section's example, the
   =mergesort=, while having better asymptotic time complexity is not
   usually faster. So getting rid of the constant factors is not
   usually a good idea.

   Most of our training however can come in handy. We know that
   e.g. =quicksort= is at worst quadratic, so we can try and fit the
   parametric bench-marking with a very good first hypothesis of the
   worst-case performance. This with a few points of contact will
   allow one to use something like e.g. nested sampling to see how
   well the data can be extrapolated, without actually going to
   extreme lengths of sampling at ridiculously large array sizes. By
   comparing the quadratic fit, to the \(O(n \lg n)\) of =mergesort=,
   we can have a relatively rigorous foundation for choosing one
   function over the other.

   While we have called it parametric, the series permits data
   augmentation, and the knobs will be turned and adjusted for
   obtaining samples over a wide spectrum of representative
   data. However, it is not the \(n\) against which parametric
   fitting is done. Instead, the \(n\) needs to be something that can
   be computed cheaply and without knowing what was used to generate
   the data. So we run a different kind of benchmark.

   #+begin_example
   :bench run 'sort on 'parametric

   Running parametric benchmark.
   Please specify how to treat the parameters (default: tune).
   > seed: de-correlate,
   > quantity: tune,

   Please specify the parameters derived from a test case.
   > n: (a) |-> a.len()

   Please specify expected correlations if any.
   > quantity == n
   > ()

   C-c C-c
   #+end_example
   After this little back and forth, the program will sample over a
   set of values. It can be given a soft deadline, or it can be run
   interactively, or given a precision criterion to
   terminate. Regardless, there are a few things to go over. First,
   the few first test cases are chosen randomly. As with most things,
   you should take care and ensure that the values are sensibly
   scaled. The upper bound is set by the hardware and the target
   platform, but it is assumed that smaller values of the parameters
   are safe to use and relatively fast to evaluate. Adjust these as
   necessary.

   The first thing that you might find unorthodox is the fact that we
   need to tell how parameters are treated. The =seed= cannot
   reasonably be correlated with the performance of the function. If
   it is, it means that the dataset isn't large enough. Moreover,
   because this is a parameter that should not be correlated, any
   dispersion seen is not a dependence that we can account for, it's
   an error. The =quantity= is the knob you want to adjust to change
   the observable =n=. The reasoning behind this choice shall become
   clear quite soon.

   What happens? The process begins by taking a few random samples,
   and based on them checks for correlations between tuning
   parameters and observable input properties. It has to do
   sufficiently many passes to identify with "good" accuracy what
   quantity do you need to get the desired =n=. At the same time, we
   get a collection of other readings: cache misses/hits, branch
   prediction failures, CPU wall time, average CPU cycle count and so
   on. These readings are prioritised, one can set these on a
   per-benchmark basis. The good news is, that this data shall be
   supplemental to choosing the best from a number of models. These
   can be polynomials of various shapes, a poly-logarithmic
   complexity, an exponential and so on. After some preliminary
   testing, the hypotheses are tested for extrapolation, but to fit
   within the deadline if the hypotheses are true with a 20% margin
   of error[fn::to be more precise, the algorithm aborts if it
   overextends the deadline by more than 20%. ].

   Finally, based on the extensive benchmarks the models for the
   readings are ranked from most to least likely. Using these models,
   one can set additional input specialisation constraints, that
   depend on the observable =n=. So our benchmark is likely to
   produce \(A n \lg n + B ...\) for the =mergesort= and something
   similar for =quicksort=.

   As a result, we can make an educated guess as to which exact cases
   produce which kinds of pathological performance. Knowing not only
   the vague scaling but the precise number of CPU cycles according
   to a relatively repeatable model, we can construct at compile time
   an optimal version of sort, that reverts to a particular kind of
   sort based on quick tests that can be done at runtime. In fact,
   this is what we humans do: the standard library versions of sort
   use a form of quicksort that reverts to bubble or insertion sorts
   for certain lengths.

   There may be a subtle problem here, however. Notice that both
   algorithms are recursive. We mentioned that we were going to name
   each function uniquely, but what would happen if we had kept
   =sort= as the part of recursive call? Wouldn't the act of
   optimisation after profiling change the behaviours of the
   function? They might! Consider what would happen if we gave
   =quicksort= a next-to-worst-case behaving array. It is made, in
   the code that is adjacent to the sorting code by gluing together
   two sorted arrays:
   #+begin_example
   list = (0, 1, 2, 3, 4, 5, 6) +++ (0, -1, -2, -3, -4, -5, -6)
   #+end_example
   the first iteration is going to divide the array into two almost
   equal halves, which is the best case behaviour for =quicksort=,
   however every subsequent recursive call will be the worst case
   scenario. We know this at compile time, but we can't reasonably
   expect for the compiler to flag the sub-collections as sorted,
   which would turn this into a =mergesort= recursive call rather
   than a =quicksort= recursive call. What would the compiler infer
   if at the time of inference, the two were allowed to mix? Probably
   that =quicksort= is better for such datasets than it really is.

   Now consider what would =mergesort= do? If it is given /a priori/
   knowledge, that the two halves of the array are pre-sorted, it
   does one recursive step and merges the two halves. But we can't
   with the guided profiling be able to identify this sort of
   optimisation. The choice is based on the length of the array, and
   it is /just/ over the threshold after which the optimiser would
   consider =quicksort= as a potential substitute. Moreover, the
   original array is not sorted, so the compiler, is not expecting
   =mergesort= to do better, even if it had access to all of the
   benchmark data.

   Had we not exposed the system that allows the optimisation to take
   place in a transparent fashion to the user, the obvious solution
   to this problem wouldn't be possible.
   #+begin_example
   list = (0, 1, 2, 3, 4, 5, 6) +++ (0, -1, -2, -3, -4, -5, -6)
   list .constrain <LikelyRepresentedBy(/benchmarks/sorted)>
   #+end_example

   Notice that we are using a new-ish language feature:
   name-spaces. First, why do we need it here? Well, because the
   context of guided optimisation made it obvious that the =sorted=
   actually means =benchmarks/sorted= in all previous cases. You
   should be surprised to know that once the code-base's state is
   committed, the instances of =sorted= shall be replaced with their
   properly name-spaced counterparts.

   Unlike most languages, e.g. Python, Haskell we don't use
   dots. Uniform function call syntax makes the dot mean "use this as
   the first argument". If we are to use the dot in another context,
   that would break the semantics [fn:: one could argue that the dot
   prefix of the compiler built-ins is not consistent with this
   either. To them I say, the first argument is the canvas of the
   language. ]. We are not going to parrot every feature of accepted
   languages just because they are standard. Moreover, I always found
   that looking at name-spaces in the same way as one would for
   directories allows for some neat tricks. But we shall touch upon
   this far later.

   For now, we see that the constraint setting, averted a catastrophe:
   =mergesort= is used, but not =quicksort=. But as the initiated
   reader can see, we leave a lot of performance on the table: what
   would happen is a potentially \(O(7 \lg 7)\) recursive ladder that
   needs to collapse back to a single collection. However, if we were
   clever, we would say that =mergesort list = list= if and only if
   =list::<Sorted>=. A single primitive (i.e. non-collection value) is
   =Sorted= according to how we defined the constraint, which recovers
   the base case and extends it to a vast class of optimisations. For
   example, we could optimise the scope of the function where we
   =mergesort= the =list=. This is a good introduction for what comes
   next, because it shows a principle that we'll need to implement
   high-level tail call optimisation. We can ask the compiler to
   specifically inline one iteration of a recursive function.
   #+begin_example
   :opt inline 'sort
   #+end_example
   This will produce something that looks like this:
   #+begin_example
   ~/mergesort/merge (mergesort list_[0:(len/2)]) (mergesort list_[(len/2):-0])
   #+end_example
   Now we can use our foreknowledge of the =<Sorted>=-ness and
   optimise away the calls to =mergesort=,
   #+begin_example
   ~/mergesort/merge list_[0:(len/2)] list_[(len/2):-0]
   #+end_example
   and even bind the two halves to a temporary object, so we don't
   merge split and then =mergesort/merge=, but rather do everything
   immediately.

   At this point, happy with ourselves, we realise that using =sort=
   was something we saw on stackoverflow, and don't necessarily want
   our supervisor to see, so we commit to spec, but are greeted with
   the following message.

   #+begin_example
     ~/mergesort/merge is outside the current scope. To commit to
     specification source, you must bring it into present scope. You can
     either [m]ove or [d]uplicate the utility function.
   #+end_example

   This message is self-explanatory, but the motivation behind it
   needs to be clear. We allow local bindings to exist solely because
   we want functions to operate as black-boxes where one can only
   observe the outward behaviour, and the internals are
   irrelevant. By allowing you to commit the optimisation that goes
   inside the scope of =mergesort=, you are opening the door to the
   possibility of not being able to strip that function from
   =mergesort= in a later restructuring. And the onus is on the user
   that stated that they need to use that function, knowing full well
   that it was defined inside =mergesort=.

   Of course, using this at optimisation, with that information
   available is fine. The use of name-spaces and scoping is not
   something that computers or compilers require, but rather
   something we invented as a common courtesy to fellow
   developers. The name-spaces were invented for the same reason.
   However, if someone is utterly convinced that the behaviour must
   be made part of the program specification (despite the better part
   of FORTRAN, C and many other major languages' experience telling
   us otherwise), we are making it so that the strategies that one
   could employ to bodge the answer, are readily available.

   In this particular case, it is better to keep things as they are,
   and not allow anything beyond the optimiser, make use of the exact
   structure of the code-base. However, if something changes and this
   particular part of the AST needs to be re-optimised, we might have
   to make the manual intervention more than once. So making this
   code part of the spec isn't such a terrible idea either (just
   marginally less clever). So we can either: move the functionality
   out of =mergesort=, making it public, but polluting the global
   =namespace=, or copy the functionality into the outer scope, also
   allowing one to make changes to the local =merge= function
   independently from the one that we use here.

   Of course, none of these approaches are ideal. We can improve the
   situation slightly, by understanding how the optimiser would work
   in other, more exotic circumstances. We must therefore have a look
   at the components that make up the optimiser.

** How it is done

   We start, as previously, with the highest level. The source code
   represents a multi-layered structure. It is not simply a text file
   that represents a program, it's an abstract file, that may be human
   readable to an extent, but is, in effect an AST. It is shipped with
   a collection of name and construct bindings, that when applied to
   the AST can generate something that you as a programmer are
   familiar with: a text file. The types of modifications that you can
   do to that text file, can be re-parsed and transformed into an AST
   transformation, that is then being recorded non-destructively, and
   applied whenever you request the newest version spec.  It is, in
   the end, a collection of hashes and a table of symbols, both of
   which are language accessible, but are far easier for a computer to
   read and optimise. Frankly, not much to see here, that we haven't
   already seen or talked about.

   What we haven't talked about, is discuss what the AST can contain,
   and should contain. In a normal C program, the source file, when
   all of the macro substitutions are done, contains definitions of
   functions and =structs=. Sure, you /can/ regard macros a part of the
   source code, but they are highly contextual, and anyone that uses
   them in any serious capacity is asking for trouble in either C or
   C++. Some languages like C++ have more fancy features, like
   templates, that don't necessarily have to be instantiated, but are
   meaningless otherwise, classes that are =structs=, but follow
   specific rules about object lifetime, and recently, module level
   declarations, as to what is to be exported and what isn't.

   In our case, it is a bit more complicated. First of all, our
   language has distinct operational types: functions, procedures and
   subroutines. While subroutines are most closely resembling macros
   that evaluate to inline assembly, procedures and functions are most
   similar to function templates in =C++=. Whether or not a template
   can be instantiated, in what form, and where depends is what the
   C++ compiler works hard to solve.

   But what about types? Well, we don't really have any types. They
   are an illusion. But what you might find surprising, is that most
   languages don't treat types any better, they're just far better at
   obfuscating the fact. =C++='s guiding design principle was strong
   typing which to a large extent lead to pure concepts of what a type
   and a class is and a clear delineation of what it isn't. So would
   it surprise you to know that an =int= in =C++= can technically both
   be a language keyword and a =typedef=? There are no rules in the
   standard that prohibit you from defining your own compiler
   intrinsic architecture dependent and largely inconsequential set of
   types like =std::uint8= but done by you and by hand. You could then
   =typedef= what you had previously, and end up with a working
   language, that has most of the standard semantics. In other words,
   the compiler gets to decide if =int= is a 32-bit integer, or not.

   What we allow our programmers to do is to rely on a different set
   of ideas. When we say =Int= or =Integer=, we don't usually mean a
   specific type, but actually want something a little more subtle. In
   the first case, in Haskell, we want a fixed precision number that
   is standard on the platform in question. It could be 32-bit, or it
   could be 8-bit, but what is important, is that it is standard and
   fixed length. When we say the same thing to our language, we
   communicate an intent, and that intent leads to the compiler doing
   most of the dirty work:
   1) it deduces the minimum and maximum possible results of
      calculations that are to be bound to =Int= s.
   2) If then looks at the optimisation criteria: if you want to
      optimise for speed, it may be faster to use a larger type, and
      if you're squeezing as much memory as you can, you may use two
      =shorts= (16 bit) that add up to an Int (32 bit).
   3) It looks at the usage of the bound value. Under certain
      constraints, it may be faster to do most operations on floating
      points, or to artificially manufacture a floating-point type
      number by gluing several integral types.
   4) It then ranks the implementation available types and chooses one
      concrete type. It can be a random pick, or it could be an
      educated guess. Either way, it's not taken out of a vacuum, it
      at least tries to ensure that it's picking the type that isn't
      the worst.
   5) Every function/procedure that uses these types, then gets turned
      into a subroutine that accepts specific length types. There
      could be many variations, depending on the optimisation
      criteria, typically, if you want maximum performance, the same
      value can be represented with multiple variables, each of which
      is processed by an optimum subroutine.
   6) All of the inline assembly is in-lined.
   This step is called /type solidification/ or /crystallisation/. By
   contrast in Haskell, saying something is an =Int=, means that it is
   an =Int=, if the architecture supports it. It can be done away
   with, but it can't (shouldn't) become a =short=, or a floating
   point.

   Haskell does do type deduction, and usage dictates type to an
   extent. But the Haskell typing system allows for a limited
   granularity, so you can't say, "oh, this function doesn't contain
   anything that could lead me to believe that the value is non-zero,
   but I have an explicit conditional that checks if the value is
   greater than one in every case it is used. I can technically get
   away with an unsigned short. "

   So instead of storing types and =structs= and classes, we instead
   store constraints. Having the constraints with their definitions,
   with the usage of values abiding by those constraints, shall permit
   us to generate the types and =structs= and classes. Each constraint
   is given a unique ID, such that only if you are using two identical
   constraints, can you reliably say that you did. A constraint ca be
   packaged with the source code as a declaration, or a definition, so
   you don't have to embed a large library in order to be able to use
   it in several code-bases.

   What else is in the source file? As we've already discussed, there
   are two representations of the source code, the specification and
   the optimised render, both of which are represented with
   intermediate steps in the Source control system. Leaves of the tree
   of optimisations can be embedded or referenced, depending on
   whether or not the optimisation is specific to the project, or a
   popular optimisation pulled in from the internet or some other
   source.  So when you want to produce a program, we take either the
   spec or the render and produce assembly.

* /Utilitas, Firmitas, Venustas/

  Part of the reason why Clang and LLVM are such popular choices for
  fledgling languages, is the layered and intelligible structure of
  the optimisation process.

  It is easy to write a compiler for a new language, because if you
  can represent it in the LLVM language, or an intermediate
  representation you can, in effect, produce a compiler. Indeed, one
  of the ways our language can be implemented at a very early stage
  is to have LLVM-capable subroutines rather than inline assembly,
  as we have previously shown. In the end, we'd like to be able to
  do both, and simply switch between one and the other. The reason
  we are able to do that, is because what we're doing is essentially
  borrowing the main idea behind LLVM, but rather than use that as a
  guideline for creating a new optimising compiler, we use the idea
  of layered optimisation as the entire guiding principle for the
  design of the compilation process. The only crucial difference is
  that we want the language to be a bit higher level.

  Let's take a step back and consider how typically a program is
  made.

  First you have a text file, human readable, intelligible and this
  is the extent to which most programmers are acquainted with most
  languages. The point here, is that we use a language that humans
  can understand more easily, to explain what they want the computer
  to do in a more comfortable and precise setting. Part of the
  reason why compilers were invented in the first place, was that
  people want to write things like
  #+begin_example
  1 + 2*3*sin x
  #+end_example
  instead of e.g.
  #+BEGIN_SRC elisp
    (add 1 (times 2 3 (call 'sin (valueOf 'x))))
  #+END_SRC
  or even
  #+BEGIN_SRC asm
    result:
            subq    $8, %rsp
            movsd   x(%rip), %xmm0
            call    sin
            mulsd   .LC0(%rip), %xmm0
            addsd   .LC1(%rip), %xmm0
            addq    $8, %rsp
            cvttsd2sil      %xmm0, %eax
            ret
    x:
            .zero   8
    .LC0:
            .long   0
            .long   1075314688
    .LC1:
            .long   0
            .long   1072693248
  #+END_SRC
  Because of the fact that we don't think in binary or octal
  (despite that being much better for the accuracy of our mental
  maths), or the fact that in maths we do use expressions of the
  first kind for their compactness, we need a system that translates
  expressive language to Machine code.

  Parsing the human readable text can be very easy, or exceptionally
  difficult. The original high level programming languages, like
  FORTRAN or Algol, have mostly included features that would have
  made parsing the text exceptionally easy. As a result, the
  compilation didn't take long, and the resulting code is very easy
  to optimise and translate to machine instructions. The trouble is,
  it take much longer to solve an abstract problem in said
  languages. FORTRAN was tailored to solving mathematical and
  Physics-related problems, with profound applications in sciences,
  but not particularly useful for systems programming. Just try to
  write device drivers in FORTRAN, and see what I mean.

  The issue with creating a language is a balancing act of creating
  something that can be turned into efficient machine code, is
  expressive, and easy to parse. Haskell, as a functional language
  has the scale tipped strongly towards being expressive, while its
  ML brethren sacrifice a bit of expressiveness for being more
  efficient at run-time. The early lisps, of course, tend towards
  being more easy to transform into machine code, especially on a
  stack-based CPU architecture. Do you want a program that you can
  write in a jiffy, that runs in a jiffy, or compiles in a jiffy, is
  a tough question. However, clever use of the best computer, one
  that's housed in your cranium, you can make significant progress
  on all of these fronts and find a sweet spot: not all features
  that make your language more expressive, necessarily make it
  harder to interpret. Sometimes, you can build an entire language
  on that principle, such as LLVM.

  If you think about it, we also have something that looks like an
  intermediate representation. In fact, everything that goes beyond
  spec, or is an optimisation thereof, technically is an
  intermediate representation.

  Finally, and historically the first thing that our fathers and
  grandfathers developed, was a very crude intermediary between
  machine code and something that can be interpreted by humans, but
  operates fully on the basis of how a transistor would. So one can
  look at the assembly, and tell immediately, what's going on under
  the hood.

  But why then, do we prefer to translate to multiple assembly
  languages directly, rather than using LLVM? Mainly, because this
  assumes a model that worked for more-less half a century, that we
  deliberately want to avoid. LLVM assumes that the programmer wants
  to create a single text file, that becomes a translation unit, then
  uses that information to obtain an end program. If they want to see
  the optimisations, they can look at the intermediate representation
  or the assembly, if they wish. I take fundamental issue with that
  attitude: optimisations are always a game of trade-offs, and while
  it is technically possible to instruct LLVM to do fine-grained
  tuning, even profiling guided tuning, all of that is stuff that
  you're not supposed to touch. None of the C++ textbooks ever tell
  you to read the assembly output, or start teaching you with
  assembly, and most certainly never teach you about the intermediate
  representations. This is a consequence of the fact that only the
  top-level language is being fixed by the standard, and everything
  else is considered an implementation detail. So I can't just take
  the improvements that were made to clang and drop them into GCC, if
  you want separate compilers to do things the same way, you have to
  have a hard guarantee that they are the same compiler in essence.

  So our language is a very different beast. There is no intermediate
  representation, just a high level language and a set of rewrite
  rules, that the compiler has to figure out how to apply. So
  optimisations to the program can be made at least somewhat
  reproducible, so even if there are seven competing compilers for
  the language, they all agree on the optimisations that they can
  make. As a result, the performance of the compiler itself is taken
  out of the equation and the one that is the most maintainable or
  has other intangible qualities that you might want to encourage --
  wins. By lifting the optimisations into the standard, we allow
  people everywhere across the world to make use of audit-able
  optimisations, with compilers sufficiently simple to be audit-able
  themselves, without much penalty.

  That final point needs some elaboration. We shall assume that one
  is familiar with "meditations on trusting trust". If not, the brief
  is that even in the 60's it was possible to patch compilers to
  inject malicious behaviour into open source programs (because
  closed-source didn't make much sense back then). Just because you
  could audit the program's source code and didn't find any
  suspicious behaviour, didn't mean that a binary that you yourself
  compiled didn't have a backdoor, or didn't gather telemetry. You
  not only need to be able to audit the program, but also the
  compiler, and to this day, not a lot of compilers can be searched
  for malicious code injection. They are simply far too big and far
  too difficult to understand. Moreover, because the optimisations
  are kept opaque, even if you audited the source code for the
  compiler, or better yet, wrote one yourself, the compiler you used
  to compile the compiler could have injected malicious behaviour. If
  it is smart enough to understand if you're trying to add numbers in
  a for loop and replace that with an analytical formula, then it can
  definitely make good progress on what could be a parser.

  So, what we want, is for the high level language to be able to
  produce things that are simple macro substitutions of the inline
  assembly, in an open transparent and strictly-opt-in fashion. but
  this kind of approach necessitates a completely different
  architecture of optimisation, compilation and source
  distribution. What follows is a brief overview of the proposed
  architecture.

  But here, you can see, that I can opt out of telling you what the
   optimisation steps were. You know the spec, which may be
   exceedingly slow and inefficient, and you aren't given much in
   terms of what the optimisation was. I can technically compile the
   spec and get something that complies to all of the external
   contracts that a program may have, or I can compile the final
   rendition, and get something that does the job, and is technically
   a specification of what it does, but not the best choice if looking
   for code quality. This is still much better than what the
   proprietary distribution model is.

   But why does most software come as a binary only? This is a
   complicated, multifaceted problem, we shall try to delve not too
   deeply into.

** FOSS binaries

   The open source software world, uses binary only distribution of
   software primarily because compiling on the end user machine can be
   remarkably difficult and time-consuming.  The fact of the matter
   is, that if compiler automatic optimisation is like for example –
   03 in GCC could bring a significant performance uplift, on the
   factor of several times, distributions like Gentoo, and package
   management systems similar to emerge, would be much more
   prolific. In fact one of the main reasons why updates on Linux
   systems are generally much less painful than a Microsoft based
   systems, is due to the fact that packages on Linux are distributed
   in binary only format. Compiling and optimising is usually not
   worth the end users time, and even the users of Gentoo, generally
   compile for reasons other than optimisations, or at least not of
   temporal characteristics of their programs. There is however a
   significant penalty in the temporal sense, to trying to compile all
   your software. Certain low-level languages such as C and FORTRAN
   have become increasingly scarce. On the other hand
   difficult-to-compile languages such as C++, or Haskell, are
   becoming more and more common. The time it takes to compile a
   certain utility has become increasingly less tenable, to the point
   where compiling your own compiler, and your own kernel are daunting
   tasks that no-one is willing to undertake on a regular basis. The
   issue is compounded by the fact, that most open source software is
   improving at an ever increasing pace, making it impossible to
   compile a version of software once, and not need to recompile it in
   less than a year after initial deployment.

   Another bit far less important consideration is that of size on the
   hardware storage. Solid-state devices have a limited number of
   write cycles they can endure without failure. As a result, people
   are far more conscious of pulling in the entire GitHub repository
   for GCC, even though this is what security mandates. People are
   however far less conscious of the fact that the proprietary
   distribution models that don't permit alternate means of
   installation, and general apathy towards common standards of
   software resource management can lead to multiple orders of
   magnitude more wear than the several megabytes that the GitHub
   repository would have pulled in. Proper usage of =USE flags= have
   saved me a considerable amount of wear on my system, as well as
   allowed me to make due with a measly 8Gb of system memory. And for
   all intents and purposes, while a source-centric distribution model
   most often relies on client-side compilation, it doesn't have
   to. Software can still be distributed in binary form, but net the
   user easy access to the source code, should they so wish. Part of
   the reason why binary-only distribution is so prolific, is because
   it actually uses a lowest common denominator configuration of a
   program, that can be safely distributed and provide the best
   average user experience. The reason why USE flags are not as
   popular in binary distributions, is because the modularity of the
   programs doesn't really permit modularity of user-facing functions.

   When someone designs an application, they rarely if ever have
   alternate and parallel modularisation schemes in mind. For example,
   suppose I have a text editor program. I can have object files
   responsible for printing, for keybindings, for the graphical user
   interface. Suppose that you're a minimalist, and he didn't want any
   pesky printer support, eating up our valuable screen real estate,
   and coming up in command-line searches. We can patch it out. But we
   have to recompile the entire code-base to achieve that: mainly
   because you want to have a default keybinding for printing, and a
   GUI button and then some core function that takes the memory buffer
   and turns that into postscript. And languages like C++ won't even
   allow you to not link against a static object without modifying at
   least all of its dependencies. And you can't really ask the program
   not to install any default keybindings; technically this isn't part
   of the program, more of the documentation, but if I shipped you a
   program that was a blank slate, without even the function to call
   other functions, you'd be very unhappy. But if you think about it,
   you have to come up with a sophisticated set of patches that achieve
   the effect of USE flags. You can't elide the process of re-parsing
   the compiled objects, because re-linking to the proper object is
   something that humans do by hand and usually poorly.

   The trouble here is that when humans write code, they modularise
   according to two guiding principles. The first, and the one usually
   cited as the more important, is for maintainability and logical
   code organisation. The single entry point is usually in a file
   called main, configuration options are usually listed in an
   altogether different language, or at the very list brought back out
   into a list, rather than placed where they logically belong. If you
   think about how the source code for =dwm= is organised, you'll see
   that it has a =config.h= that contains the keybindings all in one
   place, rather than near the definition of the macro that is used to
   define the keybindings, or even the functions that these
   keybindings call. So if I wanted to recompile =dwm= without support
   for calling sub-processes, but still able to receive general purpose
   keybindings for controlling its internal state, I would not be able
   to just tell the compiler not to link against one object that it
   already compiled. Instead I'd need to define a macro. Macros are
   terrible, but I'm not just saying that. In a second we'll see why
   they're bad, and what can be done to avoid them.

** Suckless configuration

   This may seem an odd place to discuss configuration, but it is
   about time. First of all, allow me to remark, that most modern
   software follows a set and quite particular pattern of
   operation. Be it a daemon or a single fire utility, what we
   normally see is a program that has a pre-set structure. It can be
   published as a binary, or it can be published as source code, or it
   could be an interpreted program that nonetheless does something
   incredibly useful and can only be done in an interpreted
   fashion. We do however have a procedural view of programming: when
   we want to create a directory, we call a procedural program
   #+BEGIN_SRC bash
   mkdir -p /path/to/file/
   #+END_SRC
   that has no return value, but has an output, with a separate stream
   just for errors. It can have side effects, and unfortunately, as
   someone who may have attempted to clean up their home directory on
   a Linux machine, to make it XDG specification compliant, pollute
   the file-system. While it was fine in the age of "don't run software
   that you don't understand the source code, of", in the age of
   enormous desktop environments, and a complete disregard for
   application security, such a gaping hole is to be filled.

   The main issue is that rather than provide a standardised
   in-language mechanism for tracking changes of parameters and
   configuration, the approach had been majorly stuck in the days of
   assembly: much like when entering a function, some registers are to
   be prepared, the main function accepts a collection of strings,
   that the programmer is responsible for parsing. Moreover, there are
   no mechanisms for enforcing some of the more sane defaults of
   configuration file behaviour: local files take priority over global
   configuration, each command line parameter has a corresponding
   in-file parameter, passing an invalid configuration of parameters
   leads to help message being shown, etc.

   The plethora of implementations and insufficient development of
   standards lead to these rules being taken per faith, rather than as
   a solid warrant. Moreover, there are some implicit assumptions that
   are implementation rather than standard dependent: Windows/DOS
   programs use the front slash to delineate short option names, UNIX
   uses a dash, but not always. It is assumed that arguments given
   with a single dash are short and that arguments with more than one
   are long, but not always, there might not always be a space in
   between the argument and the value. The arguments are not to be
   permuted, there's always a single correct order to put them in, but
   some implementations shall allow for the permutations, while others
   will not.

   This lack of standardisation is something that we should resolve,
   but unfortunately our chosen architecture isn't the best at
   attacking the problem. However, much like the =Int= s, =Float= s,
   and =Collection= s we must provide a standard. Unlike languages
   such as C/C++ and Haskell, we shall not content ourselves with the
   standard binary blob distribution model. Instead, we shall describe
   the final piece of the puzzle, that when falling into place, shall
   make everything else make more sense.

   We shall make a final built-in constraint alias for =Parameter=,
   =Setting= and =Option=. We shall describe each individually:
   - =Parameter=: something the program needs for correct
     operation. It makes no sense to create a directory if the path is
     completely unspecified. All of the parameters need to be
     specified to execute the program. There may be equivalent
     specifications of parameters, hence the Parameter type accepts a
     specification identifier, that is a type and optionally, a
     symbol. For example:
     #+begin_example
     main :: Parameter(File, 'fromDisk) -> ()
     main filename = .infer

     main :: Parameter(URL, 'fromInternet) -> ()
     main fileURL = .infer
     #+end_example
     No parameter can ever have a default value.
   - =Setting=. Something that the program needs for correct
     operation, but isn't something that the user of the program
     necessarily knows about or can specify. Every setting has to have
     a default value, that needs to be set explicitly in the
     configuration file, and if no such file exists, then one must be
     created by the program in the most global scope to which writing
     permissions are given, and a clear message shown to the
     user. Settings are read-only.
   - =Option=: something that the program doesn't need, but can and
     should use. As the name suggests, these can either be specified
     or not, and the program should run regardless.

   So, let's make a few examples.

*** Parameters

    First, we have a program that computes the n-Th Fibonacci
    number. It makes no sense to run this program without the ordinal,
    hence the number =n= is a parameter. It has an integral type so
    it's safe to assume that it's an =Int=.

    #+begin_example
    main :: Parameter(Int, 'singleNumber) -> Int
    main n =
       fib (unwrap(n) -1) + fib (unwrap (n) -2)
    #+end_example

    Then we want to produce a program that can also optionally take a
    pair of Fibonacci numbers and produce everything in between the
    upper and the lower bound.
    #+begin_example
    main :: Parameter(Int, 'twoNumbers) -> Parameter(Int, 'twoNumbers') -> Int
    main lower upper =
       fibs (unwrap lower) (unwrap upper)
    #+end_example

    And at some point we realise that we can specify them as inclusive
    or exclusive ranges,
    #+begin_example
    main :: Int<Parameter(a:'twoInclusive)> -> Int<Parameter(a)> -> Int
    main lowerInc upperInc =
       fibs ((unwrap lower ) -1 ) ((unwrap upper) +1 )
    #+end_example
    and make note of the different symbols. These have to be distinct,
    all parameter tuples have to have a series to which they belong to,
    although one can use a shorthand for that, because there may be
    cases where information from two equivalent specifications can be
    used.

    The symbols must be bound to the following structure:
    #+begin_example
      singleNumber = (
        "description" : "Compute the {'n}-Th Fibonacci number",
        'n : "The ordinal of the Fibonacci numbers" :: Int<NonNegative>
      )
    #+end_example
    here we've used recurrent naming, because we want the description
    to be bound to the representation of the symbol in the source code,
    even when that changes.

    While the usage of the parameters may differ, there must be a
    constraint that uniquely identifies which parameter in the
    specification is used. We've used =Parameter(Int, 'singleNumber)=,
    but here the description also requires that it be non-negative,
    hence this will fail to compile unless we amend one or the other.

    A similar binding for the second case would also fail to compile:
    #+begin_example
      twoNumbers = (
          "Description" : "specify the {'lower} and the {'upper}
                 between which to generate Fibonacci numbers",
          'lower : "non-inclusive lower bound" :: Int,
          'upper : "non-inclusive upper bound" :: Int<GreaterThan(lower)
          )
    #+end_example
    This binding also reveals a problem in our definition, the function
    signature did not specify which one was which. This will fail to
    compile, but a helpful fix will be proposed: another alias
    #+begin_example
    main :: Parameter(a:Int, 'twoNumbers) -> Parameter(Int<GreaterThan(a)>, 'twoNumbers') -> Int
    main lower upper =
       fibs (unwrap lower) (unwrap upper)
    #+end_example
    which tells us exactly how to order the numbers. The only issue
    that remains is that we could have made the range specification
    more explicit with a different signature, but we'll come back to
    this.

    It is important to note, that Parameters' values are assumed to be
    immutable but volatile. That seems an odd choice, but one with
    profound consequences. The program cannot physically access the
    input parameters, so it cannot change them. That's true in C-like
    languages, changing the =argv= has no bearing on the operating
    system or the shell, the parameters are copied and hence no
    observable consequences can take place. Hence trying to re-use
    that memory is a mistake, hence why we make the parameters
    immutable. This has the added benefit of clarifying the intent, if
    one needs to produce something that is dependent on the parameters,
    they should specify them separately.  Additionally, parameter
    unwrapping is lazy. Unless an explicit unwrap was called, the
    parameters remain encoded as a string. Finally, it is possible
    that the user might want to change the program parameters midway
    through execution. The volatility means that a signal shall be
    emitted and necessary re-computations shall be done as needed. The
    programmer is free to opt out of such processing by further
    restricting some parameters to be =NonVolatile=.

*** Settings & options

    A next step is that we recognise that sometimes the user might not
    want to use the exponentially complex implementation of =fib=. In
    fact, we might recognise that they want to use the shorter, but
    potentially more fragile Binet's formula.

    Unlike many recursive series, there is a closed form solution to
    the Fibonacci sequence. The only trouble is that the integer is
    attained by
    \begin{equation}
    F_n = \frac{\phi^n - (-\phi)^{-n}}{\sqrt{5}}
    \end{equation}
    which cannot be trivially assumed to be an integer: the golden
    ratio is not rational, hence one cannot assume that a finite
    precision computer arithmetic system can reliably produce the
    right integer. Hence, there may be good reasons (performance) to
    use this formula by default, when computing the =fib n=, but to
    let the user disable the usage of the closed form solution in
    favour of an infinite precision integer approach, because in the
    appropriate limit, the two have comparable performance.

    So how can we phrase this? Multiple ways as usual, so let's start
    simple. We can phrase this as a variable that we pass in from a
    configuration file or the command line. This is the simplest
    approach one can take. We could say, that it's an integer that
    specifies to use the Binet's formula if 0, use a memoised
    recursion if 1, and a full exponential recursion if 2. We can
    certainly do that.

    #+begin_example
    main :: Int<Parameter('singleNumber)> -> Settings -> Int
    main n settings =
         CASE settings_strategyNumber OF
         0: binet n,
         1: memoizedRecursiveFib n (),
         2: fib n.
    #+end_example
    While it's hardly neat, it definitely opened up the code. We have
    an explicit choice, but we have an oddity. We had to pass in the
    settings explicitly, do we need to do that every time? I'd argue
    that indeed, that is the case. =settings= is a name that is bound
    by the compiler, it is global, immutable, volatile, and can't be
    shadowed in any enclosing scope. =settings=, like =int= in C, must
    be regarded as a language keyword, even though it is a plain
    ``variable'', its behaviour should not generally be overridden.

    OK, so how do we populate the settings?
    #+begin_example
    settings_strategyNumber :: Setting(Int<NonNegative><LessThan(2)>)
    settings_strategyNumber =
    "Algorithm used to generate the number:
    0 - Closed form solution,
    1 - memoised recursion,
    2 - (CAUTION SLOW) recursive definition implementation"
    #+end_example
    Note that we've bound a setting's lookup value directly, which is
    not generally considered good practice. Normally, we'd have a
    JSON-like markup with keys being symbolic names, and the value
    being a tuple of string and type.

    Now the question may arise as to why would you do something like
    that? It is possible that you're in the early stages of porting
    some C/FORTRAN code that has no clear distinction between an Int
    and the value that it enumerates.

    But suppose you didn't need to preserve the old APIs, and instead
    opted for something more modern. You could do that more
    comprehensively.

    #+begin_example
    main :: Int<Parameter('singleNumber)> -> Settings -> Int
    main n settings =
         CASE settings_strategy OF
         "Binet": binet n,
         "Memoise": memoizedRecursiveFib n (),
         "Exponential": fib n.
    #+end_example
    Which is defined similarly.

    Now is an important time to think about how this is going to be
    specified. I firmly subscribe to the doctrine of separation of
    concerns. Instead of every program trying to manage its own
    settings and using standard libraries to provide them, we should
    have all programs accept the settings in a binary format. One key
    takeaway from the entire present work should be that

    """ Configuring a program to perform a task in a specific way is
    indistinguishable from programming in the configuration language
    of the program.  """

    In other words, you're not configuring =emacs= or =vim=, you're
    writing a program in =emacs lisp= and =vimscript=. Instead of
    trying to provide a plug-in that in the spirit of GNU guile makes
    multiple assumptions about the entire code-base, the standard
    library approach in our case would be that the configuration is a
    (dynamic) library that we may link against, that has a bit more
    suitable and terse syntax. This has a few advantages:
    1) The configuration checker is a separate program. It can be more
       complex than the main application, and can tell you in advance
       if a certain combination of settings breaks the application.
    2) If linking statically, there is no overhead additional to
       parsing and storing the settings. You can get software that
       runs as fast as =st= or =dwm=, because you essentially
       re-compile with burnt-in settings each time you change the
       configuration file.
    3) If linking dynamically, you save yourself the overheads of
       choosing a specific configuration format, medium and having to
       recompile the code-base as soon as something changes in some
       setting. You also avoid the overheads of parsing the
       configuration file.
    4) The program can define ways in which certain options are to be
       set, and the logical structure, but the exact implementation
       may be different, so the user is free to choose a graphical
       configuration format, or choose one of a plethora: =yaml=,
       =JSON=, =INI=, system registry, etc.
    5) You as the programmer no longer need to worry about providing a
       configuration utility. All you need to ensure is that each
       setting is properly documented and that settings are properly
       grouped. The configuration maintenance program will do the
       rest.
    6) The access rights of the application can be safely tightened,
       an unprivileged application can request the configuration be
       provided, but has no direct access to system configuration
       files, including its own.
    7) The configuration is quickly and straightforwardly possible to
       generate in-memory, allowing for things like tests and
       benchmarks to rely on realistic performance indicators.
    8) If program has configuration presets, common customisation's
       can be shared, so that people that enjoy one's workflow can
       easily adopt it.
    9) Because there's less overhead to adding a setting, more
       settings will be available.

    Thus we get the performance of a Suckless program, with the
    flexibility of a program that has graphical configuration. With
    clever application of good programming practices, one can turn
    this into a coherent configuration language, where one doesn't
    need to search far and wide within the menus of each application,
    but rather needs know only the name of the application. This
    paradigm is crudely very similar to the approach to manuals in
    command-line programs. One common paradigm is to provide a help
    option, that would print the version of the program, alongside a
    very crude representation of the program's capabilities. However,
    these features are rarely optional, cannot be removed if space is
    a consideration, and lack a certain agility. Going into a man
    page, always offers a familiar set of names, descriptions and each
    option is described in meticulous detail, while the same can
    rarely if ever be said about the =stdout= of something even as
    sophisticated as =git=.

    Options, are of course, settings that are not vital for the
    operation of the program, and can be treated in a very similar
    way. The only difference is that while the settings have to be
    specified one way or another, the options are optional.

* Primitives, =enums=, constraints

At this point we come back to the original problem of how do we create
a type out of a constraint.

We should start from simple things.  A unit structure is a placeholder
that exists solely for the type checking, but doesn't actually contain
any data.  The compiler, naturally, should try and optimise away these
data.  As such, unit structures have a few more uses here, since a
type, really is an alias for a lot of constraints.

#+begin_example
type Fun = NonZero, Divisible<15>, IncrediblySexy

function :: () -> Fun
function = | | (Fun)
#+end_example

Normally the such types wouldn't contribute much other than make the
program more readable.  In our case the situation is quite a bit more
interesting, since any function can generate differently constrained
types, and thus you can effect the strategy pattern, without having to
define each individual case.

If all of this is known at compile time, we're in the clear.  No
allocations, and no in-code representation necessary.  Unlike other
languages, these unit types can be constructed at run time as well.
And there may be need to figure out which independent combinations can
occur (in general) and which precise combinations should be
represented by bit patterns.

The second most interesting case is an enumeration:

#+begin_example
type EvenMoreFun = Something :: NonZero | SomethingElse :: <Divisible(15)> | SomethingSexy :: IncrediblySexy

function :: Int -> EvenMoreFun
function a = IF a < 3 THEN Something ELSE SomethingSexy
#+end_example

Here we make use of a few aforementioned properties of our language.
First and foremost, we use the partitioning of constraints.  Secondly,
we use the fact that types are just elaborate sets of constraints, so
they can be partitioned just as well as constraints could be.  One of
the variants is only accomplished as a constraint.  These are unit
variants of non-load carrying enumeration in Rust terminology.

Obviously, since these variants can be instantiated both at compile
time (when gluing functions together), but also at run-time, if we
depend on something, we need some form of discrimination.

In Rust, we'd have a single machine word discriminant at run-time,
which would allow us to encode the information lossless.

Now consider a load-baring variant:

#+begin_example
type Option(T) = Some of T | None
#+end_example

This is a classic monad, that made its way into Rust and C++, and is
gaining traction.  The reason for its unreasonable success is that
it's convenient, especially if you add in syntactic sugar like
=unwrap= and the =?= operator.

The first problem here is that it is fully generic.  We cannot infer
or constrain the type beyond what we have.  So the compiler has no
choice but to infer that this =T= is anything.  Potentially even
heap-allocated.  It should, of course, complain to you about it, and
ask "are you really sure that you can tell me nothing about this
thing".  The relatively good news is that whenever it can infer some
information it can avoid having to heap allocate.

** Concrete representations

We now want to know how to teach the compiler to instantiate the data
at run-time with specific properties.

Enter the =Sized= constraint.

#+begin_example
constraint <Sized(n)> = …
#+end_example

We don't need to know much about what it does, beyond that some types
are constrained to be sized.  Now it stands to reason that being sized
leads to some implications.

#+begin_example
<Sized(n)> & !.has <Linked> .implies <Enumerable(2.pow(n))>
#+End_example

Let's unpack this.  If you have a sized structure that you need to
represent at run-time, and it doesn't contain any heap-allocated
links, you know that this entire type can be enumerated by all
possible bit patterns.  Trivial example: Rust's =u32= is
=<Enumerable(2.pow(32))>=.  Trivial counter example is a Linked list
of byte-sized objects.  You can easily have more than 256 different
elements in the list to violate the constraint.  This is also
(kind-of) why you should be *really* careful with constraint
definitions.

Firstly, it should be obvious how to encode (in-memory) the appearance
of an enumerated structure.  You impose an arbitrary hash function on
the source code (e. g.  a string hash function) order the results
somehow (alphabetically) and you have your bit-representations.


#+begin_example
<BitRepresentable(n)> .follows-from <Enumerable(n)>

ordinal :: <Enumerable(n)> → (Int & <LessThan(floot (log 2 n))>)
ordinal = …

as-bits :: Int & LessThan(n) → Bits(floor (log 2 n))
as-bits = …

represent :: <BitRepresentable(n)> → Bits(n)
represent = as-bits · ordinal

represent :: <BitRepresentable(n)> → Bytes(ceil (n / 8))
represent = byte-align · represent

represent :: <BitRepresentable(n)> → MachineWords(ceil ((ceil (n / 8)) / word-length))
represent = word-align · pad ·represent
#+end_example

The =ordinal= function determines the order in which your enumerables
will be encoded, =as-bits=, is useful for determining the precise
bit-representation of the given number.  We omit these functions,
because we don't want to attach ourselves to a particular Endian-ness.

Now suppose that we want to heap-allocate a =<BitRepresentable(n)>=,
the three overloads come in handy.  Specifically we use a standard
function =ptr-write= that accepts a pointer and some =MachineWords(n)=
(all parameterised in =n=), and simply writes out that bit pattern.

In effect, the way that our language/compiler represent the data on
the heap is contingent on us providing a =represent= function.  It's
not in any way special, other than the standard library requires you
to implement this function.  A different library for a different
architecture will do things differently.  And since the standard
library is optional, and a module, you can do things a different way.
But crucially the logic of padding, alignment and figuring out how to
represent data is /not/ limited to being the compiler's job.  You can,
if you wish to, override this.

More importantly, we don't to have a representation unless we want to
produce programs which do something at run-time.  If you want to use
this programming language as a REPL, you don't need to think about how
you're going to represent the data.  The internal representation has
no bearing on the representation at run-time, though it's preferable
not to stray too far.

Now suppose that we want to implement a =represent= for an option monad.

#+begin_example
represent :: Option<T :: <BitRepresentable(n-1)> → Bits(n)
represent obj = MATCH obj
    CASE Some(some) IS concat 1 (represent some)
    CASE None IS concat 0 (repeat 0 n-1)
#+end_example

Obviously we have a good solution.  If the leading bit is =1=, we can
represent the data using its old implementation.  If the leading bit
is =0=, we replace the entire rest of the structure with =0= s.

*** Comparison to Rust traits

Rust internally does the same thing if you have a niched-structure,
like a reference (not only to the heap, but in general), a non-zero
integer and so on.  Rust however doesn't do a great job in case of
enumerations that don't look like this one.

#+begin_src rust
  pub enum SomeNumber {
      Positive(NonZeroU64),
      Negative(NonZeroU64)
  }

#+end_src

This enumeration will (on average) on x86_64 allocate two machine
words: one for the union's tag, and one for the actual value.  Both
variants have a niche, and are mutually exclusive.  The discriminant
elision that we've done previously doesn't work for this type.  It
would, however work in Grey.

#+begin_example
type SomeNumber = Positive of Int | Negative of Int

represent SomeNumber → Bits(64)
represent obj = MATCH obj
    CASE Positive(num) IS concat 1 (as-bits num)
    CASE Negative(num) IS concat 0 (as-bits num)
#+end_example

And in this case we can reduce the memory footprint compared to Rust.
But that's not all.  All we've done so far isn't some form of trickery
that you can never repeat that is completely opaque.  On the contrary.
This is a standard model API for your type to be allocate-able on the
heap or on the stack.  You can specialise the less constrained
implementations, and get custom on-heap representations for your
custom types.

The only missing piece of the puzzle is that we want to have a theorem
somewhere saying that

#+begin_example
.defined-function represent :: T → _* → Bits(n) .implies T :: <BitRepresentable(n)>
#+end_example

As such, defining a function with this signature automatically
constrains the type to be =<BitRepresentable(n)>=.  We think that this
is the right choice in this particular case, having a bit
representation is equivalent to being Bit-represent-able.

Note, that in Rust, the equivalent (if it allowed you to control the
representation at all) would be

#+begin_src rust
  pub trait BitRepresentable {
      const BITNESS;

      pub fn represent(&self) -> BitPattern;
  }

  impl BitRepresentable for NonZeroU64 {
      BITNESS = 63;

      pub fn represent(&self) -> BitPattern {
          …
      }
  }


  impl BitRepresentable for SomeNumber {
      BITNESS = 64;

      pub fn represent(&self) {
          match self {
              Self::Positive(n) => 1.append(n.represent()),
              Self::Negative(n) => 0.append(n.represent()),
          }
      }

  }
#+end_src

Rust doesn't need the extra step of proving that everything for which
this function is defined is =BitRepresentable=, because Rust's trait
system is built around functions and constants, as opposed to the
interaction of constraints/traits.

You might think that we have extra verbosity for no good reason, but
you would be mistaken.  Firstly, in Rust, the rule that a niche should
be used for types that don't use a full machine word is a hard-coded
into the compiler.  In our case, this behaviour is part of a library.

Secondly, in Rust, it would be impossible for a programmer to define a
rule that would allow one to encode the tag in any other way.  Even
for two enumerations of equal size, where both variants encode a niche
the allocated number of words is going to include a whole word for a
tag, even though there's a free bit.  We've already shown how to do
it.

Moreover, we cannot have different pathways to being bit-represent-able
in Rust.  The only way, the way which we defined, is to have a
function =represent= defined for us.  This allows Rust generics to be
maximally constraining, but also severely limits the ways in which we
can create explicit pathways to become bit-represent-able.  Just above
we added the implication that being =Enumerable= implies being
bit-represent-able.  In Rust the equivalent would be

#+begin_src rust
  impl<T: Enumerable> BitRepresentable for T {
      BITNESS = <Self as Enumerable>::RANGE;

      pub fn represent(&self) -> BitPattern {
          self.ordinal().as_bits()
      }
  }
#+end_src

However, because Rust lacks (for the time being) specialisation, you
cannot reasonably expect to have any other blanket implementations for
things that could also be enumerable.

Another thing that Rust can't do, is have a one-or-the-other form of
implementation. If you are Bit represent-able then the function
represent must exist and have the correct signature.  In our language
that definition can be (due to currying) arbitrarily variadic. This of
course could be accomplished by a wrapping marker trait in Rust.

Finally, in our language something can be BitRepresentable without
having a =represent= implementation.  What this means is that library
users can define the equivalents of Rust's blanket implementations for
overlapping constraints.  If you ever wrote lots of wrapping
structures just to bypass the orphan rule, you know that this would be
a beneficial change.

But what this also allows you to do, is to prevent certain kinds of
implementations.  The orphan rule exists to enforce a simple principle
in Rust.  All types that are not demonstrated to implement a trait and
already defined are necessarily not implementing the trait. However,
this does not preclude people from defining wrapper structures to
bypass this rule.

Suppose you had a serialisation library and wanted to ensure that
architecture-dependent types are not included in the interface for
(de)serialisation. In Rust, obviously, something like =usize= if it
isn't included in the =Encode= trait, will not ever implement the
=Encode= trait and so =Derive(Encode)= will fail if it sees =usize=.
However, there is a simple way to by-pass this limitation.  Just wrap
=usize= in an object, and somehow mimic the encode implementation for
=u64=.  In C++ it's even worse, since templates are minimally
constraining, or simply put Duck-typed.

In our language, however, we can say the following:
#+begin_example
<ArchitectureDependent> .implies !<Encodeable>

.any ( T .defined-member(_+ :: <ArchitectureDependent>)
  T .defined-variant(_+ :: <ArchitectureDepedent>)
  T LinksTo(TT :: <ArchitectureDependent>) ) .implies T :: <ArchitectureDependent>

T :: Usize .implies T :: <ArchitectureDependent>
#+end_example

And now, you can't have it as a member, you can't be a smart pointer
to that type, and you definitely can't be an enumeration with it as a
variant.  There's no way around this rule.  You can only disable the
rule.  Of course nobody can stop you from creating your own version of
the library with a different set of rules, but then the hash of said
library will change.  You can't hack your way around this limitation
without having an explicit =.relax= somewhere in the code.

But you have to remember that =.constrain= -able and =.relax=-able
constraints are purely opt in.  Some theorems can only be asserted by
proof, and the author of said theorems gets to decide if they can be
explicitly =.relax=-ed.

* Some basics

For us to be able to function properly, some forms of basic
first-kind-ed types need to be present.  Some languages (C-family)
prefer for that to be an array.  The functional family of languages
most often retain a list, due to it not technically requiring a
modification of the underlying memory.  Though a language like Python
might mislead you into thinking that it too prefers a list as a
first-class citizen that has first-party special treatment, in reality
it is the dictionary type that has the largest importance.

This choice is not too simple, and as such requires a bit of
deliberation.

**** Arrays

Choosing an array has a lot of benefits, the chief of which is that
arrays offer the best performance out of the box.  Arrays don't
require any special tooling, the same pointer arithmetic that allows
you to treat =struct= pointers as single objects is actually the
mechanism used to address elements of an array.  Indeed, arrays are
the most cache-efficient and space-efficient.  They can be allocated
on the stack or on the heap, and lend themselves to the most commonly
used alternative to linked lists: a dynamically re-allocated array.

It is a natural choice for imperative languages.  You can treat
information as directly as you like, and from the point of view of a
Von-Neumann architecture, you're dealing with memory directly (as in
knowing precisely what's going on behind the scenes) rather than
through some pretence.  In most cases an array is the best choice for
a collection of objects.  But arrays come with a few limitations.

Once allocated an array cannot be resized, only relocated to a place
where more free space is available, if such a place exists.
Sometimes, the sum total of free RAM can be more than the largest
contiguous space that needs to be allocated, but situations like this
one are rare.

Another limitation is that arrays' elements are of fixed sizes.
Pointer arithmetic becomes impossibly complex as soon as you allow for
objects to be of different sizes.  To avoid this nightmare, the case
where elements are of different sizes is simply discarded.  You can
store the elements in different arrays if you needed to.  A
consequence of this limitation is that in most strongly typed
languages arrays must be of the same type (and not just size).  This
would pose no issue to C++ or Rust, but greatly impede progress in a
language like Python or Lisp, where objects are dynamically typed.

Functional languages dislike arrays too, but for different reasons
which to me are completely opaque.  For some reason it is assumed that
it is easy to implement a linked list in the type system, but
impossible to implement an array.  It should be noted though that the
simple recursive =cons= that is responsible for the ease of creation
of linked lists also maps onto arrays parameterised by their size.
The problem here lies in the fact that a literal natural number in
most programming languages is a shortcut to its binary representation,
but in e. g.  Rust, to emulate a type literal constant 1, you need to
define it via a macro and instantiate all of the numbers at once.

As you can see, making arrays the designated first-class citizens of
grey would take away our ability to be more general.  We want to
support them, but not as the starting point, lest we want to enforce
the limitations in many other areas.

**** Linked lists

These types are straightforward provided you know what a pointer is.
Pointer arithmetic is not necessary for the implementation to be
efficient, one must only be able to dereference a pointer.  Being able
to add or subtract from them is optional.  As a side effect linked
lists aren't required to store items of the same size or in adjacent
locations.

While modestly inefficient in terms of both access and storage space
(a linked list of bytes, is literally mostly the pointer overhead),
they can take advantage of RAM in more locations.  If you want to
store a large number of large data-structures that are accessed either
from the beginning or the end (but not anywhere else), linked lists
are just as efficient as array-based implementations.

However, they /only/ make sense if what one deals with is a large
collection of large objects in a highly fragmented memory environment.
Remove one of those, and you are (at best) tied with arrays.  In fact,
worse, since an array requires no extra storage, and linked lists do.

Ironically, linked lists make sense on memory starved machines with a
low-bit architecture.  If your pointer is literally 8 times greater
than the smallest addressable unit, you could waste up to 87.5% of the
storage space.  If the pointer is itself the smallest addressable
unit, it's much more manageable at 50% (though less than ideal in
either case).

Linked lists, are extremely flexible.  Dynamically typed languages
(like Python and Lisp) allow linked lists to store arbitrary data.
Strongly typed languages, like Standard ML and Haskell enforce that
data be of some specific type, even if said type could be a union of
many.  Rust supports both approaches, but wants you to be explicit
about it: if you want to store a list of anything and everything, you
need to create a linked list of type =<dyn std::any::Any>=, which is
exactly what's happening under the hood in Python or Lisp (and in
reality in Java too, but the type restriction is opt-out).

For our purposes Linked lists make a lot of sense as a starting point.
They are the most flexible, and from the point of view of language
designers a natural first-choice.  But in my opinion, they are just
too easy to implement without going into compiler internals.

To implement arrays, we want to expose access to memory allocation
primitives, and allow interpreting random memory contents as objects
of some type, (but remove that ability later for strong typing).  By
contrast, if you allow people to allocate ordinary structures and
implement pointers to those structures, you already have all of the
ingredients to build a linked list.  It makes sense to start with
linked lists, but as soon as you provide one, it becomes the
definitive version… and herein the problem: linked lists can be
tweaked.

I can implement them inside arrays, or across slabs of contiguous
memory, I can emulate pointers by having a hybrid structure, I can
have bidirectional connections, and store as many end-caps as I
like. I could also disallow cyclic linked lists, and store something
called a length instead of computing it. I can also appreciate that a
linked list can be extremely wasteful if implemented with all of the
extra bells and whistles. If you want one, it's fairly easy to write
one.  If you don't want any particular kind, it makes no sense to even
attempt to provide one.

**** Dictionaries

A dictionary is the more abstract of the three basic types.  While an
array is pretty much the same thing everywhere save for padding an
syntactic sugar around pointer arithmetic, associative structures are
not.

A mapping is an extremely abstract concept.  You can interpret it as a
function, or you can interpret it as a collection of tuples, with the
first argument being more special.  You can interpret it as a map
based on equivalence and linear search, based on hashing or based off
of binary (or ternary) comparison search.

Depending on the implementation a dictionary can be the dependent
structure.  It can also be the independent structure from which all
others are produced, as in Python.  An object in Python is a reference
to a dictionary, with some keys known ahead of time.  This allows for
objects to have other objects, and since everything, including
integers are pointers to heap-allocated values, there are no more
efficient semantically compatible implementations[fn::Incidentally,
Sockets are that implementation, just with the caveat of being an
outlier. ].

Unfortunately, the variety of implementations precludes it from being
the fundamental collection.

** The fundamental collection

The brief excursion into fundamentals of Programming language design,
taught us that what we want is closest to an array.  If we were any
other programming language designer, we'd just throw in the towel and
say, arrays it is, then.  But no.

Remember, all types are syntactic sugar around constraints.  If we set
good constraints to start with, we can add all of the three structures
along with their special (not really) syntactic treatment.  In reality
what we want is the constraints that tell us what we want, and a means
of choosing the right tool for the right job as a matter of constraint
resolution and not as a rule.

So, without further ado, what can we do instead.  As a matter of
paradigm, 


# Local Variables:
# jinx-local-words: "AlgebraicRecord ArchitectureDepedent ArchitectureDependent BetterButStillBad BitRepresentable Bool BuiltIn Coq Crobember DataProfile DivideByZero Encodeable EvenMoreFun ExposedClass FibonacciHeap FireTruck FizzBarBuzz FizzBuzz FizzBuzzBar FloatingPoint GammaFunc Gammafunc Garand Geary GreaterThan Haskells HiddenString Hm IncrediblySexy InputType InputeType IsEqualTo IsGreaterThan IsGreaterthan Kirigami LessThan LikelyRepresentedBy Linde LinksTo MLs MachineWords MeCember MersenneTwister Millner NonNegative NonZero NotEarlierThan Octopi OfType OrdinaryCar OutdoorsOberverSingletonController OutputType PowerOFTwo PowerOfTwo PrintOnly Ptr REPLs Recordd SomeNumber SomethingElse SomethingSexy SpecialNumericDisplay Str SubTask Succ Suckless TooManyFieldsForADataType TopLevel Toplevel TwosComplement Usize WindowBlinds Zig aa accessor addedOn anotherInstance applyArgs arctangent arg args argv arithmeticSeries ascii asm atanNormalised baz bb binet blahFunction ccall ceil checkMergeAbleSorted checksorted childOne childTwo color concat cond couraged defraccculator deref dereference destructor divideBySelf divisibleBy dll doSomethingWithWorld doubleInts dptr dummyRecord eax edi elif emergently enum enumerables enumify exampleuseofSort exposedInstance expr externalState fb fileURL findAnswerToAll fixedLength fizzBuzz fizzbuzz fizzer floot frac fromDisk fromInternet func functionArgs generify geometricSeries greaterThan hasBlahFunction hasFireEquipment imperativity imull instantiation interpSubrountine interpretFromPointer isA isGreaterThan isTrue isType len leq lessThan libbotan lingua linter lowerInc lsp math memoizedRecursiveFib mergesort movl mutify myAlgebraicRecord myRecord namelist nextFun niched num ors pre prettyPrint printf priori privateField proc ptr quicksort ret rst singleNumber somefunc sqrt stderr strategyNumber subscripting tempVal tempp todo tokenisation tokeniser twoInclusive twoNumbers typedef typeof undef upperInc variadic wooow woow"
# End:
